```{r, stand-alone-preamble}
require(tidyverse)
require(magrittr)

require(brms)
require(tidybayes)
require(MVBeliefUpdatr)
require(phonR)
require(cowplot)

source("functions.R")
```

# Experiment 1: Listener's expectations prior to informative exposure
Experiment 1 investigates native (L1) US English listeners' categorization of word-initial stop voicing by an unfamiliar female L1 US English talker, prior to more informative exposure. Specifically, listeners heard isolated recordings from a /d/-/t/ continuum, and had to respond which word they  heard (e.g., "din" or "tin"). The recordings varied in voice onset time (VOT), the primary phonetic cue to word-initial stop voicing in L1 US English, as well as correlated secondary cues (f0 and rhyme duration). Critically, exposure was relatively uninformative about the talker's use of the phonetic cues in that all phonetic realizations occurred equally often. The design of Experiment 1 serves two  goals.

The first goal is methodological. We use Experiment 1 to test basic assumptions about the paradigm and stimuli we employ in the remainder of this study. We obtain estimates of the category boundary between /d/ and /t/ *for the specific stimuli used in Experiment 2*, as perceived by *the type of listeners we seek to recruit for Experiment 2*. We also test whether prolonged testing across the phonetic continuum changes listeners' categorization behavior. Previous work has found that prolonged testing on uniform distributions can reduce the effects of previous exposure [e.g., @mitterer2011; @liu-jaeger2018], at least in listeners of the age group we recruit from [@scharenborg-janse2013]. However, these studies employed only a small number of 5-7 perceptually highly ambiguous stimuli, each repeated many times. In Experiment 1, we employ a much larger set of stimuli that span the entire continuum from very clear /d/s to very clear /t/s, each presented only twice. If prolonged testing changes listeners' responses, this has to be taken into account in the design of Experiment 2.

The second purpose of Experiment 1 is to introduce and illustrate relevant theory. We compare different models of listeners' prior expectations against listeners' categorization responses in Experiment 1. The different models all aim to capture the implicit expectations of an L1 adult listener of US English might have about the mapping from acoustic cues to /d/ and /t/ based on previously experienced speech input. As we describe in more detail after the presentation of the experiment, the models differ, however, in whether these prior expectations take into account that talkers can differ in the way they realize /d/ and /t/. This ability to take into account talker differences even prior to more informative exposure is predicted---though through qualitatively different mechanisms, as we discuss below---both by normalization accounts [@cole2010; @mcmurray2011information] and by accounts that attribute adaptive speech perception to changes in category representations [Bayesian ideal adaptor theory, @kleinschmidt2015robust; EARSHOT, @magnuson2020earshot; episodic theory, @goldinger1998echoes; exemplar theory, @johnson1997talker; @pierrehumbert2001]. It is, however, unexpected under accounts that attribute adaptive speech perception solely to ad-hoc changes in decision-making. We did not expect that Experiment 1 yields a decisive conclusion with regard to this second goal, which is also addressed in Experiment 2. Rather, we use Experiment 1 as a presentationally convenient way to introduce some of the different models and provide readers with initial intuitions about what experiments of this type can and cannot achieve.

## Methods 
### Participants
Participants were recruited over Amazon's Mechanical Turk platform, and paid $2.50 each (for a targeted remuneration of \$6/hour). The experiment was only visible to Mechanical Turk participants who (1) had an IP address in the United States, (2) had an approval rating of 95% based on at least 50 previous assignments, and (3) had not previously participated in any experiment on stop voicing from our lab. 

24 L1 US English listeners (female = 9; mean age = 36.2 years; SD age = 9.2 years) completed the experiment. To be eligible, participants had to confirm that they (1) spent at least the first 10 years of their life in the US speaking only English, (2) were in a quiet place, and (3) wore in-ear or over-the-ears headphones that cost at least \$15. 

### Materials 
We recorded multiple tokens of four minimal word pairs ("dill"/"till", "dim"/"tim", "din"/"tin", and "dip"/"tip") from a 23-year-old, female L1 US English talker with a mid-Western accent. These recordings were used to create four natural-sounding minimal pair VOT continua (dill-till, dip-tip, din-tin, and dip-tip) using a Praat script [@winn2020manipulation]. The full procedure is described in the supplementary information (SI, \@ref(sec:SI-XXX)). The VOT continua ranged from -100ms VOT to +130ms VOT in 5ms steps. Experiment 1 employs 24 of these steps (-100, -50, -10, 5  `r paste0(seq(15, 90, 5), collapse = ", ")`, `r paste0(seq(100, 130, 10), collapse = ", ")`). VOT tokens in the lower and upper ends were distributed over larger increments because stimuli in those ranges were expected to elicit floor and ceiling effects, respectively. 

We further set the F0 at vowel onset to follow the speaker's natural correlation which was estimated through a linear regression analysis of all the recorded speech tokens. We did this so that we could determine the approximate corresponding f0 values at each VOT value along the continua as predicted by this talker's VOT. The duration of the vowel was set to follow the natural trade-off relation with VOT reported in @allen1999effects. This approach closely resembles that taken in @theodore2019distributional, and resulted in continuum steps that sound highly natural [unlike the robotic-sounding stimuli employed in @clayards2008perception; @kleinschmidt2016you]. All stimuli are available as part of the OSF repository for this article.

In addition to the critical minimal pair continua we also recorded three words that did not did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used as catch trials. Stimulus intensity was set to 70 dB sound pressure level for all recordings. 

### Procedure
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu/FILLIN-FULL-URL). The first page of the experiment informed participants of their rights and the requirements for the experiment: that they had to be native listeners of English, wear headphones for the entire duration of the experiment, and be in a quiet room without distractions. Participants had to pass a headphone test, and were asked to keep the volume unchanged throughout the experiment. Participants could only advance to the start of the experiment by acknowledging each requirement and consenting to the guidelines of the Research Subjects Review Board of the University of Rochester. 

On the next page, participants were informed about the task for the remainder of the experiment. They were informed that they would heard a female talker speak a single word on each trial, and had to select which word they heard. Participants were instructed to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. This was done to encourage their full attention.

Each trial started with a green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen, as shown in Figure \@ref(fig:exp1-example-trial). At 1000ms from trial onset, an audio recording from the matching minimal pair continuum started playing. Participants were required to click on the word they heard. For each participant, /d/-initial words were either always displayed on the left side or always displayed on the right side. Across participants, this ordering was counter-balanced. After participants clicked on the word, the next trial began.

```{r exp1-example-trial, fig.cap="Example trial display. The words were displayed 500ms after trial onset and the audio recording of the word was played 1000ms after trial onset"}
knitr::include_graphics("../figures/exp1_trial_example.png")
```

Participants heard 192 target trials (four minimal pair continua, each with 24 VOT steps, each heard twice). In addition, participants heard 12 catch trials. On catch trials, participant saw two written catch stimuli on the screen (e.g., "flare" and "rare"), and heard one of them (e.g. "rare"). Since these recordings were easily distinguishable, they served as a check on participant attention throughout the experiment. 

The order of trials was randomized for each participant with the only constraint that no stimulus was repeated before each stimulus had been heard at least once. Catch trials were distributed randomly throughout the experiment with the constraint that no more than two catch trials would occur in a row. <!-- TO DO: correct? --> Participants were given the opportunity to take breaks after every 60 trials. Participants took an average of 12 minutes (SD = 4.8) to complete the 204 trials, after which they answered a short survey about the experiment. 

## Results
We first present the behavioral analyses of participants' categorisation responses. Then we compare participants' responses to the predictions of different models fit on the distribution of stop voicing cues in a large database of L1 US English productions of word-initial /d/s and /t/s [@chodroff-wilson2018].

```{r, echo=FALSE}
# load formatted dataframe from experiment 1
d.test <- read_csv("../data/d.test.Exp1.csv", show_col_types = F)

# mark catch trials rows and check correct
d.test %<>% 
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == TRUE & Item.MinimalPair == Response, TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == FALSE, NA, CatchTrial.Correct)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Excluded.due.to.CatchTrial = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 10, TRUE, FALSE))

# get the rows removed due to catch trial performance
excluded.data.due.to.catch <- 
  d.test %>% 
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == TRUE)

# set RT exclusion criteria and create excluded columns
d.test %<>%
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == FALSE) %>% 
  mutate(Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
         # deducts each value with subject's own mean, div by own SD
         Response.log_RT.scaled = scale(Response.log_RT), 
         # this gives each subject's mean log_RT
         Response.log_RT.mean = mean(Response.log_RT, na.rm = T), 
         # Make distributional outliers
         Excluded.participant.due.to.mean.RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE)) 

excluded.participants.due.to.mean.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE)

d.test %<>% 
  filter(Excluded.due.to.CatchTrial == FALSE | Excluded.participant.due.to.mean.RT == FALSE) %>% 
  mutate(Excluded.participant.due.to.RT = ifelse(abs(Response.log_RT.scaled) > 3, TRUE, FALSE))
  
# get the rows removed due to RT
excluded.data.due.to.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE | Excluded.participant.due.to.RT == TRUE) 

# get the proportion of rows excluded from analysis
proportion.excluded <- (nrow(excluded.data.due.to.RT)) / (nrow(d.test))

# make a dataframe after exclusion to be used for analysis
d.test.excluded <- 
  d.test %>% 
  filter(
    Is.CatchTrial == FALSE, 
    Excluded.due.to.CatchTrial == FALSE, 
    Excluded.participant.due.to.RT == FALSE) %>% 
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0))
```

### Exclusions
We excluded from analysis participants who committed more than 3 errors out of the 12 catch trials (<75% accuracy, N = 3), participants with an average reaction time (RT) more than three standard deviations from the mean of the by-participant means (N = 0), and participants who reported not to have used headphones (N = 0) or not to be native (L1) speakers of US English (N = 0). For the remaining participants, trials that were more than three SDs from the participant's mean RT were excluded from analysis (`r proportion.excluded * 100 `%). Finally, we excluded participants (N = 0) who had less than 50% data remaining after these exclusions. 

### Behavioral analyses
The goal of our behavioral analyses was to address three methodological questions that are of relevance to Experiment 2: (1) whether our stimuli resulted in 'reasonable' categorisation functions, (2) whether these functions differed between the four minimal pair items, and (3) whether participants' categorisation functions changed throughout the 192 test trials.

To address these questions, we fit a single Bayesian mixed-effects psychometric model to participants' categorization responses on critical trials [e.g., @prins2011]. This model is essentially an extension of mixed-effects logistic regression that also takes into account attentional lapses. A failure to do so---while commonplace in research on speech perception [incl. our own work, but see @clayards2008; @kleinschmidt2016you]---can lead to biased estimates of categorization boundaries [e.g., @wichman-hill2001]. The mixed-effects psychometric model describes the probability of "t"-responses as a weighted mixture of a lapsing-model and a perceptual model. The lapsing model is a mixed-effects logistic regression [@jaeger2008categorical] that predicts participant responses that are made independent of the stimulus---for example, responses that result from attentional lapses. These responses are independent of the stimulus, and depend only on participants' response bias. The perceptual model is a mixed-effects logistic regression that predicts all other responses, and captures stimulus-dependent aspects of participants' responses. The relative weight of the two models is determined by the lapse rate, which is described by a third mixed-effects logistic regression.

The *lapsing model* only contained an intercept (the response bias in log-odds) and by-participant random intercepts. Similarly, the *model for the lapse rate* only had an intercept (the lapse rate) and by-participants random intercepts. Previous studies with similar paradigms have typically found lapse rates of 0-10% [< -2.2 log-odds, e.g., @clayards2008perception; @kleinschmidt2016you]. No by-item random effects were included for the lapse rate nor lapsing model since these parts of the analysis---by definition---describe stimulus-*in*dependent behavior. The *perceptual model* included an intercept and VOT, as well as the full random effect structure by participants and items (the four minimal pair continua), including random intercepts and random slopes by participant and minimal pair. We did not model the random effects of trial to reduce model complexity. This however makes our analysis of trials in the model anti-conservative. 

Based on previous experiments, we expected a strong positive effect of VOT, with increasing proportions of "t"-responses for increasing VOTs. We did not have clear expectations for the effect of trial other than that responses should become more uniformed (i.e move towards 50-50 "d"/"t"-bias or 0-log-odds) as the experiment progressed [@liu2018inferring] due to the un-informativeness of the stimuli. Finally, the models included the covariance between by-participant random effects across the three linear predictors for the lapsing model, lapse rate model, and perceptual model. This allows us to capture whether participants who lapse more often have, for example, different response biases or different sensitivity to VOT (after accounting for lapsing).

We fit the model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. Following previous work from our lab [@horberg2021rational; @xie2021cross], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008standardize], and used Student priors centered around zero with a scale of 2.5 units [following @gelman2008weakly] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $\hat{R}$ were close to 1.

```{r}
# load (or run) the psychometric model with interaction
fit_mix <- brm(
  bf(
    Response.Voicing == "voiceless" ~ 1,
    mu1 ~ 1 + (1 | g | ParticipantID),
    mu2 ~ 1 + sVOT * sTrial + (1 + sVOT | g | ParticipantID) + (1 + sVOT | h | Item.MinimalPair),
    theta1 ~ 1 + (1 | g | ParticipantID)),
  data = d.test.excluded,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  family = mixture(bernoulli("logit"), bernoulli("logit")),
  control = list(adapt_delta = .99),
  file = "../models/Exp-NORM-lapsing-bias-GLMM")

# set the mean and SD values for scaling/unscaling purposes
VOT.mean_norm <- mean(d.test.excluded$Item.VOT)
VOT.sd_norm <- sd(d.test.excluded$Item.VOT)
Trial.mean <- mean(d.test.excluded$Trial) 
Trial.sd <- sd(d.test.excluded$Trial)
d.test.excluded %<>%
  mutate(
    sVOT = (Item.VOT - VOT.mean_norm) / (2 * VOT.sd_norm),
    sTrial = (Trial - Trial.mean) / (2 * Trial.sd))

psychometric_fit_data <- 
  conditional_effects(
    fit_mix, 
    effects = "sVOT", 
    plot = F)[[1]]

if (file.exists("../data/conditional_effects_VOT_Trial.rds")) {
  psychometric_fit_interaction <- read_rds("../data/conditional_effects_VOT_Trial.rds")
} else {
  int_conditions <- list(sTrial = sort(unique((d.test.excluded$Trial - Trial.mean)) / (2 * Trial.sd)))
  
  psychometric_fit_interaction <-
  conditional_effects(
    fit_mix,
    effects = "sVOT:sTrial",
    int_conditions = int_conditions,
    plot = F)[[1]]

write_rds(psychometric_fit_interaction, file = "../data/conditional_effects_VOT_Trial.rds")
}

# get the PSE from the fitted categorisation function
PSE.fit_mix <- descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_norm, VOT.sd_norm) 

# get posterior samples of intercept and slope, and median qi of the PSEs
post_sample_norm <- spread_draws(fit_mix, b_mu2_Intercept, b_mu2_sVOT) %>% 
  mutate(PSE = descale(-(b_mu2_Intercept/b_mu2_sVOT), VOT.mean_norm, VOT.sd_norm)) %>% 
  median_qi(PSE)
```

```{r}
newdata <- expand_grid(
  sVOT = (seq(-100, 130, 1) - VOT.mean_norm) / (2 * VOT.sd_norm),
  Item.MinimalPair = levels(factor(d.test.excluded$Item.MinimalPair)),
  ParticipantID = levels(factor(d.test.excluded$ParticipantID)),
  sTrial = 0)

if (file.exists("../data/categorisation_by_min_pair.rds")) {
  cat_minimalpair <- read_rds("../data/categorisation_by_min_pair.rds")
} else {
  cat_minimalpair <- fit_mix %>%
  epred_draws(newdata = newdata,
              ndraws = 1000,
              re_formula = ~ (1 + sVOT | Item.MinimalPair))
  write_rds("../data/categorisation_by_min_pair.rds")
}
```

```{r fitted-categorisation-minimal-pair"}
MinPair_plot <- cat_minimalpair %>% 
  group_by(sVOT, Item.MinimalPair) %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = .epred, colour = Item.MinimalPair)) +
  stat_lineribbon(alpha = .9, .width = 0.95) +
  scale_x_continuous("VOT (ms)") +
  scale_y_continuous("Fitted proportion of 't'-responses") +
  scale_color_manual("Minimal Pair", breaks = c("dilltill", "dimtim", "dintin", "diptip"),  
                     values = c("#002699", "#0040ff", "#668cff", "#b3c6ff"), 
                     labels = c("dill/till", "dim/tim", "din/tin", "dip/tip")) +
  scale_fill_brewer("CI", palette = "Greys", type = "qual") +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r psychometric-plot, fig.width=3.8, fig.height =2.6, message=FALSE}
p.psychometric.vot <- psychometric_fit_data %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = estimate__)) +
  scale_x_continuous("VOT (ms)", breaks = scales::pretty_breaks(n = 4), limits = c(-100, 130)) +
  scale_y_continuous("Fitted proportion of 't' responses") +
  ## transformation of by-participant means into empirical logits
   # stat_summary(
   #  data = d.test.excluded %>% 
   #    group_by(ParticipantID, Item.VOT) %>% 
   #    mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
   #    summarise(Response.EmpiricalLogitVoiceless = qlogis((sum(Response.ProportionVoiceless) + .5)/(length(Response.ProportionVoiceless) + 1))), 
   #  mapping = aes(x = Item.VOT, 
   #                y = Response.EmpiricalLogitVoiceless),
   #  geom = "pointrange",
   #  fun.data = function(x){mean_cl_boot(x) %>% mutate(across(c(y, ymin, ymax), ~ plogis(.x)))},
   #  size = .4,
   #  colour = "#c4b7a6",
   #  alpha = .6,
   #  inherit.aes = F)  
  stat_summary(
    data = d.test.excluded %>% 
      group_by(ParticipantID, Item.VOT) %>% 
      mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
      summarise(Response.ProportionVoiceless = mean(Response.ProportionVoiceless)), 
    mapping = aes(x = Item.VOT, 
                  y = Response.ProportionVoiceless),
    geom = "pointrange",
    fun.data = "mean_cl_boot",
    size = .4,
    colour = "#c4b7a6",
    alpha = .6,
    inherit.aes = F)

p.vot <- p.psychometric.vot +
  geom_ribbon(
    aes(ymin = lower__, 
        ymax = upper__), alpha = .08) +
  geom_line(size = 1.5, 
            colour = "#333333",
            alpha = .8) +
  # annotate(geom = "segment", 
  #          y = .5, yend = 0, 
  #          x = PSE.fit_mix, xend = PSE.fit_mix, 
  #          arrow = arrow(angle = 15, length = unit(0.1, "inches"), 
  #                        ends = "last", 
  #                        type = "closed"), 
  #          color = "darkgray", 
  #          linetype = 1) +
  # annotate(geom = "text", 
  #          y = 0, 
  #          x = PSE.fit_mix + 10, 
  #          label = paste("PSE =", round(PSE.fit_mix, 1), "ms"), 
  #          color = "darkgray", 
  #          hjust = 0, 
  #          vjust = 0, size = 3.5) +
  geom_errorbarh(
    data = post_sample_norm %>% 
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y), 
    color = "#333333",
    height = 0,
    alpha = .5,
    size = 1, 
    inherit.aes = F) +
  geom_point(
    data = post_sample_norm %>% 
      median_qi(PSE) %>% 
      mutate(y = 0.01),
    mapping = aes(x = PSE, y = y), 
    color = "#333333", 
    size = 1.3,
    alpha = .5) +
  annotate(
    geom = "text",
    x = 75,
    y = 0.01, 
    label = paste(round(post_sample_norm[[2]]), "ms", "-", round(post_sample_norm[[3]]), "ms"),
    size = 2.5,
    colour = "darkgray") 
```

(ref:fitted-categorisation-plots) Categorisation functions and points of subjective equality (PSE) derived from the Bayesian mixed-effects psychometric model fit to listeners' responses in Experiment 1. The categorization functions include lapse rates and biases. The PSEs correct for lapse rates and lapse biases (i.e., they are the PSEs of the perceptual component of the psychometric model).\protect\footnote{Here and in Experiment 2, lapse biases were close to uniform (.5/.5). For such scenarios, bias-corrected PSEs will be very similar to uncorrected PSEs. This is also evident in Figure \ref{fig:fitted-categorisation-plots}.} **Panel A:** Effects of VOT, lapse rate, and lapse bias, while marginalizing over trial effects as well as all random effects. Vertical point ranges represent the mean proportion and 95% bootstrapped CIs of participants' "t"-responses at each VOT step. Horizontal point ranges denote the mean and 95% quantile interval of the points of subjective equality (PSE), derived from the 8000 posterior samples of the population parameters. **Panel B:** The same but showing the fitted categorization functions for each of the four minimal pair continua. Participants' responses are omitted to avoid clutter. **Panel C:** Joint effects of VOT and trial as well as lapse rate and bias, while marginalizing over random effects.   

```{r fitted-categorisation-plots, warning=FALSE, fig.width=6, fig.height = 3.3, fig.cap="(ref:fitted-categorisation-plots)"}
legend1 <- get_legend(MinPair_plot + theme(legend.position = "top"))

MinPair_plot <- MinPair_plot + theme(legend.position = "none")

grid1 <- plot_grid(p.vot + theme(axis.title.x = element_blank()),
                   MinPair_plot,
                   ncol = 2, 
                   align = "h", 
                   labels = NULL, 
                   label_size = 10,
                   hjust = .1, 
                   rel_widths = c(0.95, 0.8))

grid2 <- plot_grid(
  legend1, 
  grid1,
  ncol = 1,
  rel_heights = c(0.1, 1))

ggdraw(add_sub(grid2, "VOT (ms)", size = 10, fontface = "bold"))
```

```{r by-participant-lapse-bias, fig.height=2.6, fig.width=6.5}
# lapse_participant <- fit_mix %>% spread_draws(r_ParticipantID__theta1[ParticipantID, term], b_theta1_Intercept) %>%
#   group_by(ParticipantID) %>%
#   mutate(ParticipantID = factor(ParticipantID),
#          Participant_lapse = b_theta1_Intercept + r_ParticipantID__theta1,
#          estimated_lapse = plogis(Participant_lapse) * 100) %>%
#   select(ParticipantID, term, r_ParticipantID__theta1, Participant_lapse, estimated_lapse) %>%
#   mode_hdci(estimated_lapse)
# 
# bias_participant <- fit_mix %>%
#   spread_draws(r_ParticipantID__mu1[ParticipantID, term], b_mu1_Intercept) %>%
#   group_by(ParticipantID) %>%
#   mutate(ParticipantID = factor(ParticipantID),
#          Participant_bias = b_mu1_Intercept + r_ParticipantID__mu1,
#          estimated_bias = plogis(Participant_bias) * 100) %>%
#   select(ParticipantID, term, r_ParticipantID__mu1, Participant_bias, estimated_bias) %>%
#   mode_hdci(estimated_bias)
# 
# estimate_minpair <- fit_mix %>% spread_draws(r_Item.MinimalPair__mu2[Item.MinimalPair, term], b_mu2_Intercept, b_mu2_sVOT) %>%
#   group_by(Item.MinimalPair) %>%
#   mutate(predicted_eff = ifelse(term == "Intercept", r_Item.MinimalPair__mu2 + b_mu2_Intercept, r_Item.MinimalPair__mu2 + b_mu2_sVOT)) %>%
#   group_by(Item.MinimalPair, term) %>%
#   mode_hdci(predicted_eff) %>%
#   pivot_wider(names_from = term,
#               values_from = c(predicted_eff, .lower, .upper))
```



The lapse rate was estimated to be on the slightly larger side, but within the expected range 
(`r make_CI(fit_mix, "theta1_Intercept", "theta1_Intercept < 0")`). Maximum a posteriori (MAP) estimates of by-participant lapse rates ranged from XX . Very high lapse rates were estimated for four of the participants with one in particular whose CI indicated exceptionally high uncertainty. These lapse rates might reflect data quality issues with Mechanical Turk that started to emerge over recent years [see @REFS; and, specifically for experiments on speech perception, @cummings2023], and we return to this issue in Experiment 2. 

The response bias were estimated to slightly favor "t"-responses (`r make_CI(fit_mix, "mu1_Intercept", "mu1_Intercept > 0")`), as also visible in Figure \@ref(fig:fitted-categorisation-plots) (left).
Unsurprisingly, the psychometric model suggests high uncertainty about the participant-specific response biases, as it is difficult to reliably estimate participant-specific biases while also accounting for trial and VOT effects (range of by-participant MAP estimates: XX). For all but four participants, the 95% CI includes the hypothesis that responses were unbiased. Of the remaining four participants, three were biased towards "t"-responses and one was biased toward "d"-responses.

There was no convincing evidence of a main effect of trial ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sTrial", "mu2_sTrial < 0")`). Given the slight overall bias towards "t"-responses, the direction of this effect indicates that participants converged towards a 50/50 bias as the test phase proceeded. This is also evident in Figure \@ref(fig:fitted-categorisation-plots) (right). In contrast, there was clear evidence for a positive main effect of VOT on the proportion of "t"-responses ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT", "mu2_sVOT > 0")`). The effect of VOT was consistent across all minimal pair words as evident from the slopes of the fitted lines by minimal pair \@ref(fig:fitted-categorisation-plots) (left). MAP estimates of by minimal pair slopes ranged from . The by minimal-pair intercepts were more varied (MAP estimates: ) with one of the pairs, dim/tim having a slightly lower intercept resulting in fewer 't'-responses on average. In all, this justifies our assumptions that word pair would not have a substantial effect on categorisation behaviour. From the parameter estimates of the overall fit we obtained the category boundary from the point of subjective equality (PSE) (`r descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_norm, VOT.sd_norm)`ms) which we use for the design of Experiment 2.

Finally to accomplish the first goal of experiment 1, we look at the interaction between VOT and trial. There was weak evidence that the effect of VOT decreased across trials ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT:sTrial", "mu2_sVOT:sTrial < 0")`). The direction of this change---towards more shallow VOT slopes as the experiment progressed---makes sense since the test stimuli were not informative about the talker's pronunciation. Similar changes throughout prolonged testing have been reported in previous work. [@liu-jaeger2018; @liu-jaeger2019; @REFS].

Overall, there was little evidence that participants substantially changed their categorisation behaviour as the experiment progressed. Still, to err on the cautious side, Experiment 2 employs shorter test phases.

## Comparisons to model of adaptive speech perception
We now turn to final aim of experiment 1 which is to make use of computational models to delve into the theoretical underpinnings that inform the assumptions we make in studies of this kind.

Speakers' productions can act as a proxy for listeners' implicit knowledge of the distributional patterns of cues. This production-perception relationship within a phonological system was observed in early work by [@abramson1973voice] who found that production statistics of talkers along VOT aligned well with data from listeners who had categorised a separate set of synthesised VOT stimuli. This allows for the use of analytic models as tools for predicting categorisation behaviour from speech production [@nearey1986phonological].

We apply this principle in  fitting ideal observer (IO) models by linking the distributional patterns of input to the categorisation behaviour that listeners make in the perception of our stimuli. We compare the categorisation behaviour against predictions of several IO models differentiated by the various assumptions they incorporate. These IOs are trained on cue measurements extracted from an annotated database of 92 L1 US-English talkers' productions [@chodroff2017structure] of word initial stops. By using IOs trained solely on production data to predict behaviour we avoid additional computational degrees of freedom and limit the risk of overfitting the model to the data thus reducing bias.

Hypotheses about the nature of long-term representations maintained by listeners continues to be debated and revised. On one hand there is the proposition that automatic processes that operate purely on the acoustic input is sufficient mechanism for listeners to cope with variation; this can be loosely referred to as normalization accounts. On the other hand are hypotheses that listeners learn and store cue distributions in memory for later retrieval --this does not however, preclude cue normalisation and may even happen in conjunction to it. Within this latter hypothesis, there is debate over the resolution of the input that is actually learned and stored, with exemplar models arguably, accounting for the greatest degree of granularity -- listeners could for instance store talker-specific statistics. A more parsimonious account would suggest that listeners store models of groups of talkers, according to a structure that is most informative for robust speech perception [@kleinschmidt2015robust; @kleinschmidt2019structure]. We thus compare listener categorisations to models that incorporate one or more of these hypotheses (see SI for details of IO fitting). 

Each panel in figure \@ref(fig:comparative-IO-plot) shows 92 talker-specific ideal observer models colour-coded by talker sex, bearing different assumptions plotted against the psychometric fit of listener categorisations (thick black line). We focus mainly on comparing the points of subjective equality (PSEs) which represents the boundary between the two categories. While the functions are not simply described by their PSEs since their slope also matters, we focus on it here as this is most relevant to the design of experiment 2. All IO plots in figure \@ref(fig:comparative-IO-plot) except for (A) are integrated with a noise variance to simulate perceptual noise on the part of listeners [@kronrod2016unified]. The IOs were trained on unnormalised VOTs without noise (A); unnormalised VOTs (B); unnormalised bivariate cues of VOT and F0 (C); C-CuRE-normalised VOT and F0 (D) (@mcmurray2011information see SI section).

Beginning with a qualitative assessment of the plots, IOs that incorporate perceptual noise in the models (B-D) appear to capture the uncertainty reflected in our data better. The slopes of the IOs in panel A are far steeper than the fitted categorisation function but with added noise, as with the IOs in B-D the IO slopes flatten out to better match the slope of the fitted line. This itself indicates that perception of acoustic stimuli is not entirely faithful to the bottom-up signal but is inferred through a combination of what listeners actually perceived and their existing knowledge of the underlying linguistic category [@kronrod2016unified].
Noticeably, in all IO types the median estimated PSE from our participant data is located to the left of the IO-predicted median PSEs although the range of fitted estimates do overlap with the IOs in the upper region. 



\newpage

```{r}
# prepare production corpus from Chodroff & Wilson
d.chodroff_wilson <-
  get_ChodroffWilson_data(
    database_filename = "../data/all_observations_with_non-missing_vot_cog_f0.csv",
    min.n_per_talker_and_stop = 25,
    limits.VOT = c(-Inf, Inf),
    limits.f0 = c(0, 350),
    max.p_for_multimodality = .1
  ) %>%
  mutate_at(
    c("VOT", "f0_Mel"),
    list("centered" = function(x) apply_ccure(x, data = .)))

d.chodroff_wilson.selected <-
  d.chodroff_wilson %>%
  filter(poa == "/d/-/t/") %>%
  group_by(Talker, category) %>%
  mutate(n = n()) %>%
  group_by(Talker) %>%
  # subsample n tokens, as determined by category with fewer tokens
  mutate(
    n_min = min(n),
    n_category = n_distinct(category)) %>%
  # select talkers with both /d/ and /t/ observations
  filter(n_category == 2) %>%
  group_by(Talker, category) %>%
  sample_n(size = first(n_min)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))%>% 
  mutate(category = factor(category))
```


```{r IOs-univariate-predictions, fig.width=3.7, fig.height=2.5, cache=FALSE}
plot_IO_fit <- function(
    data,
    PSEs
) {
  plot <- ggplot() + 
  # geom_ribbon(
  #   data =
  #     data %>%
  #     select(-x) %>%
  #     unnest(categorization) %>%
  #     group_by(gender, x) %>%
  #     summarise(
  #       VOT = map(x, ~ .x[1]) %>% unlist(),
  #       across(response, list("lower" = ~ quantile(.x, .025), "upper" = ~ quantile(.x, .975)))),
  #   mapping = aes(x = VOT, ymin = response_lower, ymax = response_upper, fill = gender),
  #   alpha = .1) +
  data$line + 
  scale_x_continuous("VOT (msec)", breaks = scales::pretty_breaks(n = 3), limits = c(-15, 85), expand = c(0, 0)) +
  scale_y_continuous('Proportion "t"-responses') +
  scale_colour_manual("Model", 
                      values = c(colours.sex), 
                      labels = c("IO (female)", "IO (male)"),
                      aesthetics = c("color", "fill")) +
  geom_errorbarh(
    data = PSEs %>%
      mutate(y = ifelse(gender == "male", -.025, - .06)),
    mapping = aes(xmin = PSE.lower, xmax = PSE.upper, y = y, color = gender),
    height = 0, alpha = .5, size = 1) +
  geom_point(
    data = PSEs %>%
      mutate(y = ifelse(gender == "male", -.025, - .06)),
    mapping = aes(x = PSE.median, y = y, color = gender),
    size = 1.2) +
      annotate(geom = "text",
           y = -.025, x = 65,
           label = paste(PSEs[[1, 2]], "ms", "-", PSEs[[1, 4]], "ms"),
           size = 1.8,
           colour = "#87bdd8") +
  annotate(geom = "text",
           y = -.06, x = 65,
           label = paste(PSEs[[2, 2]], "ms", "-", PSEs[[2, 4]], "ms"),
           size = 1.8,
           colour = "#c1502e") 
  plot + 
  geom_line(
    data = psychometric_fit_data,
    mapping = aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
                   y = estimate__),
    colour = "#333333", 
    size = 1,
    alpha = .8,
    inherit.aes = F) +
  geom_ribbon(
    data = psychometric_fit_data, 
    mapping = aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
                  ymin = lower__, 
                  ymax = upper__),
    alpha = .08,
    inherit.aes = F) +
  geom_errorbarh(
    data = post_sample_norm %>% 
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y), 
    color = "#333333",
    height = 0,
    alpha = .5,
    size = 1) +
  geom_point(
    data = post_sample_norm %>% 
      mutate(y = .01),
    mapping = aes(x = PSE, y = y), 
    color = "#333333", size = 1.3) +
   annotate(
    geom = "text", 
    y = .02, x = 65,
    label = paste(round(post_sample_norm[[2]]), "ms", "-", round(post_sample_norm[[3]]), "ms"),
    size = 1.8) +
  geom_rug(
    data = d.test.excluded %>% 
      ungroup() %>% 
      distinct(Item.VOT),
    mapping = aes(x = Item.VOT),
    colour = "grey",
    alpha = .6,
    inherit.aes = F)
}  
```


```{r, fig.width=3.7, fig.height=2.5}
talker.VOT.no.noise <- 
  get_IO_categorization(cues = c("VOT"), 
                        groups = c("Talker", "gender"), 
                        with_noise = FALSE, 
                        alpha = .1, 
                        size = 0.3,
                        io.type = "talker.VOT.no.noise")

PSE_VOT.no.noise <- 
  talker.VOT.no.noise %>%
  group_by(gender) %>%
  summarise(
    PSE.lower = round(quantile(PSE, probs = c(.025))),
    PSE.median = round(quantile(PSE, probs = c(.5))),
    PSE.upper = round(quantile(PSE, probs = c(.975))))

p.IOs.VOT.no.noise <- plot_IO_fit(
  data = talker.VOT.no.noise,
  PSEs = PSE_VOT.no.noise
)
```


```{r, fig.width=3.7, fig.height=2.5}
# get IOs from VOT only
talker.VOT <- 
  get_IO_categorization(cues = c("VOT"), 
                        groups = c("Talker", "gender"), 
                        alpha = .1, 
                        size = 0.3,
                        io.type = "talker.VOT")

PSE_talker.VOT <- 
  talker.VOT %>%
  group_by(gender) %>%
  summarise(
    PSE.lower = round(quantile(PSE, probs = c(.025))),
    PSE.median = round(quantile(PSE, probs = c(.5))),
    PSE.upper = round(quantile(PSE, probs = c(.975))))

p.IOs.VOT <- plot_IO_fit(
  data = talker.VOT,
  PSEs = PSE_talker.VOT
)  
```


```{r, warning=FALSE, fig.width=3.7, fig.height=2.5}
# get IOs with multivariate cues
talker.VOT_F0 <- 
  get_IO_categorization(cues = c("VOT", "f0_Mel"), 
                        groups = c("Talker", "gender"), 
                        alpha = .1, 
                        size = .3,
                        io.type = "talker.VOT_F0") 

PSE_VOT_F0 <- 
  talker.VOT_F0 %>%
  group_by(gender) %>%
  summarise(
    PSE.lower = round(quantile(PSE, probs = c(.025))),
    PSE.median = round(quantile(PSE, probs = c(.5))),
    PSE.upper = round(quantile(PSE, probs = c(.975))))

p.IOs.VOT_F0 <- plot_IO_fit(
  data = talker.VOT_F0,
  PSEs = PSE_VOT_F0
  )
```


```{r, warning=FALSE, fig.width=3.7, fig.height=2.5}
# using centered cues
talker.VOT_F0.centered <- get_IO_categorization(
  cues = c("VOT_centered", "f0_Mel_centered"), 
  groups = c("Talker", "gender"), alpha = .1, size = .3,
  io.type = "talker.VOT_F0.centered") 

PSE_VOT_F0.centered <- talker.VOT_F0.centered %>%
  group_by(gender) %>%
  summarise(
    PSE.lower = round(quantile(PSE, probs = c(.025))),
    PSE.median = round(quantile(PSE, probs = c(.5))),
    PSE.upper = round(quantile(PSE, probs = c(.975))))

p.IOs.VOT_F0.centered <- 
plot_IO_fit(
  data = talker.VOT_F0.centered,
  PSEs = PSE_VOT_F0.centered
)
```

(ref:comparative-IO-plot) Comparing predicted vs. observed categorization functions for Experiment 1. The black line and interval show the psychometric fit and 95% CI for Experiment 1 marginalizing over all random effects. Each thin line shows the prediction of a single talker-specific ideal observers derived from a database of word-initial stop productions [data: @chodroff2017structure; data preparation & model code: @Kurumada_Xie_Jaeger_2022]. The lapse rate and response bias for the ideal observers was set to match the MAP estimates of the psychometric model. For ease of comparisons, horizontal point ranges show the PSE and its 95% CI after discounting lapses.

```{r comparative-IO-plot, warning=FALSE, fig.width=6, fig.height=5, fig.cap="(ref:comparative-IO-plot)"}
p1 <- p.IOs.VOT.no.noise + 
  theme(legend.position = "none", 
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
p2 <- p.IOs.VOT + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
p3 <- p.IOs.VOT_F0 + 
  theme(legend.position = "none",
        axis.title.x = element_blank()) 
p4 <- p.IOs.VOT_F0.centered + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.ticks.y = element_blank())
  
legend <- get_legend(p.IOs.VOT_F0 + theme(legend.position = "top"))

IO_grid1 <- plot_grid(p1, p3, 
                   ncol = 1, 
                   align = "v", 
                   labels = c("A", "C"), 
                   label_size = 10, 
                   hjust = -7, 
                   vjust = 3,
                   rel_heights = c(0.90, 1))

IO_grid2 <- plot_grid(p2, p4, 
                   ncol = 1, 
                   align = "v", 
                   labels = c("B", "D"), 
                   label_size = 10, 
                   hjust = -2.5, 
                   vjust = 3,
                   rel_heights = c(0.90, 1))

IO_grid3 <- plot_grid(IO_grid1, IO_grid2, ncol = 2, align = "h", rel_widths = c(1.1, 1))


IO_grid4 <- plot_grid(legend, IO_grid3, align = "h", ncol = 1, rel_heights = c(.1, 1))
ggdraw(add_sub(IO_grid4, "VOT (ms)", size = 10, fontface = "bold"))
```
<!--TEMPORARY CHUNK: CHECKING LIKELIHOODS generated FROM get_IO_categorization function. This problem shows up for IO types based on a single cue. Increasing the step size doe not fix it. To avoid a PSE range estimate across talkers that extends to very high VOTs (e.g. 90ms and above), the VOT range for likelihood calculation must be limited to 85 ms or less.-->

```{r average-accuracy-of-IOs, fig.width=8.5, fig.height=6}
all_IOs <- rbind(talker.VOT.no.noise, talker.VOT, talker.VOT_F0, talker.VOT_F0.centered)

all_IOs %<>% 
  filter(PSE > 60) %>% 
  mutate(likelihood = 
           map(likelihood, ~ mutate(.x, log_like_diff = abs(.x[[2]] - .x[[3]])) %>% 
                 arrange(log_like_diff)))

all_IOs %>% 
  unnest(likelihood) %>% 
  ggplot(aes(x = VOT, y = log_like_diff)) +
  geom_line() +
  scale_y_continuous(limits = c(0, .08)) +
  facet_wrap(~Talker + io.type, labeller = label_wrap_gen(multi_line = F))
```




### Comparing ideal observer accuracies
Assess how well each of the four IOs fit human data. 
Get


```{r}
# Specify sequence of postive VOTs
test.VOTs.positive <- c(5, seq(15, 90, 5), seq(100, 130, 10))

# prepare data of IO for joining
d.IOs <- rbind(talker.VOT.no.noise, talker.VOT, talker.VOT_F0, talker.VOT_F0.centered) %>% 
  select(Talker, gender, io, io.type)

# We need to give the IOs that are built on 2 cues, the F0 values of the stimuli. T
d.f0.5ms <- read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv") %>% 
  filter(VOT %in% test.VOTs.positive) %>% 
  select(filename, VOT, f0_5ms_into_vowel) %>% 
  rename(Item.VOT = VOT,
         f0_5ms = f0_5ms_into_vowel,
         Item.Filename = filename) %>% 
  mutate(Item.Filename = paste0(Item.Filename, ".wav"))

d.temp <- d.test.excluded %>%
  ungroup() %>%
  select(Item.Filename, Item.VOT, Response, Response.Voicing) %>%
  filter(Item.VOT %in% test.VOTs.positive) %>%
  left_join(d.f0.5ms, by = c("Item.Filename", "Item.VOT")) %>% # this incorporates the F0 measurements that align better with C&W
  mutate(Item.F0.5ms = normMel(f0_5ms),
         VOT_F0 = map2(Item.VOT, Item.F0.5ms, ~ c(.x, .y)),
         Response.category = ifelse(Response.Voicing == "voiced", "/d/", "/t/"),
         Response.t = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  nest(d.perception = everything()) %>%
  crossing(d.IOs) %>%
  mutate(accuracy =
           pmap(list(d.perception, io, io.type), ~
                  get_average_accuracy_of_IO(observations = ifelse(str_detect(..3, "VOT_F0"), ..1$VOT_F0, ..1$Item.VOT), responses = ..1$Response.category, model = ..2)))
```













