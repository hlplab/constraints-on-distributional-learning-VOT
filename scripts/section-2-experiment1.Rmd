```{r, stand-alone-preamble}
require(tidyverse)
require(magrittr)

require(brms)
require(tidybayes)
require(MVBeliefUpdatr)
require(phonR)
require(cowplot)

source("functions.R")
```

# Experiment 1: Listener's expectations prior to informative exposure
Experiment 1 investigates native (L1) US English listeners' categorization of word-initial stop voicing by an unfamiliar female L1 US English talker, prior to more informative exposure. Specifically, listeners heard isolated recordings from a /d/-/t/ continuum, and had to respond which word they  heard (e.g., "din" or "tin"). The recordings varied in voice onset time (VOT), the primary phonetic cue to word-initial stop voicing in L1 US English, as well as correlated secondary cues (f0 and rhyme duration). Critically, exposure was relatively uninformative about the talker's use of the phonetic cues in that all phonetic realizations occurred equally often. The design of Experiment 1 serves two  goals.

The first goal is methodological. We use Experiment 1 to test basic assumptions about the paradigm and stimuli we employ in the remainder of this study. We obtain estimates of the category boundary between /d/ and /t/ *for the specific stimuli used in Experiment 2*, as perceived by *the type of listeners we seek to recruit for Experiment 2*. We also test whether prolonged testing across the phonetic continuum changes listeners' categorization behavior. Previous work has found that prolonged testing on uniform distributions can reduce the effects of previous exposure [e.g., @mitterer2011; @liu-jaeger2018], at least in listeners of the age group we recruit from [@scharenborg-janse2013]. However, these studies employed only a small number of 5-7 perceptually highly ambiguous stimuli, each repeated many times. In Experiment 1, we employ a much larger set of stimuli that span the entire continuum from very clear /d/s to very clear /t/s, each presented only twice. If prolonged testing changes listeners' responses, this has to be taken into account in the design of Experiment 2.

The second purpose of Experiment 1 is to introduce and illustrate relevant theory. We compare different models of listeners' prior expectations against listeners' categorization responses in Experiment 1. The different models all aim to capture the implicit expectations of an L1 adult listener of US English might have about the mapping from acoustic cues to /d/ and /t/ based on previously experienced speech input. As we describe in more detail after the presentation of the experiment, the models differ, however, in whether these prior expectations take into account that talkers can differ in the way they realize /d/ and /t/. This ability to take into account talker differences even prior to more informative exposure is predicted---though through qualitatively different mechanisms, as we discuss below---both by normalization accounts [@cole2010; @mcmurray2011information] and by accounts that attribute adaptive speech perception to changes in category representations [Bayesian ideal adaptor theory, @kleinschmidt2015robust; EARSHOT, @magnuson2020earshot; episodic theory, @goldinger1998echoes; exemplar theory, @johnson1997talker; @pierrehumbert2001]. It is, however, unexpected under accounts that attribute adaptive speech perception solely to ad-hoc changes in decision-making. We did not expect that Experiment 1 yields a decisive conclusion with regard to this second goal, which is also addressed in Experiment 2. Rather, we use Experiment 1 as a presentationally convenient way to introduce some of the different models and provide readers with initial intuitions about what experiments of this type can and cannot achieve.

## Methods 
### Participants
Participants were recruited over Amazon's Mechanical Turk platform, and paid $2.50 each (for a targeted remuneration of \$6/hour). The experiment was only visible to Mechanical Turk participants who (1) had an IP address in the United States, (2) had an approval rating of 95% based on at least 50 previous assignments, and (3) had not previously participated in any experiment on stop voicing from our lab. 

24 L1 US English listeners (female = 9; mean age = 36.2 years; SD age = 9.2 years) completed the experiment. To be eligible, participants had to confirm that they (1) spent at least the first 10 years of their life in the US speaking only English, (2) were in a quiet place, and (3) wore in-ear or over-the-ears headphones that cost at least \$15. 

### Materials 
We recorded multiple tokens of four minimal word pairs ("dill"/"till", "dim"/"tim", "din"/"tin", and "dip"/"tip") from a 23-year-old, female L1 US English talker with a mid-Western accent. These recordings were used to create four natural-sounding minimal pair VOT continua (dill-till, dip-tip, din-tin, and dip-tip) using a Praat script [@winn2020manipulation]. The full procedure is described in the supplementary information (SI, \@ref(sec:SI-XXX)). The VOT continua ranged from -100ms VOT to +130ms VOT in 5ms steps. Experiment 1 employs 24 of these steps (-100, -50, -10, 5  `r paste0(seq(15, 90, 5), collapse = ", ")`, `r paste0(seq(100, 130, 10), collapse = ", ")`). VOT tokens in the lower and upper ends were distributed over larger increments because stimuli in those ranges were expected to elicit floor and ceiling effects, respectively. 

We further set the F0 at vowel onset to follow the speaker's natural correlation which was estimated through a linear regression analysis of all the recorded speech tokens. We did this so that we could determine the approximate corresponding f0 values at each VOT value along the continua as predicted by this talker's VOT. The duration of the vowel was set to follow the natural trade-off relation with VOT reported in @allen1999effects. This approach closely resembles that taken in @theodore2019distributional, and resulted in continuum steps that sound highly natural [unlike the robotic-sounding stimuli employed in @clayards2008perception; @kleinschmidt2016you]. All stimuli are available as part of the OSF repository for this article.

In addition to the critical minimal pair continua we also recorded three words that did not did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used as catch trials. Stimulus intensity was set to 70 dB sound pressure level for all recordings. 

### Procedure
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu/FILLIN-FULL-URL). The first page of the experiment informed participants of their rights and the requirements for the experiment: that they had to be native listeners of English, wear headphones for the entire duration of the experiment, and be in a quiet room without distractions. Participants had to pass a headphone test, and were asked to keep the volume unchanged throughout the experiment. Participants could only advance to the start of the experiment by acknowledging each requirement and consenting to the guidelines of the Research Subjects Review Board of the University of Rochester. 

On the next page, participants were informed about the task for the remainder of the experiment. They were informed that they would heard a female talker speak a single word on each trial, and had to select which word they heard. Participants were instructed to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. This was done to encourage their full attention.

Each trial started with a dark-shaded green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen, as shown in Figure \@ref(fig:exp1-example-trial). At 1000ms from trial onset, the fixation dot would turn bright green and an audio recording from the matching minimal pair continuum started playing. Participants were required to click on the word they heard. For each participant, /d/-initial words were either always displayed on the left side or always displayed on the right side. Across participants, this ordering was counter-balanced. After participants clicked on the word, the next trial began.

```{r exp1-example-trial, fig.cap="Example trial display. The words were displayed 500ms after trial onset and the audio recording of the word was played 1000ms after trial onset"}
knitr::include_graphics("../figures/exp1_trial_example.png")
```

Participants heard 192 target trials (four minimal pair continua, each with 24 VOT steps, each heard twice). In addition, participants heard 12 catch trials. On catch trials, participant saw two written catch stimuli on the screen (e.g., "flare" and "rare"), and heard one of them (e.g. "rare"). Since these recordings were easily distinguishable, they served as a check on participant attention throughout the experiment. 

The order of trials was randomized for each participant with the only constraint that no stimulus was repeated before each stimulus had been heard at least once. Catch trials were distributed randomly throughout the experiment with the constraint that no more than two catch trials would occur in a row. <!-- TO DO: correct? --> Participants were given the opportunity to take breaks after every 60 trials. Participants took an average of 12 minutes (SD = 4.8) to complete the 204 trials, after which they answered a short survey about the experiment. 

```{r, echo=FALSE}
# load formatted dataframe from experiment 1
d.test <- read_csv("../data/d.test.Exp1.csv", show_col_types = F)

# load f0-5ms-into-vowel measurements of stimuli
d.f0.5ms <- 
  read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv", show_col_types = F) %>% 
  select(filename, VOT, f0_5ms_into_vowel) %>% 
  rename(Item.VOT = VOT,
         Item.f0_5ms = f0_5ms_into_vowel,
         Item.Filename = filename) %>% 
  mutate(Item.Filename = paste0(Item.Filename, ".wav"))

d.test %<>% 
ungroup() %>%
  left_join(d.f0.5ms, by = c("Item.Filename", "Item.VOT")) %>% 
              mutate(Item.Mel_f0_5ms = normMel(Item.f0_5ms))

# mark catch trials rows and check correct
d.test %<>% 
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == TRUE & Item.MinimalPair == Response, TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == FALSE, NA, CatchTrial.Correct)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Excluded.due.to.CatchTrial = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 10, TRUE, FALSE))

# get the rows removed due to catch trial performance
excluded.data.due.to.catch <- 
  d.test %>% 
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == TRUE)

# set RT exclusion criteria and create excluded columns
d.test %<>%
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == FALSE) %>% 
  mutate(Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
         # scale the log RTs
         Response.log_RT.scaled = scale(Response.log_RT), 
         # this gives each subject's mean log_RT
         Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(Excluded.participant.due.to.mean.RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE))
         # Get participants whose means are more than 3x sd of mean of participant means. Mean of means is the same as mean of all rows because each participant has the same number of rows.
         

excluded.participants.due.to.mean.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE)

d.test %<>% 
  filter(Excluded.due.to.CatchTrial == FALSE & Excluded.participant.due.to.mean.RT == FALSE) %>% 
  group_by(ParticipantID) %>% 
  mutate(Excluded.trial.due.to.RT = ifelse(abs(Response.log_RT.scaled) > 3, TRUE, FALSE))
  
# get the rows removed due to RT
excluded.data.due.to.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE | Excluded.trial.due.to.RT == TRUE) 

# get the proportion of rows excluded from analysis
proportion.excluded <- (nrow(excluded.data.due.to.RT)) / (nrow(d.test))

# make a dataframe after exclusion to be used for analysis
d.test.excluded <- 
  d.test %>% 
  filter(
    Is.CatchTrial == FALSE, 
    Excluded.due.to.CatchTrial == FALSE, 
    Excluded.trial.due.to.RT == FALSE) %>% 
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0))
```


### Exclusions
We excluded from analysis participants who committed more than 2 errors out of the 12 catch trials (<83% accuracy, N = 3), participants with an average reaction time (RT) more than three standard deviations from the mean of the by-participant means (N = 0), and participants who reported not to have used headphones (N = 0) or not to be native (L1) speakers of US English (N = 0). For the remaining participants, trials that were more than three SDs from the participant's mean RT were excluded from analysis (`r proportion.excluded * 100 `%). Finally, we excluded participants (N = 0) who had less than 50% data remaining after these exclusions. 


## Behavioral results
We first present the behavioral analyses of participants' categorisation responses. Then we compare participants' responses to the predictions of different models fit on the distribution of stop voicing cues in a large database of L1 US English productions of word-initial /d/s and /t/s [@chodroff-wilson2018].

### Analysis approach
The goal of our behavioral analyses was to address three methodological questions that are of relevance to Experiment 2: (1) whether our stimuli resulted in 'reasonable' categorisation functions, (2) whether these functions differed between the four minimal pair items, and (3) whether participants' categorisation functions changed throughout the 192 test trials.

To address these questions, we fit a single Bayesian mixed-effects psychometric model to participants' categorization responses on critical trials [e.g., @prins2011]. This model is essentially an extension of mixed-effects logistic regression that also takes into account attentional lapses. A failure to do so---while commonplace in research on speech perception [incl. our own work, but see @clayards2008; @kleinschmidt2016you]---can lead to biased estimates of categorization boundaries [e.g., @wichman-hill2001]. The mixed-effects psychometric model describes the probability of "t"-responses as a weighted mixture of a lapsing-model and a perceptual model. The lapsing model is a mixed-effects logistic regression [@jaeger2008categorical] that predicts participant responses that are made independent of the stimulus---for example, responses that result from attentional lapses. These responses are independent of the stimulus, and depend only on participants' response bias. The perceptual model is a mixed-effects logistic regression that predicts all other responses, and captures stimulus-dependent aspects of participants' responses. The relative weight of the two models is determined by the lapse rate, which is described by a third mixed-effects logistic regression.

The *lapsing model* only contained an intercept (the response bias in log-odds) and by-participant random intercepts. Similarly, the *model for the lapse rate* only had an intercept (the lapse rate) and by-participants random intercepts. No by-item random effects were included for the lapse rate nor lapsing model since these parts of the analysis---by definition---describe stimulus-*in*dependent behavior. The *perceptual model* included an intercept and VOT, as well as the full random effect structure by participants and items (the four minimal pair continua), including random intercepts and random slopes by participant and minimal pair. We did not model the random effects of trial to reduce model complexity. This potentially makes our analysis of trials in the model anti-conservative. Finally, the models included the covariance between by-participant random effects across the three linear predictors for the lapsing model, lapse rate model, and perceptual model. This allows us to capture whether participants who lapse more often have, for example, different response biases or different sensitivity to VOT (after accounting for lapsing).

We fit the model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. Following previous work from our lab [@horberg2021rational; @xie2021cross], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008standardize], and used Student priors centered around zero with a scale of 2.5 units [following @gelman2008weakly] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $\hat{R}$ were close to 1.

### Expectations 
Based on previous experiments, we expected a strong positive effect of VOT, with increasing proportions of "t"-responses for increasing VOTs. We did not have clear expectations for the effect of trial other than that responses should become more uniformed (i.e move towards 50-50 "d"/"t"-bias or 0-log-odds) as the experiment progressed [@liu2018inferring] due to the un-informativeness of the stimuli. 
Previous studies with similar paradigms have typically found lapse rates of 0-10% [< -2.2 log-odds, e.g., @clayards2008perception; @kleinschmidt2016you]. 

```{r}
# set the mean and SD values for scaling/unscaling purposes
VOT.mean_norm <- mean(d.test.excluded$Item.VOT)
VOT.sd_norm <- sd(d.test.excluded$Item.VOT)
Trial.mean <- mean(d.test.excluded$Trial) 
Trial.sd <- sd(d.test.excluded$Trial)

d.test.excluded %<>%
  mutate(
    sVOT = (Item.VOT - VOT.mean_norm) / (2 * VOT.sd_norm),
    sTrial = (Trial - Trial.mean) / (2 * Trial.sd))

# set priors for psychometric model
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"), # prior 
  prior(student_t(3, 0, 2.5), class = "b", dpar = "theta1"), 
  prior(cauchy(0, 2.5), class = "sd"),
  prior(lkj(1), class = "cor")
)

# load (or run) the psychometric model with interaction
fit_mix <- brm(
  bf(
    Response.Voicing == "voiceless" ~ 1,
    mu1 ~ 1 + (1 | g | ParticipantID),
    mu2 ~ 1 + sVOT * sTrial + (1 + sVOT | g | ParticipantID) + (1 + sVOT | h | Item.MinimalPair),
    theta1 ~ 1 + (1 | g | ParticipantID)),
  data = d.test.excluded,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  family = mixture(bernoulli("logit"), bernoulli("logit")),
  control = list(adapt_delta = .99),
  file = "../models/Exp-NORM-lapsing-bias-GLMM")

# get conditional effects from model
psychometric_fit_data <- 
  conditional_effects(
    fit_mix, 
    effects = "sVOT", 
    plot = F)[[1]]

if (file.exists("../models/conditional_effects_VOT_Trial.rds")) {
  psychometric_fit_interaction <- read_rds("../models/conditional_effects_VOT_Trial.rds")
} else {
  int_conditions <- list(sTrial = sort(unique((d.test.excluded$Trial - Trial.mean)) / (2 * Trial.sd)))
  
  psychometric_fit_interaction <-
    conditional_effects(
      fit_mix,
      effects = "sVOT:sTrial",
      int_conditions = int_conditions,
      plot = F)[[1]]
  
  write_rds(psychometric_fit_interaction, file = "../models/conditional_effects_VOT_Trial.rds")
}

# get the PSE from the fitted categorisation function
PSE.fit_mix <- descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_norm, VOT.sd_norm) 

# get posterior samples of intercept and slope, and median qi of the PSEs
post_sample_norm <- fit_mix %>% 
  spread_draws(b_mu2_Intercept, b_mu2_sVOT) %>% 
  mutate(PSE = descale(-(b_mu2_Intercept/b_mu2_sVOT), VOT.mean_norm, VOT.sd_norm)) %>% 
  median_qi(PSE)
```



```{r}
newdata <- expand_grid(
  sVOT = (seq(-100, 130, 1) - VOT.mean_norm) / (2 * VOT.sd_norm),
  Item.MinimalPair = levels(factor(d.test.excluded$Item.MinimalPair)),
  ParticipantID = levels(factor(d.test.excluded$ParticipantID)),
  sTrial = 0)

if (file.exists("../models/categorisation_by_min_pair.rds")) {
  cat_minimalpair <- read_rds("../models/categorisation_by_min_pair.rds")
} else {
  cat_minimalpair <- fit_mix %>%
    epred_draws(
      newdata = newdata,
      ndraws = 1000,
      re_formula = ~ (1 + sVOT | Item.MinimalPair))
  write_rds(cat_minimalpair, file = "../models/categorisation_by_min_pair.rds")
}
```

```{r fitted-categorisation-minimal-pair"}
MinPair_plot <- 
  cat_minimalpair %>% 
  group_by(sVOT, Item.MinimalPair) %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = .epred, colour = Item.MinimalPair)) +
  stat_lineribbon(alpha = .9, .width = 0.95) +
  scale_x_continuous("VOT (ms)") +
  scale_y_continuous("Fitted proportion of 't'-responses") +
  scale_color_manual("Minimal Pair", breaks = c("dilltill", "dimtim", "dintin", "diptip"),  
                     values = c("#002699", "#0040ff", "#668cff", "#b3c6ff"), 
                     labels = c("dill/till", "dim/tim", "din/tin", "dip/tip")) +
  scale_fill_brewer("CI", palette = "Greys", type = "qual") +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r psychometric-plot, fig.width=3.8, fig.height =2.6, message=FALSE}
p.vot <- 
  psychometric_fit_data %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = estimate__)) +
  scale_x_continuous("VOT (ms)", breaks = scales::pretty_breaks(n = 4), limits = c(-100, 130)) +
  scale_y_continuous("Fitted proportion of 't' responses") +
  geom_ribbon(
    aes(ymin = lower__, 
        ymax = upper__), alpha = .08) +
  geom_line(linewidth = 1.5, 
            colour = "#333333",
            alpha = .8) +
  geom_errorbarh(
    data = post_sample_norm %>% 
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y), 
    color = "#333333",
    height = 0,
    alpha = .5,
    size = 1, 
    inherit.aes = F) +
  geom_point(
    data = post_sample_norm %>% 
      median_qi(PSE) %>% 
      mutate(y = 0.01),
    mapping = aes(x = PSE, y = y), 
    color = "#333333", 
    size = 1.3,
    alpha = .5) +
  annotate(
    geom = "text",
    x = 75,
    y = 0.01, 
    label = paste(round(post_sample_norm[[2]]), "ms", "-", round(post_sample_norm[[3]]), "ms"),
    size = 2.5,
    colour = "darkgray") +
## transformation of by-participant means into empirical logits
   # stat_summary(
   #  data = d.test.excluded %>% 
   #    group_by(ParticipantID, Item.VOT) %>% 
   #    mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
   #    summarise(Response.EmpiricalLogitVoiceless = qlogis((sum(Response.ProportionVoiceless) + .5)/(length(Response.ProportionVoiceless) + 1))), 
   #  mapping = aes(x = Item.VOT, 
   #                y = Response.EmpiricalLogitVoiceless),
   #  geom = "pointrange",
   #  fun.data = function(x){mean_cl_boot(x) %>% mutate(across(c(y, ymin, ymax), ~ plogis(.x)))},
   #  size = .4,
   #  colour = "#c4b7a6",
   #  alpha = .6,
   #  inherit.aes = F) 
  stat_summary(
    data = d.test.excluded %>% 
      group_by(ParticipantID, Item.VOT) %>% 
      mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
      summarise(Response.ProportionVoiceless = mean(Response.ProportionVoiceless)), 
    mapping = aes(x = Item.VOT, 
                  y = Response.ProportionVoiceless),
    geom = "pointrange",
    fun.data = "mean_cl_boot",
    size = .4,
    colour = "#c4b7a6",
    alpha = .6,
    inherit.aes = F)
```

(ref:fitted-categorisation-plots) Categorisation functions and points of subjective equality (PSE) derived from the Bayesian mixed-effects psychometric model fit to listeners' responses in Experiment 1. The categorization functions include lapse rates and biases. The PSEs correct for lapse rates and lapse biases (i.e., they are the PSEs of the perceptual component of the psychometric model).\protect\footnote{Here and in Experiment 2, lapse biases were close to uniform (.5/.5). For such scenarios, bias-corrected PSEs will be very similar to uncorrected PSEs. This is also evident in Figure \ref{fig:fitted-categorisation-plots}.} **Left:** Effects of VOT, lapse rate, and lapse bias, while marginalizing over trial effects as well as all random effects. Vertical point ranges represent the mean proportion and 95% bootstrapped CIs of participants' "t"-responses at each VOT step. Horizontal point ranges denote the mean and 95% quantile interval of the points of subjective equality (PSE), derived from the 8000 posterior samples of the population parameters. **Right:** The same but showing the fitted categorization functions for each of the four minimal pair continua. Participants' responses are omitted to avoid clutter.   

```{r fitted-categorisation-plots, warning=FALSE, fig.width=6, fig.height = 3.3, fig.cap="(ref:fitted-categorisation-plots)"}
legend1 <- get_legend(MinPair_plot + theme(legend.position = "top"))

MinPair_plot <- MinPair_plot + theme(legend.position = "none")

grid1 <- 
  plot_grid(
    p.vot + theme(axis.title.x = element_blank()),
    MinPair_plot,
    ncol = 2, 
    align = "h", 
    labels = NULL, 
    label_size = 10,
    hjust = .1, 
    rel_widths = c(0.95, 0.8))

grid2 <- 
  plot_grid(
    legend1, 
    grid1,
    ncol = 1,
    rel_heights = c(0.1, 1))

ggdraw(add_sub(grid2, "VOT (ms)", size = 10, fontface = "bold"))
```


```{r by-participant-lapse-bias, warning=FALSE, fig.height=2.6, fig.width=6.5}
lapse_participant <- fit_mix %>% spread_draws(r_ParticipantID__theta1[ParticipantID, term], b_theta1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_lapse = b_theta1_Intercept + r_ParticipantID__theta1,
         estimated_lapse = plogis(Participant_lapse) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__theta1, Participant_lapse, estimated_lapse) %>%
  mode_hdci(estimated_lapse)

bias_participant <- fit_mix %>%
  spread_draws(r_ParticipantID__mu1[ParticipantID, term], b_mu1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_bias = b_mu1_Intercept + r_ParticipantID__mu1,
         estimated_bias = plogis(Participant_bias) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__mu1, Participant_bias, estimated_bias) %>%
  mode_hdci(estimated_bias)

estimate_minpair <- fit_mix %>% spread_draws(r_Item.MinimalPair__mu2[Item.MinimalPair, term], b_mu2_Intercept, b_mu2_sVOT) %>%
  group_by(Item.MinimalPair) %>%
  mutate(predicted_eff = ifelse(term == "Intercept", r_Item.MinimalPair__mu2 + b_mu2_Intercept, r_Item.MinimalPair__mu2 + b_mu2_sVOT)) %>%
  group_by(Item.MinimalPair, term) %>%
  mode_hdci(predicted_eff) %>%
  pivot_wider(names_from = term,
              values_from = c(predicted_eff, .lower, .upper))
```



The lapse rate was estimated to be on the slightly larger side, but within the expected range 
(`r make_CI(fit_mix, "theta1_Intercept", "theta1_Intercept < 0")`). Maximum a posteriori (MAP) estimates of by-participant lapse rates ranged from XX . Very high lapse rates were estimated for four of the participants with one in particular whose CI indicated exceptionally high uncertainty. These lapse rates might reflect data quality issues with Mechanical Turk that started to emerge over recent years [see @REFS; and, specifically for experiments on speech perception, @cummings2023], and we return to this issue in Experiment 2. 

The response bias were estimated to slightly favor "t"-responses (`r make_CI(fit_mix, "mu1_Intercept", "mu1_Intercept > 0")`), as also visible in Figure \@ref(fig:fitted-categorisation-plots) (left).
Unsurprisingly, the psychometric model suggests high uncertainty about the participant-specific response biases, as it is difficult to reliably estimate participant-specific biases while also accounting for trial and VOT effects (range of by-participant MAP estimates: XX). For all but four participants, the 95% CI includes the hypothesis that responses were unbiased. Of the remaining four participants, three were biased towards "t"-responses and one was biased toward "d"-responses.

There was no convincing evidence of a main effect of trial ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sTrial", "mu2_sTrial < 0")`). Given the slight overall bias towards "t"-responses, the direction of this effect indicates that participants converged towards a 50/50 bias as the test phase proceeded. This is also evident in Figure \@ref(fig:fitted-categorisation-plots) (right). In contrast, there was clear evidence for a positive main effect of VOT on the proportion of "t"-responses ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT", "mu2_sVOT > 0")`). The effect of VOT was consistent across all minimal pair words as evident from the slopes of the fitted lines by minimal pair \@ref(fig:fitted-categorisation-plots) (left). MAP estimates of by minimal pair slopes ranged from . The by minimal-pair intercepts were more varied (MAP estimates: ) with one of the pairs, dim/tim having a slightly lower intercept resulting in fewer 't'-responses on average. In all, this justifies our assumptions that word pair would not have a substantial effect on categorisation behaviour. From the parameter estimates of the overall fit we obtained the category boundary from the point of subjective equality (PSE) (`r descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_norm, VOT.sd_norm)`ms) which we use for the design of Experiment 2.

Finally to accomplish the first goal of experiment 1, we look at the interaction between VOT and trial. There was weak evidence that the effect of VOT decreased across trials ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT:sTrial", "mu2_sVOT:sTrial < 0")`). The direction of this change---towards more shallow VOT slopes as the experiment progressed---makes sense since the test stimuli were not informative about the talker's pronunciation. Similar changes throughout prolonged testing have been reported in previous work. [@liu-jaeger2018; @liu-jaeger2019; @REFS].

Overall, there was little evidence that participants substantially changed their categorisation behaviour as the experiment progressed. Still, to err on the cautious side, Experiment 2 employs shorter test phases.

## Comparisons to model of adaptive speech perception
We now turn to final aim of experiment 1 which is to make use of computational models to begin to understand the implicit expectations that listeners hold when perceiving input that is uninformative of a talker's cue-to-category-mappings. 

Speakers' productions can act as a proxy for listeners' implicit knowledge of the distributional patterns of cues. This production-perception relationship within a phonological system was observed in early work by [@abramson1973voice] who found that production statistics of talkers along VOT aligned well with data from listeners who had categorised a separate set of synthesised VOT stimuli. This allows for the use of analytic models as tools for predicting categorisation behaviour from speech production data [@nearey1986phonological]. 

We apply this principle when fitting ideal observer (IO) models by linking the distributional patterns of talker productions to the categorisation behaviour of listeners.  All models were trained on cue measurements extracted from an annotated database of 92 L1 US-English talkers' productions [@chodroff2017structure] of word initial /d/ and /t/. By using IOs trained solely on production data to predict behaviour we avoid additional computational degrees of freedom and limit the risk of overfitting the model to the data. 

The IOs' predictions apply Bayes' theorem to achieve optimal categorization; the posterior probability of recognising a token as the "t" category is function of its prior prior probability $p(c = t)$ and the probability of observing the token under the hypothesis that the talker intended the voiceless category $p(cue | c = t)$ taken as a proportion of the sum of probabilities of observing the token under all possible hypotheses.


We compare listener categorisation behaviour against the predictions of five IO models which reflect different assumptions about perceptual processes and the normalization (or lack thereof) of input. Beginning with a minimal model (raw VOT cues with no added perceptual noise), each successive model increased in complexity either with the addition the F0 cue or an assumption about speech encoding (Figure \@ref(fig:comparative-IO-plot)). All IO models were adjusted by the estimated lapse-rate from the psychometric fit to the perceptual data while bias was held at .5. In models that included perceptual noise we added a noise variance of 80ms [cf. @kronrod2016unified] to the likelihoods. In addition to transforming the F0 cue measurements from raw Hz into Mel [@stevens1940relation] to reflect the tonotopy of the auditory system, normalization was applied to cues to compare effects of hypothesised pre-linguistic processes. We applied C-CuRE (@mcmurray2011information; @toscano2015time), a general purpose normalization procedure which captures the hypothesis that listeners overcome multiple sources of variability by interpreting cues relative to the expected distribution of cues given the present context. While C-CuRE has the potential to be applied in various ways depending on the context to be evaluated, we implemented it in its most basic form, which is to center the cues-- here VOT and F0 -- relative to the talker population means across categories. In the final model we extended this centering process to the cues in the exposure stimuli. This additional step fully implements the assumption of pre-linguistic normalization being an automatic process.

Each of these models are then assessed for their goodness-of-fit to the categorisation data by comparing the likelihood of human responses under the assumptions represented by the respective IO models (Figure \@ref(fig:comparative-IO-plot)). For this we applied Luce's choice axiom [@luce1959]; for each token categorised by each listener, the expected accuracy for that token is the model's posterior for the category selected by each listener. We took the average log posterior of all responses to get the average likelihood for the entire experiment under each model.

The first point that stands out from the visual comparisons is that models that incorporate perceptual noise fit the perceptual data better than those that do not. This itself indicates that perception of acoustic stimuli is not entirely faithful to the bottom-up signal but is inferred through a combination of what listeners actually perceived and their existing knowledge of the underlying linguistic category [@kronrod2016unified]. For the univariate VOT models, the difference is most noticeable from the flatter slopes of the IOs indicating greater uncertainty in listener categorisations. The second pattern is that models trained with VOT and F0 cues (multiple cues) are better fits overall than models trained on a single cue. This trend is expected given the literature that report F0 reliably covarying with the voicing of stop consonants [@house1953influence; @ohde1984fundamental]. When VOT fails to provide sufficient support to voicing status, F0 has been found to influence listeners' categorisation behaviour [@abramson1985relative; @whalen1993f; @idemaru2011word; @winn2013 @burchilljaeger2023]. This further speaks to the advantage of multivariate ideal observers because they assess the likelihood of a cue observation under a given category relative to the joint distributions of all relevant cues. 



```{r}
# prepare production corpus from Chodroff & Wilson
d.chodroff_wilson <-
  get_ChodroffWilson_data(
    database_filename = "../data/all_observations_with_non-missing_vot_cog_f0.csv",
    min.n_per_talker_and_stop = 25,
    limits.VOT = c(-Inf, Inf),
    limits.f0 = c(0, 350),
    max.p_for_multimodality = .1
  ) %>%
  mutate_at(
    c("VOT", "f0_Mel"),
    list("centered" = function(x) apply_ccure(x, data = .)))

d.chodroff_wilson.selected <-
  d.chodroff_wilson %>%
  filter(poa == "/d/-/t/") %>%
  group_by(Talker, category) %>%
  mutate(n = n()) %>%
  group_by(Talker) %>%
  # subsample n tokens, as determined by category with fewer tokens
  mutate(
    n_min = min(n),
    n_category = n_distinct(category)) %>%
  # select talkers with both /d/ and /t/ observations
  filter(n_category == 2) %>%
  group_by(Talker, category) %>%
  sample_n(size = first(n_min)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .))) %>% 
  mutate(category = factor(category))
```
```{r}
# run an alternative model with f0 as a predictor
# get f0 mean and sd for scaling
F0.mean <- mean(d.test.excluded$Item.Mel_f0_5ms)
F0.sd <- sd(d.test.excluded$Item.Mel_f0_5ms)
d.test.excluded %<>% 
  mutate(sF0 = (Item.Mel_f0_5ms - F0.mean)/(2 * F0.sd))

# specify new model formula (priors will be same as previous model without f0)
fit_mix_f0 <- brm(
  bf(
    Response.Voicing == "voiceless" ~ 1,
    mu1 ~ 1 + (1 | g | ParticipantID),
    mu2 ~ 1 + sVOT + sF0 + (1 + sVOT + sF0 | g | ParticipantID) + (1 + sVOT + sF0 | h | Item.MinimalPair),
    theta1 ~ 1 + (1 | g | ParticipantID)),
  data = d.test.excluded,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  family = mixture(bernoulli("logit"), bernoulli("logit")),
  control = list(adapt_delta = .99),
  file = "../models/Exp-NORM-lapsing-bias-f0")

summary(fit_mix)
summary(fit_mix_f0)  

psychometric_fit_data %>% 
     ggplot(aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = estimate__), linetype = 1, linewidth = 1.5) +
    geom_ribbon(
    data = conditional_effects(
    fit_mix_f0, 
    effects = c("sF0"), 
    plot = F)[[1]], 
    mapping = aes(ymin = lower__, 
             ymax = upper__),
    alpha = 0.08,
    inherit.aes = T) +
  geom_line(linewidth = 1.5, 
            colour = "#333333",
            alpha = .2) +
    geom_line(
      data = conditional_effects(
    fit_mix_f0, 
    effects = c("sVOT"), 
    plot = F)[[1]], 
      mapping = aes(x = descale(sVOT, VOT.mean_norm, VOT.sd_norm), 
             y = estimate__), 
      linetype = 2, 
      colour = "red",
      alpha = .5,
      linewidth = 1.5,
      inherit.aes = F
    ) + 
  scale_x_continuous("VOT (ms)", breaks = scales::pretty_breaks(n = 4), limits = c(-100, 130)) +
  scale_y_continuous("Fitted proportion of 't' responses") 


conditional_effects(
    fit_mix_f0, 
    effects = "sF0", 
    plot = F)[[1]] %>% 
     ggplot(aes(x = descale(sF0, F0.mean, F0.sd), 
             y = estimate__), linetype = 1, linewidth = 1.5) + 
  geom_line() +
  scale_x_continuous("F0", breaks = scales::pretty_breaks(n = 8)) +
  scale_y_continuous("Fitted proportion of 't' responses", limits = c(0, 1))

```




```{r, fig.width=3.7, fig.height=2.5}
VOT.no.noise <- 
  get_IO_categorization(
    cues = c("VOT"), 
    groups = c("Talker", "gender"),
    lapse_rate = plogis(summary(fit_mix)$fixed[3, 1]),
    with_noise = FALSE, 
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT.no.noise") 

PSE_VOT.no.noise <- get_PSE_quantiles(data = VOT.no.noise, group = "gender")

p.IOs.VOT.no.noise <- plot_IO_fit(
  data.production = VOT.no.noise, 
  data.perception = psychometric_fit_data %>% 
    mutate(Item.VOT = descale(sVOT, VOT.mean_norm, VOT.sd_norm)),
  data.test = d.test.excluded %>% 
    ungroup() %>% distinct(Item.VOT),
  posterior.sample = post_sample_norm,
  PSE_VOT.no.noise
)

p.likelihoods.VOT <- plot_talker_UVGs(VOT.no.noise, d.test.excluded)
```


```{r, fig.width=3.7, fig.height=2.5}
# get IOs from VOT only
VOT <- 
  get_IO_categorization(
    cues = c("VOT"), 
    groups = c("Talker", "gender"), 
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT")

PSE_VOT <- get_PSE_quantiles(data = VOT, group = "gender")

p.IOs.VOT <- 
  plot_IO_fit(
    data = VOT,
    PSEs = PSE_VOT,
    x = descale(psychometric_fit_data$sVOT, VOT.mean_norm, VOT.sd_norm))  


p.likelihoods.VOT_noise <- plot_talker_UVGs(VOT, d.test.excluded, noise = TRUE)
```


```{r, warning=FALSE, fig.width=3.7, fig.height=2.5}
# get IOs with multivariate cues
VOT_F0 <- 
  get_IO_categorization(
    cues = c("VOT", "f0_Mel"), 
    groups = c("Talker", "gender"), 
    alpha = .1, 
    linewidth = .3,
    io.type = "VOT_F0") 

PSE_VOT_F0 <- get_PSE_quantiles(data = VOT_F0, group = "gender")

p.IOs.VOT_F0 <- 
  plot_IO_fit(
    data = VOT_F0,
    PSEs = PSE_VOT_F0,
    x = descale(psychometric_fit_data$sVOT, VOT.mean_norm, VOT.sd_norm))

p.likelihoods.VOT_F0 <- plot_talker_MVGs(cues = c("VOT", "f0_Mel"))
```


```{r, warning=FALSE, fig.width=3.7, fig.height=2.5}
# using centered cues
VOT_F0.centered <- 
  get_IO_categorization(
    cues = c("VOT_centered", "f0_Mel_centered"), 
    groups = c("Talker", "gender"), alpha = .1, linewidth = .3,
    io.type = "VOT_F0.centered") 

PSE_VOT_F0.centered <- get_PSE_quantiles(data = VOT_F0.centered, group = "gender")

p.IOs.VOT_F0.centered <- 
  plot_IO_fit(
    data = VOT_F0.centered,
    PSEs = PSE_VOT_F0.centered,
    x = descale(psychometric_fit_data$sVOT, VOT.mean_norm, VOT.sd_norm))

p.likelihoods.VOT_F0.centered <- plot_talker_MVGs(cues = c("VOT_centered", "f0_Mel_centered"))
```

```{r, warning=FALSE, fig.width=3.7, fig.height=2.5}
# prepare data for IO that centers perceptual input relative to population mean before categorization
chodroff.means <- d.chodroff_wilson.selected %>% 
  group_by(Talker) %>% 
  summarise(across(c(VOT, f0_Mel), mean)) %>% 
  ungroup() %>% 
  summarise(across(c(VOT, f0_Mel), mean)) 

chodroff.mean_VOT <- chodroff.means %>% pull(VOT)
chodroff.mean_f0 <- chodroff.means %>% pull(f0_Mel)

# center exposure data and
d.test.excluded %>% 
      mutate(Item.VOT = (Item.VOT + (chodroff.mean_VOT - VOT.mean_norm)),
             sVOT = (Item.VOT - VOT.mean_norm_centered)/(2 * VOT.sd_norm))

adjusted_line <- fit_mix %>% 
  epred_draws(
    newdata = d.test.excluded %>% 
      mutate(Item.VOT = (Item.VOT + (chodroff.mean_VOT - VOT.mean_norm)),
             sVOT = (Item.VOT - chodroff.mean_VOT)/(2 * VOT.sd_norm)),
    re_formula = NA,
    ndraws = 2000)

adjusted_line %>% 
  group_by(sVOT) %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_norm_centered, VOT.sd_norm_centered), 
             y = .epred)) +
  stat_lineribbon(alpha = .9) +
  scale_x_continuous("VOT (ms)") +
  scale_y_continuous("Fitted proportion of 't'-responses") +
  scale_fill_brewer("CI", palette = "Greys", type = "qual") 


p.IOs.VOT_F0.centered.input <- plot_IO_fit(
    data = VOT_F0.centered,
    PSEs = PSE_VOT_F0.centered,
    x = descale(psychometric_fit_data$sVOT, VOT.mean_norm, VOT.sd_norm) + (chodroff.mean_VOT - VOT.mean_norm),
    centered = TRUE)

p.likelihoods.exposure.centered <- plot_talker_MVGs(cues = c("VOT_centered", "f0_Mel_centered"), centered = T)
```





```{r}
# make plot grid
theme_spec <- theme(axis.title.x = element_blank(),
        axis.title.y = element_blank()) 
guide_spec <-  guides(linetype = "none")

p1 <- p.likelihoods.VOT +
  theme_spec + guide_spec

p2 <- p.likelihoods.VOT_noise +
   theme_spec + guide_spec

p3 <- p.likelihoods.VOT_F0 +
   theme_spec + guide_spec

p4 <- p.likelihoods.VOT_F0.centered +
   theme_spec + guide_spec

p5 <- p.likelihoods.exposure.centered +
  theme_spec

p6 <- p.IOs.VOT.no.noise + 
  theme_spec

p7 <- p.IOs.VOT + 
  theme_spec 

p8 <- p.IOs.VOT_F0 + 
  theme_spec 

p9 <- p.IOs.VOT_F0.centered + 
  theme_spec 

p10 <- p.IOs.VOT_F0.centered.input + 
  theme_spec  
```



(ref:comparative-IO-plot) **Right column:** Comparing predicted vs. observed categorization functions for Experiment 1. The black line and interval show the psychometric fit and 95% CI for Experiment 1 marginalizing over all random effects. Each thin line shows the prediction of a single talker-specific ideal observer derived from a database of word-initial stop productions [data: @chodroff2017structure; data preparation & model code: @Kurumada_Xie_Jaeger_2022]. The lapse rate and response bias for the ideal observers was set to match the MAP estimates of the psychometric model. For ease of comparisons, horizontal point ranges show the PSE and its 95% CI after discounting lapses. **Left column:** Gaussian distributions indicate talker likelihoods by category. Rug shows show exposure stimuli location along VOT and along VOT and F0 (points). Talker cues (second-to-last-row) and exposure stimuli (last row) are centered relative to overall cue means across categories.

```{r, comparative-IO-plot, warning=FALSE, fig.width=6, fig.height=9, fig.cap="(ref:comparative-IO-plot)"}

p.col1 <- p1 +p2 + p3 + p4 + p5 +
  plot_layout(ncol = 1, byrow = F, guides = "collect") &
  theme(legend.position = "top")  

p.col2 <- p6 + p7 + p8 + p9 + p10 +
  plot_layout(ncol = 1, byrow = F, guides = "collect") &
  theme(legend.position = "top") 

glob_lab1 <- "Density"
glob_lab2 <- "Proportion 't'-responses"

p_lab1 <- 
  ggplot() + 
  annotate(geom = "text", x = 1, y = 1, label = glob_lab1, angle = 90) +
  coord_cartesian(clip = "off") +
  theme_void()

p_lab2 <- 
  ggplot() + 
  annotate(geom = "text", x = 1, y = 1, label = glob_lab2, angle = 90) +
  coord_cartesian(clip = "off")+
  theme_void()

(p.col1 | p.col2) +
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "top")
```






```{r}
# Specify sequence of postive VOTs
test.VOTs.positive <- c(5, seq(15, 90, 5), seq(100, 130, 10))

# make VOT_F0_centered data set with centered perceptual stimuli for simulating centering of input
VOT_F0.centered.input <- VOT_F0.centered %>% 
  mutate(io.type = "VOT_F0.centered.input")

# join all 5 IOs into one dataframe
d.IOs <- 
  rbind(VOT.no.noise, VOT, VOT_F0, VOT_F0.centered, VOT_F0.centered.input) %>% 
  select(Talker, gender, io, io.type)

d.test.IO <- 
  d.test.excluded %>%
  ungroup() %>%
  select(Item.Filename, Item.VOT, Item.Mel_f0_5ms, Response, Response.Voicing) %>%
  filter(Item.VOT %in% test.VOTs.positive) %>%  # filter to only the +ve for the time being may need to include the -ve values
  mutate(
    VOT_F0 = map2(Item.VOT, Item.Mel_f0_5ms, ~ c(.x, .y)),
    Item.VOT_centered = Item.VOT + (chodroff.mean_VOT - VOT.mean_norm),
    Item.Mel_F0_centered = normMel(predict_f0(Item.VOT_centered)),
    VOT_F0_centered = map2(Item.VOT_centered, Item.Mel_F0_centered, ~ c(.x, .y)),
    Response.category = ifelse(Response.Voicing == "voiced", "/d/", "/t/"),
    Response.t = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  nest(d.perception = everything()) %>%
  crossing(d.IOs) 

d.test.IO %<>%
  mutate(
    log_likelihood_per_response =
      pmap(
        list(d.perception, io, io.type), 
        ~ get_average_log_likelihood_of_perception_data_under_IO(
          observations = if(str_detect(..3, "centered.input")) {..1$VOT_F0_centered}
          else if (str_detect(..3, "VOT_F0")) {..1$VOT_F0} 
          else {..1$Item.VOT}, 
          responses = ..1$Response.category, 
          model = ..2)) %>% 
      unlist(),
    likelihood_per_response = exp(log_likelihood_per_response))

IO.likelihood <- d.test.IO %>% 
  group_by(io.type) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response),
    mode_likelihood_per_response = max(likelihood_per_response, na.rm = T)
  )

# use bootstrap samples by io type to find the CI of the median
set.seed(356)
d.bootstrap <- d.test.IO %>% 
  droplevels() %>% 
  select(-c(d.perception, io)) %>% 
  mutate(io.type = factor(io.type)) %>% 
  # nest so that we can sample from groups of rows corresponding to each io type
  nest(data = -io.type) %>% 
  mutate(samples = 
           map(data, ~ bootstraps(.x, times = 100))) %>% 
  unnest(samples) %>% 
  mutate(splits = map(splits, ~ as_tibble(.x)),
         median_sample = map_dbl(splits, ~ median(.x$likelihood_per_response)),
         mode_sample = map_dbl(splits, ~ max(.x$likelihood_per_response)))
```




```{r, fig.width=6, fig.height=3, warning=FALSE}
# plot median likelihood per response with bootstrapped 95% CI
p.likelihood <- d.test.IO %>% 
  group_by(io.type) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response)) %>%
  mutate(io.type = fct_relevel(io.type, levels = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered",  "VOT_F0.centered.input"))) %>% 
  ggplot(aes(x = io.type, 
             y = median_likelihood_per_response)) +
  geom_point(size = 2, colour = "#333333") +
  geom_errorbar(
    data = d.bootstrap %>% 
      group_by(io.type) %>% 
      mutate(io.type = fct_relevel(io.type, levels = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered",  "VOT_F0.centered.input"))) %>% 
      summarise(
        median.lower = quantile(median_sample, probs = .025),
        median.upper = quantile(median_sample, probs = .975)
      ),
    mapping = 
      aes(x = io.type, ymin = median.lower, ymax = median.upper),
    colour = "#333333",
    linewidth = 1.5,
    alpha = .6,
    width = 0,
    inherit.aes = F) +
  scale_y_continuous("Median likelihood per response (log base 10)") +
   scale_x_discrete( 
     limits = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered", "VOT_F0.centered.input"),
     breaks = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered", "VOT_F0.centered.input"),
     labels = c(VOT.no.noise = "VOT", VOT = "VOT\n(+perceptual noise)", VOT_F0 = "VOT-F0\n", VOT_F0.centered = "VOT-F0\n (+talker centered)", VOT_F0.centered.input = "VOT-F0 \n (+exposure-centered)")) +
  theme(axis.title.x = element_blank(),
        legend.position = "none") +
  guides(x = guide_axis(angle = 45)) +
  coord_trans(y = "log10")
```


```{r, fig.width=4, fig.height=3, warning=FALSE}
# zoom in on multi-variate models
p.likelihood.multivar <- d.test.IO %>% 
  group_by(io.type) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response),
    mode_likelihood_per_response = max(likelihood_per_response)) %>%
  mutate(io.type = fct_relevel(io.type, levels = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered",  "VOT_F0.centered.input"))) %>%
  ggplot(aes(x = io.type, 
             y = median_likelihood_per_response)) +
  geom_point(size = 3,
             colour = "#333333") +
  geom_point(aes(x = io.type, 
                 y = mode_likelihood_per_response), 
             size = 3, 
             shape = 17,
             colour = "#333333") +
  geom_errorbar(
    data = d.bootstrap %>% 
      group_by(io.type) %>%
  mutate(io.type = fct_relevel(io.type, levels = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered",  "VOT_F0.centered.input"))) %>% 
      summarise(
        median.lower = quantile(median_sample, probs = .025),
        median.upper = quantile(median_sample, probs = .975)
      ),
    mapping = 
      aes(x = io.type, ymin = median.lower, ymax = median.upper),
    colour = "#333333",
    linewidth = 1.5,
    alpha = .6,
  width = 0,
    inherit.aes = F) + 
    geom_errorbar(
    data = d.bootstrap %>% 
      group_by(io.type) %>%
  mutate(io.type = fct_relevel(io.type, levels = c("VOT.no.noise", "VOT", "VOT_F0", "VOT_F0.centered",  "VOT_F0.centered.input"))) %>% 
      summarise(
        mode.lower = quantile(mode_sample, probs = .025),
        mode.upper = quantile(mode_sample, probs = .975)
      ),
    mapping = 
      aes(x = io.type, ymin = mode.lower, ymax = mode.upper),
    colour = "#333333",
    size = 1.5,
    alpha = .6,
  width = 0,
    inherit.aes = F) +
  scale_y_continuous("Median log-likelihood \n per response", limits = c(.55, .67)) +
  scale_x_discrete(expand = expansion(mult = c(0.09, 0.09)), 
                   limits = c("VOT_F0", "VOT_F0.centered", "VOT_F0.centered.input"),
                   breaks = c("VOT_F0", "VOT_F0.centered", "VOT_F0.centered.input"),
                   labels = c(VOT_F0 = "VOT-F0\n", VOT_F0.centered = "VOT-F0\n (talker centered)", VOT_F0.centered.input = "VOT-F0 \n (exposure-centered)")) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        legend.position = "none") +
  guides(x = guide_axis(angle = 45)) +
  coord_trans(y = "log10") 
```



(ref:IO-likelihood-plot) Median likelihood estimates of the human behavioural data under the assumptions of each IO model. Inset: Triangles indicate the mode, error bars represent the 95% quantile intervals from 100 bootstrap samples of by-talker estimates of each IO type.

```{r IO-likelihood-plot, fig.width=8, fig.height=6.5, fig.cap="(ref:IO-likelihood-plot)", warning=FALSE}
p.likelihood + inset_element(p.likelihood.multivar, .406, .05, .935, .82, align_to = "panel")
```




\newpage








