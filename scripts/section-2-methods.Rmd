```{r, stand-alone-preamble}
require(tidyverse)
require(magrittr)

require(brms)
require(tidybayes)
require(MVBeliefUpdatr)
require(phonR)
require(cowplot)

source("functions.R")
```

# Experiment
We aimed to design our experiment to provide high statistical power to detect effects of exposure, both incrementally within each exposure condition, and cumulatively across exposure conditions. To this end, we employed the repeated exposure-test design shown in Figure \@ref(fig:exp2-design-figure). The use of test blocks that repeated same stimuli across blocks and exposure conditions deviates from previous work [@clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020]. This design feature allowed us to assess how increasing exposure affects listeners' perception without making strong assumptions about the nature of these changes (e.g., linear changes across trials). Since previous work has found that repeated testing over uniform test continua can reduce or undo the effects of informative exposure [@liu-jaeger2018; @liu-jaeger2019; @cummings202X], we kept test blocks short, each consisting of only 12 trials. The final test blocks were intended to ameliorate the potential risks of this novel design: in case adaptation remains stable despite repeated testing, those additional test blocks were meant to provide additional statistical power to detect the effects of cumulative exposure. Finally, as we detail below, our design also allowed us to measure adaptation during exposure.

```{r exp2-design-figure, fig.height=3, fig.width=5, fig.cap="Exposure-test design of the experiment. Test blocks presented identical stimuli within and across conditions"}
knitr::include_graphics("../figures/experiment2_design_image.png")
```


```{r load data}
d <- 
  read_csv("../data/experiment2_raw_formatted.csv", show_col_types = F) %>%
  # load f0 measurements of stimuli (these were taken 5 msecs into the vowel)
  left_join(
    read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv", show_col_types = F) %>% 
      select(filename, VOT, f0_5ms_into_vowel) %>% 
      rename(Item.VOT = VOT,
             Item.F0_5ms = f0_5ms_into_vowel,
             Item.Filename = filename) %>% 
      mutate(Item.Filename = paste0(Item.Filename, ".wav")),
    by = c("Item.Filename", "Item.VOT")) %>% 
  mutate(
    Response.Voiceless = ifelse(Response.Voicing == "voiceless", 1, 0),
    across(
      c(Experiment, starts_with("List"), 
        ParticipantID, Participant.Race, Participant.Ethnicity, Participant.Sex,
        Condition.Exposure, Phase, 
        Trial.ImageSelection, Item.ExpectedResponse, Item.ExpectedResponse.Voicing, Item.MinimalPair,
        Response.ClickPosition, Response.Voicing),
      factor),
    Participant.Sex = str_to_lower(Participant.Sex),
    Item.Mel_F0_5ms = normMel(Item.F0_5ms)) 
```

## Methods 
### Participants
We recruited `r nlevels(d$ParticipantID) + 4` participants from the Prolific crowdsourcing platform. We used Prolific's pre-screening to limit the experiment to participants (1) of US nationality, (2) who reported to be English speaking monolinguals, and (3) had not previously participated in any experiment from our lab on Prolific. Prior to the start of the experiment, participants had to confirm that they (4) had spent the first 10 years of their life in the US, (5) were in a quiet place and free from distractions, and (6) wore in-ear or over-the-ears headphones that cost at least \$15. An additional 115 participants loaded the experiment but did not start or complete it. 

Participants took an average of 31.6 minutes to complete the experiment (SD = 20 minutes) and were remunerated \$8.00/hour. An optional post-experiment survey recorded participant demographics using NIH prescribed categories, including participant sex (`r d.Participant <- (d %>% distinct(ParticipantID, Participant.Age, Participant.Sex, Participant.Ethnicity, Participant.Race, Participant.Raceother)); d.Participant %>% group_by(Participant.Sex) %>% tally() %>% mutate(x = paste(n, Participant.Sex, sep = " = ", collapse = ", ")) %>% pull(x) %>% first()`), age (mean = `r mean(d.Participant$Participant.Age, 1)` years; 95% quantiles = `r paste(quantile(d.Participant$Participant.Age, c(.025, .975), na.rm = T), collapse = "-")` years), race (`r d.Participant %>% group_by(Participant.Race) %>% tally() %>% mutate(x = paste(n, Participant.Race, sep = " = ", collapse = ", ")) %>% pull(x) %>% first()`), and ethnicity (`r d.Participant %>% group_by(Participant.Ethnicity) %>% tally() %>% mutate(x = paste(n, Participant.Ethnicity, sep = " = ", collapse = ", ")) %>% pull(x) %>% first()`). 

Participants' responses were collected via Javascript developed by the Human Language Processing Lab at the University of Rochester [@JSEXP] and stored via Proliferate developed at, and hosted by, the ALPs lab at Stanford University [@schuster].

### Materials 
We recorded 8 tokens each of four minimal word pairs ("dill"/"till", "dim"/"tim", "din"/"tin", and "dip"/"tip") from a 23-year-old, female L1-US English talker from New Hampshire, judged to have a "general American" accent. These recordings were used to create four natural-sounding minimal pair VOT continua using a script [@winn2020manipulation] in Praat [@praat]. The VOTs generated for each continuum ranged from -100 to +130 msec in 5 msec steps.^[For simplicity's sake, we follow previous work [@kleinschmidt2020; @OTHERS] and refer to prevoicing as negative VOTs though we note that prevoicing is perhaps better conceived of as a separate phonetic feature [for discussion, see @REF]. In L1-US English, the occurence of prevoicing varies between study 20% - 48% of word-initial voiced stops and 0% of voiceless stops [@lisker-abramson1967; @smith1978].] The procedure also maintained the natural correlations between the most important cues to word-initial stop-voicing in L1-US English (VOT, F0, and vowel duration). Specifically, the F0 at vowel onset of each stimulus was set to respect the linear relation with VOT observed in the original recordings of the talker. The duration of the vowel was set to follow the natural trade-off relation with VOT [@allen1999effects]. Further details on the recording and resynthesis procedure are provided in the supplementary information (SI, \@ref(sec:SI-XXX)). 

This approach resulted in continuum steps that sound natural [unlike the highly robotic-sounding stimuli employed in @clayards2008; @kleinschmidt2016you]. A post-experiment survey asked participants: "*Did you notice anything in particular about how the speaker pronounced the different words (e.g. till, dill, etc.)?*". No participant reported that the stimuli sounded unnatural [in contrast to other experiments we have conducted with robotic-sounding stimuli like those of @clayards]. In addition to the critical minimal pair continua we also recorded three words that did not did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used for catch trials. Stimulus intensity was normalized to 70 dB sound pressure level for all recordings. 

A norming experiment (N = 24 participants) reported in the SI (\@ref(sec:norming-experiment)) was used to select the three minimal pairs that elicited the most similar categorization responses (dill-till, din-tin, and dip-tip). These three continua were used to create the three exposure conditions shown in Figure \@ref(fig:exp2-design-figure).


### Procedure
At the start of the experiment, participants acknowledged that they met all requirements and provided consent, as per the Research Subjects Review Board of the University of Rochester. Participants also had to pass a headphone test [@REF], and were instructed to not change the volume throughout the experiment. Following instructions, participants completed 234 two-alternative forced-choice categorisation trials (Figure \@ref(fig:exp1-example-trial)). Participants were instructed that they would hear a female talker say a single word on each trial, and were asked to select which word they heard. Participants were asked to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. 

```{r example-trial, fig.cap="Example trial display. When the green button turned bright green, participants had to click on it to play the recording."}
knitr::include_graphics("../figures/exp1_trial_example.png")
```

Unbeknownst to participants, the 234 trials were split into exposure blocks (54 trials each) and test blocks (12 trials each). Participants were given the opportunity to take breaks after every 60 trials, which was always during an exposure block. Finally, participants completed an exit survey and an optional demographics survey.

*Test blocks. * The experiment started with a test block. Test blocks were identical within and across conditions, always including 12 minimal pair trials assessing participants' categorization at 12 different VOTs
(-5, 5, 15, 25, 30, 35, 40, 45, 50, 55, 65, 70 msec). A uniform distribution over VOTs was chosen to maximize the statistical power to determine participants' categorisation function. The assignment of VOTs to minimal pair continua was randomized for each participant, while counter-balancing it within and across test blocks. Each minimal pair appear equally often within each test block (four times), and each minimal pair appear with each VOT equally often (twice) across all six test blocks (and no more than once per test block). 

Each trial started with a dark-shaded green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen, as shown in Figure \@ref(fig:exp1-example-trial). At 1000ms from trial onset, the fixation dot would turn bright green and participants had to click on the dot to play the recording. This was meant to reduce trial-to-trial correlations by resetting the mouse pointer to the center of the screen at the start of each trial. Participants responded by clicking on the word they heard and the next trial would begin. 

*Exposure blocks. * Each exposure block consisted of 24 /d/ and 24 /t/ trials, as well as 6 catch trials that served as a check on participant attention throughout the experiment (2 instances for each of three combinations of the three catch recordings). With a total of 144 trials, exposure was substantially shorter than in similar previous experiments [cf. 228 trials in @clayards2008; 222 trials in @kleinschmidt2020; 2 x 236 trials, @theodore-monto2019; 456 trials, @nixon2016]. <!-- TO DO: cited total trials but  -->

The distribution of VOTs across the 48 /d/-/t/ trials depended on the exposure condition. Specifically, we first created a *baseline* condition. Although not critical to the purpose of the experiment, we aimed for the VOT distribution in this condition to closely resemble participants' prior expectations for a `typical' female talker of L1-US English (for details, see SI, \@ref(XXX)). The mean and standard deviations for /d/ along VOT were set 5 msecs and 50 msecs, respectively. The mean and standard deviations for /t/ were set 80 msecs and 270 msecs, respectively. To create more realistic VOT distributions, we *sampled* from the intended VOT distribution (top row of Figure \@ref(fig:exp2-design-distribution)). This creates distributions that more closely resemble the type of distributional input listeners experience in everyday speech perception, deviating from previous work, which exposed listeners to highly unnatural fully symmetric samples [@clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020]. 

Half of the /d/ and half of the /t/ trials were labeled, the other half was unlabeled [paralleling one of the conditions in @kleinschmidt2015cogsci]. Unlabeled trials were identical to test trials except that the distribution of VOTs across those trials was bimodal (rather than uniform), and determined by the exposure condition. Labeled trials instead presented two response options with identical stop onsets (e.g., *din* and *dill*). This effectively labeled the input as belonging to the intended category (e.g., /d/). 

Next, we created the two additional exposure conditions by shifting these VOT distributions by +10 or +40 msecs (see Figure \@ref(fig:exp2-design-distribution)). This approach exposes participants to heterogenous approximations of normally distributed VOTs for /d/ and /t/ that varied across blocks, while holding all aspects of the input constant across conditions except for the shift in VOT.

(ref:exp2-design-distribution) Histogram of VOTs across the 48 trials of all three exposure blocks by exposure condition. The dashed gray line shows the theoretical (Normal) distribution that the baseline condition was sampled from. The order of blocks was counter-balanced across participants. 

```{r exp2-design-distribution, fig.height=6, fig.width=6.5, fig.cap="(ref:exp2-design-distribution)", warning=FALSE, message=FALSE}
d.exposure_trials <- 
  read_csv("../data/exposure_block_tokens.csv", show_col_types = F) %>% 
  mutate(
    condition = factor(case_when(condition == "Shift0" ~ "baseline",
                          condition == "Shift10" ~ "+10ms",
                          condition == "Shift40" ~ "+40ms")),
    condition = fct_relevel(condition, c("baseline", "+10ms", "+40ms")),
    labelling = as_factor(labelling))

# set variances of categories
var_d <- 80
var_t <- 270

d.means <- 
  crossing(condition = c("baseline", "+10ms", "+40ms"),
         category = c("/d/", "/t/")) %>% 
  mutate(condition = fct_relevel(condition, c("baseline", "+10ms", "+40ms")),
         mean = c(15, 60, 45, 90, 5, 50))

p.histogram_conditions <- 
  d.exposure_trials %>% 
  na.omit() %>% 
  filter(image_selection == "forward" & list_LSQ_variant == "A") %>%
    mutate(block = case_when(
    block == 2 ~ "Block A",
    block == 4 ~ "Block B",
    block == 6 ~ "Block C")) %>% 
  ggplot() +
  geom_histogram(aes(x = VOT, fill = paste(condition, category, labelling), 
                     color = paste(condition, category, labelling),
                     linetype = labelling), 
                 alpha = .8) +
  scale_colour_manual(
    "Labelling",
    values = c(
    "baseline /d/ labeled" = "#800000", 
    "baseline /d/ unlabeled" = "#ff9999",
    "baseline /t/ labeled" = "#cc0000", 
    "baseline /t/ unlabeled" = "#ffe6e6",
    "+10ms /d/ labeled" = "#0a751c",
    "+10ms /d/ unlabeled" = "#b9f9c3",
    "+10ms /t/ labeled" = "#12D432",
    "+10ms /t/ unlabeled" = "#e8fdeb",
    "+40ms /d/ labeled" = "#02427e", 
    "+40ms /d/ unlabeled" = "#b4dafe",
    "+40ms /t/ labeled" = "#0481F3", 
    "+40ms /t/ unlabeled" = "#e6f3ff"),
    aesthetics = c("color", "category", "fill"),
    labels = c("/d/ labeled", "/d/ unlabeled", "/t/ labeled", "/t/ unlabeled", "", "", "", "", "", "", "", "")) +
    guides(colour = guide_legend(
      override.aes = list(
    colour = c("#383838", "#C0C0C0", "#606060", "#F0F0F0", 0, 0, 0, 0, 0, 0, 0, 0),
    fill = c("#383838", "#C0C0C0", "#606060", "#F0F0F0", 0, 0, 0, 0, 0, 0, 0, 0),
    linetype = c(2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0),
    values = c("/d/ labeled", "/d/ unlabeled", "/t/ labeled", "/t/ unlabeled", 0, 0, 0, 0, 0, 0, 0, 0)), nrow = 1), title.position = "right") +
  # stat_function(fun = function(x) 72 * 5 * dnorm(x, 5, sqrt(var_d)),
  #               color = "black", size = .6, alpha = .7, linetype = 2) +
  # stat_function(
  #   fun = function(x) 72 * 5 * dnorm(x, 50, sqrt(var_t)),
  #   color = "black", size = .6, alpha = .5, linetype = 2) +
  #geom_rug(data = tibble(VOT = c(5, 50)), aes(x = VOT), sides = "t") +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 30)) +
  scale_y_continuous("Count") +
  geom_text(data = d.means,
            aes(x = 103, 
                y = 17,
                label = paste("mean", category, "=", mean)),
            size = 2,
            position = position_dodge2v(height = -8),
            inherit.aes = F) +
  facet_grid(condition ~ block, scales = "free_y", margins = "block") +
  guides(linetype = "none") +
  theme(legend.position = "top",
        legend.justification = "right",
        legend.key.width = unit(10, "pt"),
        legend.key.height = unit(10, "pt"),
        legend.box.spacing = unit(1, "pt"),
        legend.box.just = "right")
p.histogram_conditions
```

The order of trials was randomized within each block and participant, with the constraint that no more than two catch trials would occur in a row. Participants were randomly assigned to one of 3 (exposure condition) x 3 (block order) x 2 (placement of response options) lists. 

### Exclusions

```{r get-exclusion-indicators}
# mark catch trials rows and mark those to be excluded
d %<>% 
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(
      Is.CatchTrial == TRUE, 
      ifelse(Item.ExpectedResponse == Response, TRUE, FALSE), NA),
    Answer.sex.Correct = ifelse(sex == "woman", TRUE, FALSE)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Exclude_participant.due_to_catch_trials = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 17, TRUE, FALSE)) %>%
  ungroup()

# mark labelled trials
d %<>% 
  mutate(
    Response.Correct = ifelse(Item.ExpectedResponse == Response, TRUE, FALSE),
    LabeledTrial.Correct = ifelse(Item.Labeled == TRUE, ifelse(Response.Correct == TRUE, TRUE, FALSE), NA)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Exclude_participant.due_to_labeled_trials = ifelse(sum(LabeledTrial.Correct, na.rm = T) < 68, TRUE, FALSE))

# get data for exclusion due to categorisation slope of first 36 trials. 
# set the range of VOT values for which we require a certain categorisation
empirical_means <- c(17, 62)
VOT_for_required_proportion_t <- empirical_means + c(-20, 20)
required_proportion_t <- c(.15, .80) 

d.VOT_exclusion <- 
  d %>% 
  filter(Block == 1 | (Block == 2 & Trial %in% c(13:36))) %>% 
  drop_na(c(ParticipantID, Response.Voicing, Item.VOT)) %>% 
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  select(c(Experiment, ParticipantID, Condition.Exposure, Response.Voicing, Response.ProportionVoiceless, Item.VOT)) %>%
  # Fit logistic regression by participant to get model predictions 
  group_by(ParticipantID, Experiment, Condition.Exposure) %>%
  nest() %>%
  mutate(
    CategorizationModel =
      map(
        data, 
        ~ glm(
          Response.ProportionVoiceless ~ 1 + Item.VOT, 
          data = .x, 
          family = binomial))) %>% 
  # type = "response" in predict() gives the probability
  summarise(Model.predicted.Reponse = 
              map(
                CategorizationModel, 
                ~ predict(object = .x, 
                          newdata = tibble(Item.VOT = VOT_for_required_proportion_t), 
                          type = "response"))) %>% 
  mutate(Exclude_participant.due_to_VOT_slope = 
           map(Model.predicted.Reponse, ~ ifelse(.x[1] > required_proportion_t[1] || .x[2] < required_proportion_t[2], TRUE, FALSE)),
         Exclude_participant.due_to_lower_VOT = map(Model.predicted.Reponse, ~ ifelse(.x[1] > required_proportion_t[1], TRUE, FALSE)),
         Exclude_participant.due_to_higher_VOT = map(Model.predicted.Reponse, ~ ifelse(.x[2] < required_proportion_t[2], TRUE, FALSE))) %>% 
  select(ParticipantID, Experiment, Condition.Exposure, Exclude_participant.due_to_VOT_slope, Exclude_participant.due_to_lower_VOT, Exclude_participant.due_to_higher_VOT) %>% 
  mutate(
    across(c(Exclude_participant.due_to_VOT_slope, Exclude_participant.due_to_lower_VOT, Exclude_participant.due_to_higher_VOT), 
           unlist)) %>% 
  ungroup()

d %<>% left_join(d.VOT_exclusion)
```



```{r set-RT-exclusion-criteria}
# get number of participants excluded due to catch trial
excl.catch <- 
  d %>% 
  filter(Exclude_participant.due_to_catch_trials == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally() %>%
  pull(n)

# get number of participants excluded due to labelled trials
excl.labeled <- 
  d %>% 
  filter(Exclude_participant.due_to_labeled_trials == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally() %>%
  pull(n)

# count number of exclusions due to VOT slope
excl.VOT <- 
  d %>% 
  filter(Exclude_participant.due_to_VOT_slope == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally() %>%
  pull(n)

# get exclusions due to RT
excl.RT <- 
  d %>%
  filter(
    Is.CatchTrial == FALSE,
    Exclude_participant.due_to_catch_trials == FALSE,
    Exclude_participant.due_to_labeled_trials == FALSE,
    Exclude_participant.due_to_VOT_slope == FALSE) %>% 
  group_by(ParticipantID) %>%
  mutate(
    Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
    Response.log_RT.scaled = scale(Response.log_RT), 
    Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(Exclude_participant.due_to_RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE)) %>% 
  group_by(ParticipantID) %>% 
  filter(Exclude_participant.due_to_RT == TRUE) %>% 
  tally() %>% 
  pull(n)

# headphone types
excl.headphone <- 
  d %>% 
  group_by(ParticipantID, audio_type) %>% 
  summarise() %>% 
  filter(!(audio_type %in% c("in-ear", "over-ear"))) %>% 
  nrow()
```


```{r make-data-for-analysis}
d_for_analysis <- 
  d %>%
  filter(
    Is.CatchTrial == FALSE &
    Exclude_participant.due_to_catch_trials == FALSE &
    Exclude_participant.due_to_labeled_trials == FALSE &
    Exclude_participant.due_to_VOT_slope == FALSE)
```

Due to data transfer errors 4 participants' data were not stored and therefore excluded from analysis. We further excluded from analysis participants who committed more than 3 errors out of the 18 catch trials (<83% accuracy, N = `r excl.catch`), participants who committed more than 4 errors out of the 72 labelled trials (<94% accuracy, N = `r excl.labeled`), participants with an average reaction time more than three standard deviations from the mean of the by-participant means (N = `r excl.RT`),  participants who had atypical categorisation functions at the start of the experiment (N = `r excl.VOT`, see SI, \@ref(XXX) for details), and participants who reported not to have used headphones (N = `r excl.headphone`). This left for analysis `r nrow(d_for_analysis %>% filter(Phase == "exposure"))` exposure and `r nrow(d_for_analysis %>% filter(Phase == "test"))` test observations from `r nrow(d_for_analysis %>% distinct(ParticipantID))` participants (94% of total), evenly split across the three exposure conditions.









