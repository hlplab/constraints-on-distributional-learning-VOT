```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```


# Methods
We first describe the web-based experiment we used to investigate incremental changes in listeners' perception as a function of exposure. Then we describe the two types of normative models mentioned in Figure \@ref(fig:predictions), ideal observer models that deliver specific, quantifiable predictions for the behavior expected prior to exposure (idealized pre-exposure listener) and following exposure (idealized learners that have fully learned the phonetic distributions they encountered during exposure). These models play an important role in interpreting the results of the experiment.

## Participants
We recruited `r nlevels(d$ParticipantID) + 4` participants from the Prolific crowdsourcing platform. Participants were randomly assigned to one of three exposure conditions in Figure \@ref(fig:block-design-figure). We used Prolific's pre-screening to limit the experiment to participants (1) of US nationality, (2) who reported to be English speaking monolinguals, and (3) had not previously participated in any experiment from our lab on Prolific. Prior to the start of the experiment, participants had to confirm that they (4) had spent the first 10 years of their life in the US, (5) were in a quiet place and free from distractions, and (6) wore in-ear or over-the-ears headphones that cost at least \$15.

```{r prepare-participant-data, include=FALSE}
d.Participant <-
  d %>%
  distinct(
    ParticipantID, Participant.Age, Participant.Sex, Participant.Ethnicity,
    Participant.Race, Participant.Raceother) %>%
  mutate(
    across(
      c(Participant.Sex, Participant.Ethnicity, Participant.Race, Participant.Raceother),
      ~ ifelse(is.na(.x), "declined to report", as.character(.x))),
    across(
      c(Participant.Sex, Participant.Ethnicity, Participant.Race, Participant.Raceother),
      ~ factor(.x, levels = c(setdiff(unique(.x), "declined to report"), "declined to report"))))
```

Participants' responses were collected via Javascript developed by the Human Language Processing Lab at the University of Rochester [@JSEXP2021] and stored via Proliferate developed at, and hosted by, the ALPs lab at Stanford University [@Proliferate]. Participants took an average of 31.6 minutes (SD = 20 minutes) to complete the experiment and were remunerated \$8.00/hour. An optional post-experiment survey recorded participant demographics using NIH prescribed categories, including participant sex (`r d.Participant %>% group_by(Participant.Sex) %>% tally() %>% mutate(x = paste(Participant.Sex, n, sep = ": ", collapse = ", ")) %>% pull(x) %>% first()`), age (mean = `r round(mean(d.Participant$Participant.Age, na.rm = T), 1)` years; SD = `r round(sd(d.Participant$Participant.Age, na.rm = T), 1)`; 95% quantiles = `r paste(quantile(d.Participant$Participant.Age, c(.025, .975), na.rm = T), collapse = "-")` years), ethnicity (`r d.Participant %>% group_by(Participant.Ethnicity) %>% tally() %>% mutate(x = paste(Participant.Ethnicity, n, sep = ": ", collapse = ", ")) %>% pull(x) %>% first()`), and race (due to a technical error, all information was lost).

## Materials
We recorded 8 tokens each of four minimal word pairs with word-initial /d/-/t/ (*dill/till*, *dim/tim*, *din/tin*, and *dip/tip*) from a 23-year-old, female L1-US English talker from New Hampshire. In addition to these critical minimal pairs we also recorded three words that did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used for catch trials. Stimulus intensity was normalized to 70 dB sound pressure level for all recordings.

The critical minimal pair recordings were used to create four VOT continua ranging from -100 to +130 ms in 5 ms steps.^[We follow previous work [@kleinschmidt2020; @lisker-abramson1964] and refer to pre-voicing as negative VOTs though we note that pre-voicing is perhaps better conceived of as a separate phonetic feature [for discussion, see @mikuteit-reetz2007]. This distinction can, for example, be important when interpreting asymmetries in listeners' ability to adapt to left- vs. rightward shifts along the VOT continuum, an issue we revisit in the general discussion.] Continua were generated using a script [@winn2020] in Praat [@boersma2022]. This approach resulted in continuum steps that sound natural, unlike the highly robotic-sounding stimuli employed in previous distributional learning studies [but see @theodore-monto2019]. It also maintained the natural correlations between the most important cues to word-initial stop-voicing in L1-US English (VOT, F0, and vowel duration). Specifically, the F0 at vowel onset of each stimulus was set to respect the linear relation with VOT observed in the original recordings of the talker. The duration of the vowel was set to follow the natural trade-off relation with VOT [@allen-miller1999]. Further details on the recording and resynthesis procedure are provided in the SI (\@ref(sec:stimulus-generation)). A post-experiment survey asked participants: "*Did you notice anything in particular about how the speaker pronounced the different words (e.g. till, dill, etc.)?*" No participant responded that the stimuli sounded unnatural. Analyses reported in the SI (\@ref(sec:analysis-lapse)) found that participants exhibited few attentional lapses (< 1%), including at the start of the experiment ($\leq 1.5$%). This is a marked improvement over previous studies with robotic sounding stimuli, which elicited high lapse rates, especially at the start of the experiment [12%, @kleinschmidt2020]. A norming experiment (N = 24 participants) was used to select the three minimal pair continua that differed the least from each other in terms of the categorization responses they elicited (*dill-till*, *din-tin*, and *dip-tip*).

## Procedure {#sec:procedure}
At the start of the experiment, participants acknowledged that they met all requirements and provided consent, as per the Research Subjects Review Board of the University of Rochester. Participants had to pass a headphone test in order to continue [@woods2017], and were instructed to not change the volume throughout the experiment. Following instructions, participants completed 234 trials of two-alternative forced-choice categorization. Participants were given the opportunity to take breaks after every 60 trials, which was always during an exposure block. Finally, participants completed an exit survey and an optional demographics survey.

On each of the 234 categorization trials, participants heard a single word spoken by a female talker, and had to click on the word they heard (see Figure \@ref(fig:example-trial)). Participants were instructed to "answer as quickly and as accurately as possible". Participants were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. Each trial started with a dark-shaded green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen. At 1000ms from trial onset, the fixation dot would turn bright green and participants had to click on the dot to play the recording. This was meant to reduce trial-to-trial correlations by resetting the mouse pointer to the center of the screen at the start of each trial. Participants responded by clicking on the word they heard and the next trial would begin. Unbeknownst to participants, the 234 trials were split into three exposure blocks (54 trials each) and six test blocks (12 trials each), as shown in Figure \@ref(fig:block-design-figure).

```{r example-trial, fig.cap="Example trial display. When the green button turned bright green, participants had to click on it to play the recording. The placement of response options was counter-balanced across participants.", out.width="33%"}
knitr::include_graphics("../figures/trial_example.png")
```

*Test blocks. * The experiment started with a test block, required to test prediction 1---whether the distribution of phonetic cues in listeners' input prior to the experiment can explain their categorization behavior prior to additional exposure (a second component required for such a test is a model that links estimates of those phonetic distributions to listeners' perception; we introduce this model at the end of the *Methods* section). Test blocks were identical within and across conditions, always including 12 minimal pair trials assessing participants' categorization at 12 different VOTs (-5, 5, 15, 25, 30, 35, 40, 45, 50, 55, 65, 70 ms). <!-- A uniform, rather than bimodal, distribution over VOTs was chosen to maximize the statistical power to determine participants' categorization function.--> The same brief test block followed each exposure block to assess the effects of cumulative exposure.

Three considerations informed the decision to keep test blocks short. First, listeners' attention span is limited. Second, repeated testing over uniform test continua can reduce or undo the effects of preceding exposure [@cummings-theodore2023; @giovannone-theodore2021; @liu-jaeger2018; @liu-jaeger2019; @tzeng2021; @zheng-samuel2023]. This consideration is particularly important given our focus on rapid adaptation, as shorter exposure might be more easily overridden by intermittent testing [as opposed to testing in between hour-long training blocks over days and weeks, @logan1995; @mccandliss2002; @pisoni1982]. Third, the three exposure conditions differ in their exposure distributions, so that the "same" distribution in a test block will convey different information when evaluated relative to these exposure conditions. Theories of error-driven learning and ideal information integration (discussed in the introduction) predict that this affects adaptation. By keeping tests short relative to exposure, we aimed to minimize the influence of test trials on adaptation while still being able to estimate changes in listeners' categorization function.

The assignment of VOTs to minimal pair continua was randomized for each participant, but counter-balanced within and across test blocks. Each minimal pair appear equally often within each test block (four times), and each minimal pair appear with each VOT equally often (twice) across all six test blocks (and no more than once per test block). The order of response options---whether the /d/-initial word appeared on the left or right of the screen (see Figure \@ref(fig:example-trial))---was held constant within each participant, and counter-balanced across participants.

*Exposure blocks. * Each exposure block consisted of 24 /d/ and 24 /t/ trials forming distributions over VOT, as well as 6 catch trials that served as a check on participant attention throughout the experiment (2 instances for each of three combinations of the three catch recordings). With a total of 144 trials, and intermittent tests after 0, 48, and 96 critical trials, the experiment was designed to measure the effects of exposure incrementally and at substantially earlier moments than in similar previous experiments [cf. >200 critical trials in @clayards2008; @kleinschmidt2020; @theodore-monto2019; @nixon2016].

The distribution of VOTs across the 48 /d/-/t/ trials depended on the exposure condition. We created three conditions that shifted exposure distributions to different degrees relative to listeners' prior expectations. This combination of different exposure statistics (between participants) and incremental testing (within participants) provides us with the type of more fine-grained data we mentioned in the introduction, required for stronger tests of predictions 1-4. Following the advice of an anonymous reviewer, we name the three exposure conditions based on the *predicted* change in the PSE of listeners' categorization function---from pre-exposure to post-exposure---that is expected for a idealized learner that has fully learned the exposure distribution. These models are described at the end of the *Methods* section. We note, however, that we started data collection prior to having completed development of these models.

We first created the *-18ms* condition. We set the VOT means to 5ms for /d/ and 50ms for /t/. We took three steps to increase the ecological validity of the VOT distributions, compared to similar previous work. First, participants heard 21 distinct VOT locations during exposure and 12 during test, spanning the entire phonetic continuum and distributed across three minimal pairs. This makes the stimuli distribution in the present study considerably less repetitive, and more fine-grained than in previous work [representative examples include 12 total locations in @clayards2008; 6 exposure and 5 test locations, @colby2018; 10 exposure and 3 test locations, @zhang-holt2018; @idemaru-holt2020; for a notable exception, see @chladkova2017]. This more closely resembles the range of stimuli listeners might encounter during everyday speech perception, and makes it less likely that participants tailor their test responses to specific stimuli (each condition had more than 50 unique stimuli for 144 trials), rather than adapting their categorization functions through distributional learning. Second, previous studies exposed each group of listeners to categories with identical variance [see e.g., @clayards2008; @kleinschmidt2020; @theodore-monto2019]. We instead set the variance for /d/ to  80 ms$^2$ (SD = `r (80)^(1/2)`) and for /t/ to 270 ms$^2$ (SD = `r (270)^(1/2)`) VOT. This qualitatively follows the natural asymmetry in the variance of VOT for /d/ and /t/ found in everyday speech [@lisker-abramson1964; @docherty1992; @chodroff-wilson2017].^[The specific variance values we chose strike a compromise between the variance observed in natural productions [e.g, mean by-talker variances of `r round(d.chodroff_wilson.talker_stats %>% filter(speechstyle == "isolated", category == "/d/") %>% pull(VOT_var_mean), 1)` ms$^2$ for /d/ and `r round(d.chodroff_wilson.talker_stats %>% filter(speechstyle == "isolated", category == "/t/") %>% pull(VOT_var_mean), 1)` ms$^2$ for /t/ in hyper-articulated isolated word productions, and `r round(d.chodroff_wilson.talker_stats %>% filter(speechstyle == "connected", category == "/d/") %>% pull(VOT_var_mean), 1)` ms$^2$ for /d/ and `r round(d.chodroff_wilson.talker_stats %>% filter(speechstyle == "connected", category == "/t/") %>% pull(VOT_var_mean), 1)` ms$^2$ for /t/ in connected speech, @chodroff-wilson2017], and the range of natural-sounding VOTs that we were able to generate with our procedure (for VOTs > 130ms, some recordings would not have sounded natural).] Third, rather than to expose listeners to fully symmetric *designed* distributions that would never be experienced in everyday speech [as remains the norm in distributional learning studies, @colby2018; @escudero-williams2014; @idemaru-holt2020; @kleinschmidt2020], we *randomly sampled* from the intended VOT distribution (see top row of Figure \@ref(fig:design-distribution)). Specifically, we sampled VOTs for three exposure blocks, and then created three Latin-square designed lists that counter-balanced the order of these blocks across participants.

(ref:design-distribution) Histogram of VOTs for each of the three exposure blocks A-C by exposure condition and trial type (labeled or unlabeled, sampled from /d/ or /t/). Each exposure block contained 12 labeled /d/, 12 labeled /t/, 12 unlabeled /d/, and 12 unlabeled /t/ trials, as well as 6 catch trials (not shown). Except for the shift in VOTs, the VOT distribution of these trials--as well as the relative placement of labeled and unlabeled trials---was identical across exposure conditions. The order of exposure blocks A-C was counter-balanced across participants within each exposure condition using a Latin-square design. Tick marks along the x-axis show the location of the twelve *test* tokens, which were identical across conditions.

```{r design-distribution, fig.height=base.height*3+1/2, fig.width=base.width*4, fig.cap="(ref:design-distribution)", warning=FALSE, message=FALSE}
var_d <- 80
var_t <- 270

d.exposure <-
  d %>%
  group_by(Condition.Exposure, List.ExposureBlockOrder) %>%
  filter(Phase == "exposure",
         Is.CatchTrial == F,
         ParticipantID == first(ParticipantID)) %>%
  filter(List.ExposureBlockOrder == "A") %>%
  group_by(Condition.Exposure) %>%
  mutate(labelling = factor(ifelse(Item.Labeled == T, "labeled", "unlabeled")),
         Condition.Exposure = factor(case_when(
           Condition.Exposure == "Shift0" ~ "-20ms",
           Condition.Exposure == "Shift10" ~ "-10ms",
           Condition.Exposure == "Shift40" ~ "+20ms")),
         Block = factor(case_when(
           Block == 2 ~ "Block A",
           Block == 4 ~ "Block B",
           Block == 6 ~ "Block C")),
         category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))

d.means <-
  crossing(
    Condition.Exposure = c("-20ms", "-10ms", "+20ms"),
    category = c("/d/", "/t/")) %>%
  mutate(
    Block = "Block A",
    Condition.Exposure = fct_relevel(Condition.Exposure, c("-20ms", "-10ms", "+20ms")),
    mu = c(15, 60, 45, 90, 5, 50),
    sigma = (c(80, 270, 80, 270, 80, 270))^(1/2),
    mu_label = map2(category, mu, ~ bquote(mu[.(.x)]==.(.y))),
    sigma_label = map2(category, sigma, ~ bquote({sigma}[.(.x)]==.(round(.y, 1)))),
    label = map2(mu_label, sigma_label, ~ comb_plotmath(.x, ", ", .y)))

p.histogram_conditions <-
  d.exposure %>%
  bind_rows(d.exposure %>% group_by(Condition.Exposure) %>% mutate(Block = "Block D")) %>%
  ungroup() %>%
  mutate(Block = fct_relevel(Block, c("Block A", "Block B", "Block C", "Block D"))) %>%
  ggplot(aes(x = VOT)) +
  geom_histogram(
    aes(fill = paste(Condition.Exposure, category, labelling)),
    color = NA,
    alpha = .8) +
  geom_text(
    data = d.means,
    aes(x = -10, y = 17, label = label),
    parse = T,
    size = 2, hjust = 0, position = position_dodge2v(height = -8),
    inherit.aes = F) +
  geom_rug(data = d %>% filter(Phase == "test") %>% distinct(VOT), sides = "b") +
  guides(
    colour = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", "#606060", "#F0F0F0", 0, 0, 0, 0, 0, 0, 0, 0),
        values = c("/d/ labeled", "/d/ unlabeled", "/t/ labeled", "/t/ unlabeled", 0, 0, 0, 0, 0, 0, 0, 0)),
      nrow = 2),
    linetype = "none") +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Count (per participant)") +
  scale_colour_manual(
    "Stimulus type",
    values = c(
      "-20ms /d/ labeled" = "#800000",
      "-20ms /d/ unlabeled" = "#ff9999",
      "-20ms /t/ labeled" = "#cc0000",
      "-20ms /t/ unlabeled" = "#ffe6e6",
      "-10ms /d/ labeled" = "#0a751c",
      "-10ms /d/ unlabeled" = "#b9f9c3",
      "-10ms /t/ labeled" = "#12D432",
      "-10ms /t/ unlabeled" = "#e8fdeb",
      "+20ms /d/ labeled" = "#02427e",
      "+20ms /d/ unlabeled" = "#b4dafe",
      "+20ms /t/ labeled" = "#0481F3",
      "+20ms /t/ unlabeled" = "#e6f3ff"),
    aesthetics = c("colour", "category", "fill"),
    labels = c("/d/ labeled", "/d/ unlabeled", "/t/ labeled", "/t/ unlabeled", "", "", "", "", "", "", "", "")) +
  facet_grid(
    Condition.Exposure ~ Block,
    labeller = labeller(Block = c("Block A" = "Block A", "Block B" = "Block B", "Block C" = "Block C", "Block D" = "all")),
    scales = "free_y") +
  theme(
    legend.position = "top",
    legend.title = element_text(hjust = 0),
    legend.text = element_text(hjust = 0),
    legend.justification = c(.5, .5),
    legend.box.just = "center",
    legend.key.width = unit(12, "pt"),
    legend.key.height = unit(12, "pt"),
    legend.box.spacing = unit(1, "pt"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.minor.y = element_blank())

p.histogram_conditions
```

Following @kleinschmidt2015, half of the /d/ and half of the /t/ trials in each exposure block were labeled, and the other half were unlabeled. Unlabeled trials were identical to test trials except that the distribution of VOTs across those trials was bimodal (rather than uniform), and determined by the exposure condition (see Figure \@ref(fig:design-distribution)). Labeled trials followed the same distribution but instead presented two response options with identical stop onsets---e.g., *din* and *dill* to label the input as a /d/. <!-- Earlier distributional learning studies have mostly used fully unlabeled exposure [@clayards2008; @nixon2016; @theodore-monto2019]. This contrasts with visually- or lexically-guided perceptual learning studies, which use labeled exposure [@bertelson2003; @norris2003; @kraljic-samuel2005; @vroomen2007]. Such labeling is known to facilitate adaptation [@burchill2018; @burchill2023]---indeed, if shifted pronunciations are embedded in minimal pair or nonce-word contexts, listeners do not shift their categorization boundary [@norris2003]. --> While lexical context often disambiguates and labels sounds in everyday speech [facilitating adaptation, @burchill2023; @burchill2018], disambiguating context is not *always* available. Especially with unfamiliar accents, listeners often have uncertainty about the word sequences they hear. This reduces the labeling information available to them during typical listening. Here, we thus struck a compromise between never or always labeling the input.

Next, we created the two additional exposure conditions by shifting all VOTs sampled for the -18ms condition by +10 or +22ms ms (see Figure \@ref(fig:design-distribution)). This approach exposes participants to heterogeneous samples of VOT distributions for /d/ and /t/ that varied within and across blocks, while holding all aspects of the input constant across conditions except for the shift in VOT---including the placement of labeled and unlabeled trials relative to the exposure condition's category means. The order of trials was randomized within each block and participant, with no more than two catch trials in a row. Participants were randomly assigned to one of 18 lists, crossing 3 (exposure condition) x 3 (block order) x 2 (placement of response options during unlabeled test and exposure trials). In total, this list design resulted in 36 exposure-test combinations (Test 1-4 x 3 exposure condition x 3 block order that differed somewhat in the sampled statistics for exposure block 1-3). Together with the 12 VOT locations in each test block, the present experiment assessed participants' perception for 432 unique combinations of exposure and test stimuli, each of which can be compared against the predictions of distributional learning models. This is a marked improvement compared to previous studies, and allows us to submit distributional learning models to a much stronger quantitative test.
<!-- ^[The actual number of unique predictions for these 432 combinations is lower. For example, all distributional learning models predict no difference in Test 1, regardless of exposure condition and block order. Order-insensitive distribution learning models additionally predict that the Latin-square design for block order---which results in slightly different exposure statistics in the different exposure blocks---has no effect in Test 4, after all three exposure blocks have been experienced. Still, even with these considerations in mind, our design resulted in 264 combinations of exposure and test stimuli that should differ in terms of the predictions made by distributional learning models.]  -->


## Exclusions

```{r get-exclusion-indicators, warning=FALSE}
# get data for exclusion due to categorization slope of first 36 trials.
# set the approximate means for both d and t categories
category_means <- c(17, 62)
VOT_for_required_proportion_t <- category_means + c(-20, 20)
required_proportion_t <- c(.15, .80)

d.VOT_exclusion <-
  d %>%
  filter(Block == 1 | (Block == 2 & Trial %in% c(13:36))) %>%
  drop_na(c(ParticipantID, Response.Voicing, Item.VOT)) %>%
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  select(c(Experiment, ParticipantID, Condition.Exposure, Response.Voicing, Response.ProportionVoiceless, Item.VOT)) %>%
  # Fit logistic regression by participant to get model predictions
  group_by(ParticipantID, Experiment, Condition.Exposure) %>%
  nest() %>%
  mutate(
    CategorizationModel =
      map(
        data,
        ~ glm(
          Response.ProportionVoiceless ~ 1 + Item.VOT,
          data = .x,
          family = binomial))) %>%
  # type = "response" in predict() gives the probability
  summarise(
    Model.predicted.Reponse =
      map(
        CategorizationModel,
        ~ predict(
          object = .x,
          newdata = tibble(Item.VOT = VOT_for_required_proportion_t),
          type = "response"))) %>%
  mutate(
    Exclude_participant.due_to_VOT_slope =
      map(
        Model.predicted.Reponse,
        ~ ifelse(.x[1] > required_proportion_t[1] || .x[2] < required_proportion_t[2], TRUE, FALSE)),
    Exclude_participant.due_to_lower_VOT =
      map(
        Model.predicted.Reponse,
        ~ ifelse(.x[1] > required_proportion_t[1], TRUE, FALSE)),
    Exclude_participant.due_to_higher_VOT =
      map(
        Model.predicted.Reponse,
        ~ ifelse(.x[2] < required_proportion_t[2], TRUE, FALSE))) %>%
  select(ParticipantID, Experiment, Condition.Exposure, Exclude_participant.due_to_VOT_slope, Exclude_participant.due_to_lower_VOT, Exclude_participant.due_to_higher_VOT) %>%
  mutate(across(c(Exclude_participant.due_to_VOT_slope, Exclude_participant.due_to_lower_VOT, Exclude_participant.due_to_higher_VOT), unlist)) %>%
  ungroup()

d %<>% left_join(d.VOT_exclusion)
```

```{r set-RT-exclusion-criteria}
# get exclusions due to RT
excl.RT <-
  d %>%
  filter(
    Is.CatchTrial == FALSE,
    Exclude_participant.due_to_catch_trials == FALSE,
    Exclude_participant.due_to_labeled_trials == FALSE,
    Exclude_participant.due_to_VOT_slope == FALSE) %>%
  group_by(ParticipantID) %>%
  mutate(
    Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
    Response.log_RT.scaled = scale(Response.log_RT),
    Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>%
  ungroup() %>%
  mutate(Exclude_participant.due_to_RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE)) %>%
  filter(Exclude_participant.due_to_RT == TRUE) %>%
  distinct(ParticipantID) %>%
  nrow()
```

```{r make-data-for-analysis}
d.for_analysis <-
  d %>%
  filter(
    Is.CatchTrial == FALSE,
    Exclude_participant.due_to_catch_trials == FALSE,
    Exclude_participant.due_to_labeled_trials == FALSE,
    Exclude_participant.due_to_VOT_slope == FALSE) %>%
  group_by(Block) %>%
  add_block_labels() %>%
  ungroup()
```

Exclusion criteria were determined prior to analysis. Due to data transfer errors, four participants' data were not stored and therefore excluded from analysis. We further excluded from analysis participants who committed more than three errors out of the 18 catch trials (<83% accuracy, N = `r d %>% filter(Exclude_participant.due_to_catch_trials == TRUE) %>% group_by(ParticipantID) %>% summarise() %>% tally() %>% pull(n)`), participants who committed more than four errors out of the 72 labelled trials (<94% accuracy, N = `r d %>% filter(Exclude_participant.due_to_labeled_trials == TRUE) %>% group_by(ParticipantID) %>% summarise() %>% tally() %>% pull(n)`), participants with an average reaction time more than three standard deviations from the mean of the by-participant means (N = `r excl.RT`),  participants who had atypical categorization functions even prior to exposure (N = `r d %>% filter(Exclude_participant.due_to_VOT_slope == TRUE) %>% group_by(ParticipantID) %>% summarise() %>% tally() %>% pull(n)`, see SI, \@ref(sec:exclusions) for details), and participants who reported not to have used headphones (N = `r d %>% group_by(ParticipantID, audio_type) %>% summarise() %>% filter(!(audio_type %in% c("in-ear", "over-ear"))) %>% nrow()`). This left for analysis `r nrow(d.for_analysis %>% filter(Phase == "exposure"))` exposure and `r nrow(d.for_analysis %>% filter(Phase == "test"))` test observations from `r nrow(d.for_analysis %>% ungroup() %>% distinct(ParticipantID))` participants (94% of total), approximately evenly split across the three exposure conditions (-20ms: `r nrow(d.for_analysis %>% ungroup() %>% filter(Condition.Exposure == "Shift0") %>% distinct(ParticipantID))` participants; -10ms: `r nrow(d.for_analysis %>% ungroup() %>% filter(Condition.Exposure == "Shift10") %>% distinct(ParticipantID))`; +20ms: `r nrow(d.for_analysis %>% ungroup() %>% filter(Condition.Exposure == "Shift40") %>% distinct(ParticipantID))`).

## Developing normative predictions for model-guided data interpretation
Next, we describe how we developed normative models to derive strong, falsifiable predictions from the hypothesis of distributional learning. Specifically, we developed the two types of models already mentioned in \@ref(fig:predictions): the *idealized pre-exposure listener* to predict listeners' categorization responses prior to exposure, and one *idealized learner* model for each exposure condition that has fully learned those exposure distributions.

The model for the idealized pre-exposure listener allows us to test whether listeners' categorization functions prior to any exposure---in Test block 1---are predicted by the distribution of phonetic cues to /d/ and /t/ experienced by a 'typical' L1 listeners of US English. To this end, we used a phonetic database of L1-US English productions of syllable-initial stop productions [@chodroff-wilson2018]. The database contains information for three important cues to the word-initial /d/-/t/ contrast in L1-US English: VOT, f0, and vowel duration (for details, see SI \@ref(sec:phonetic-database)). We used this database to train Bayesian ideal observers on the distributions over VOT, f0, and vowel durations for /d/ and /t/. Specifically, since each listener is expected to have slightly different representations of /d/ and/t/ based on individual differences in listeners' previous exposure, we used five fold cross-validation to train five separate Bayesian ideal observers. This provided us with a (very rough) estimate of the range of categorization behavior that would be expected prior to any exposure to the unfamiliar talker based on the phonetic distributions of US English (for details, see SI, \@ref(sec:idealized-prior-listeners)).

(ref:conditions-predicted-PSE) The expected categorization functions of the respective conditions by idealized listeners who fully learn the exposure talker's phonetic distributions. The gray band marks the range of five-fold cross validated PSEs of idealized pre-exposure listeners estimated from a database of female talkers. Arrows point to the direction of the shift of the pre-exposure PSE after full exposure. Condition labels refer to the amount of shift relative to the median value of listeners' pre-exposure PSEs.

```{r conditions-relative-pre-exposure-PSE, fig.height=base.height*2.8, fig.width=base.width*3.8, fig.cap="(ref:conditions-predicted-PSE)"}
# Store parameters for model fitting
d.mean_sd_scaling <-
  d.for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]
rm(d.mean_sd_scaling)

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")


d.condition.exposure.IO <-
  make_IOs_from_data (
    data = d.exposure,
    cues = c("VOT"),
    groups = "Condition.Exposure") %>%
  nest(io = -Condition.Exposure) %>%
  add_x_to_IO(VOTs = seq(-10, 90, .5)) %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  select(Condition.Exposure, category, categorization, PSE) %>%
  filter(category == "/t/") %>%
  unnest(categorization, names_repair = "unique")

# Create data frame for pre-exposure IO. To avoid over-fitting, we use 5-fold cross-validation.
# We use the isolated speech data from Chodroff and Wilson, and randomly cut it into 5 folds.
set.seed(920)
d.prior <-
  d.chodroff_wilson.isolated %>%
  group_by(Talker, category) %>%
  mutate(fold = sample(1:5, n(), replace = T))

# Prior based on Chodroff & Wilson (2018). Note that this database has substantially higher
# VOT variance than our exposure conditions, which we based on estimates from Kronrod et al.
# (2016). Uses all three cues; duration and spectral noise taken from Kronrod et al.
d.prior.io <-
 make_IOs_from_data(
      data = d.prior,
      cues = c("VOT", "f0_Mel", "vowel_duration"),
      group = "fold") %>%
      nest(io = -fold) %>%
      mutate(fold = str_c("prior", fold)) %>%
      rename(Condition.Exposure = fold) %>%
      ungroup() %>%
     # Join in the data that the pre-exposure IOs will be applied to:
      cross_join(
        d.for_analysis %>%  
          filter(Phase == "test") %>%
          distinct(Condition.Exposure, Item.VOT, Item.f0_Mel, Item.VowelDuration) %>%
          mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(...))) %>%
          select(x) %>%
          nest(x = x))

d.prior.PSE <-
    get_logistic_parameters_from_model(
      model = d.prior.io,
      model_col = "io",
      groups = "Condition.Exposure") %>%
  select(Condition.Exposure, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>%
  ungroup()

d.condition.exposure.IO %>%
  ggplot(aes(x = VOT, y = response, colour = Condition.Exposure)) +
  geom_line(linewidth = 1.2) +
   geom_rect(
    data =
      d.prior.PSE %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      summarise(
        PSE.lower = mean(PSE) - sd(PSE) / sqrt(length(PSE)) * 1.96,
        PSE.upper = mean(PSE) + sd(PSE) / sqrt(length(PSE)) * 1.96),
    mapping = aes(xmin = PSE.lower, ymin = -Inf, ymax = Inf, xmax = PSE.upper),
    fill = "#d2d4dc",
    alpha = .3,
    inherit.aes = F) +
  scale_colour_manual("Condition",
    labels = c("-20ms", "-10ms", "+20ms"),
    values = c("#cc0000", "#12D432", "#0481F3")) +
  geom_segment(
    data =
      d.prior.PSE %>%
      summarise(PSE_median = median(PSE)) %>%
      uncount(3) %>%
      mutate(x = PSE_median) %>% bind_cols(xend = c(25, 35, 65), y = c(.45, .5, .55), yend = c(.45, .5, .55)),
    mapping = aes(x = x, xend = xend, y = y , yend = yend),
    alpha = 0.3,
    arrow = arrow(type = "closed", length = unit(0.02, "npc")),
    linetype = 2,
    inherit.aes = F) +
  annotate(
       geom = "text",
       x = c(30, 40, 55),
       y = c(.5, .55, .6),
       label = d.prior.PSE %>%
      summarise(PSE_median = median(PSE)) %>%
      uncount(3) %>%
        bind_cols(d.condition.exposure.IO %>%
                    group_by(Condition.Exposure) %>%
                    distinct(PSE)) %>%
        mutate(distance = paste(round(PSE - PSE_median), "ms")) %>% pull(distance),
       colour = colours.condition,
       fontface = "bold",
       size = 2.7) +
  ylab('Proportion "t"-responses') +
  scale_x_continuous(limits = c(-10, 95), breaks = c(25, 35, 45.1, 65)) +
  theme(legend.position = "top")
```

For the idealized learner model, we followed the same general procedure but instead fit Bayesian ideal observers against the labeled VOT distributions of each exposure condition. This makes it possible to assess how far participants in the different exposure conditions have converged against the exposure distributions. Unlike for the idealized pre-exposure listeners, which used five-fold cross-validation, we trained only one ideal observer per exposure condition. As mentioned above, attentional lapses during the experiment were very low, suggesting that listeners largely paid attention on all trials, and thus got more or less identical inputs.^[Some previous studies has similarly used idealized learners to test whether [@bejjanki2011; @clayards2008] or how far [@kleinschmidt-jaeger2016; @kleinschmidt2020] listeners have converged against the exposure distributions. These works have typically found that the categorization functions predicted by idealized learners are steeper than those of human listeners. Going beyond these previous works, we included an estimate of perceptual noise [obtained from @kronrod2016] in both the idealized pre-exposure listeners and the idealized learners. This turned out to improve the fit against listeners' perception. For details, see SI, \@ref(sec:model-guided).] Below, we use the predicted PSEs derived from these idealized models to better understand the adaptive changes participants' behavior.

```{r}
rm(
  d.Participant,
  d.exposure,
  category_means,
  d.VOT_exclusion,
  VOT_for_required_proportion_t,
  required_proportion_t,
  d.means,
  p.histogram_conditions)
```
