# Introduction
One of the hallmarks of human speech perception is its adaptivity. Listeners' interpretation of acoustic input can change within minutes of exposure to an unfamiliar talker, supporting robust speech recognition across talkers [@bradlow2008; @clarke2004; @xie2018; @xie2021]. Recent reviews have identified distributional learning of marginal cue statistics ['normalization', @apfelbaum-mcmurray2015; @mcmurray-jongman2011; @magnuson-nusbaum2007] or the statistics of cue-to-category mappings as an important mechanism affording this adaptivity ['representational learning', @clayards2008; @idemaru-holt2011; @kleinschmidt-jaeger2015; @davis-sohoglu2020; for review, @schertz-clare2020; @xie2023]. This hypothesis has gained considerable influence over the past decade, with findings that changes in listener perception are qualitatively predicted by the statistics of exposure stimuli [@bejjanki2011; @clayards2008; @idemaru-holt2020; @kleinschmidt2012; @munson2011; @nixon2016; @theodore-monto2019; @tan2021; for important caveats, see @harmon2019]. 

Viewing speech perception as an adaptive process has been pivotal in our understanding of how human listeners overcome the lack of invariance problem; a problem fully appreciated when one begins to map out the variability of acoustic-phonetic cues that point to a single linguistic category [e.g. @delattre1955; @peterson-barney1952; @newman2001]; compounded when talker sex, age, social class, dialect and a host of other contexts are factored into consideration. Listeners' aptitude at speech comprehension however, belies this challenge. Given the uncertainty involved it is not surprising models of speech perception that allow for probabilistic outcomes have left a lasting impression [@mcllelland-elman1986; NAM? @norris-mcqueen2008 FLMP? NAPP? ]. Yet, if a model is to elucidate on robust recognition under varying contexts it has to account for the implicit assumptions that listeners seem to bring to any speech perception task  with regard to cue-category mappings, and be able to explain how they reconcile these assumptions with recent input. Theories that explicitly bring this to bear include the influential exemplar models [@johnson1996; @pierrehumbert2001; @apfelbaum-mcmurray2015] and Bayesian inference models [@kleinschmidt-jaeger2015] (NOT SAYING THAT TRACE OR OTHER PROBABILISTIC TYPES OF MODELS DON'T ACCOUNT FOR PRIORS and updating BUT THEY DON'T SEEM OBVIOUSLY SO).  

Over the past 20 years there have been prolific investigations into how and when listeners adjust their phonological categories following exposure to atypical pronunciations of speech sounds. These acoustic manipulations take place at the margins of linguistic categories where perception can be heavily influenced by the contexts in which they are presented [@norris2003; @mcqueen2006 ; @kraljic-samuel2008; @eisner-mcqueen; etc]. A sound that is ambiguous between /s/ and /sh/ presented in the utterance *contradiction* would bias its interpretation as /sh/ since contradic*s*on is not a word. Repeated exposure to the sound in such biasing word contexts reliably elicits a shift in perception along the /s/-/sh/ continuum in subsequent testing -- those having heard the sound in /sh/-biasing words tend to give more /sh/ responses; vice-versa for those who were exposed to it in /s/-contexts. This perceptual recalibration of less prototypical category members has also been induced under audio-visual manipulations (@bertelson2003; @vroomen2007). The paradigm has been exploited to its fullest to investigate, among other things, the sustainability of perceptual changes [@eisner-mcqueen2006; @kraljic-samuel2005], its generalizability to members of the same phonological class [@kraljic-samuel2006], and its generalizability to other talkers [@kraljic-samuel; @reinisch-holt2014]. 

In general, these findings are compatible with exemplar and other probabilistic updating frameworks that link the distributions of the acoustic-phonetic cues to changes in category mappings hence perceptual recalibration findings can inform our general understanding of talker adaptation. Still, there remains debate over what mechanism in principle, motivates the perceptual changes observed. Some positions remain less specified than others. For instance the proposal that listeners expand category membership when confronted with talkers with atypical accents or pronunciations  or that they "relax their criteria" for category membership (@zheng-samuel2020; @schmale2012; @floccia2006; @bent2016). While it is possible that apparent perceptual shifts post-exposure can be explained by processes independent of distributional learning [see @xie2023 for simulations] what is needed in work going forward is better specified hypotheses coupled with stronger predictions and tests to weigh the evidence [@schertz-clare202x; @xie2023; @bent-baese-berk2021]. 

The application of analytic frameworks that facilitate modelling of perceptual processes conditioned on different assumptions is a step in that direction [e.g. ideal observers @feldman2009; @kronrod2016; ideal adaptor @kleinschmidt-jaeger2015; @hitczenko-feldman2016]

More recently the means by which to test and differentiate between closely related hypotheses founded on distributional learning have been facilitated by the formalisation of speech perception as rational inference with Bayesian belief-updating (Ideal Adaptor framework; @kleinschmidt-jaeger2015). In an ideal adaptor account speech sounds are represented as distributions of acoustic cues under a given linguistic category or context. At any given moment a listener holds prior beliefs about cue distributions formed over his or her long-term experience with talkers. Because cue distributions differ across talkers and contexts the listener infers the intended category of a novel talker by integrating prior beliefs about cue-category distributions with the present talker's input. How quickly a listener adapts to the speech of the talker depends on the degree of confidence the listener has about prior cue distributions. A greater confidence in priors requires a higher amount of evidence from the talker in order for the listener to update his beliefs about the cue distributions such that it aligns with that of the talker's. Conversely, low confidence in prior distributions would require less evidence for adaptation to happen.  On these principles various learning models can be built to simulate the amount and manner of exposure to talker inputs, allowing specific predictions of perceptual changes from each additional piece of input under different parametric assumptions [@kleinschmidt-jaeger2015; @xie2023; @hitczenko-feldman2016 for examples]. 

A characteristic prediction in an ideal adaptor account of adaptation is that one should be able to observe graded (or incremental) outcomes in line with the distribution of the acoustic input as well as the frequency of the input (amount of evidence). Recently @cummings-theodore2023 sought to test this core tenet with a series of perceptual recalibration experiments. Listeners were exposed to an ambiguous sound mid-way between two categories, /s/ and /sh/ either in an /s/-biasing or /sh/-biasing word contexts. They were then tested on their categorisation behaviour on a continuum of ambiguous /s/-/sh/ sounds. Depending on the context in which listeners heard the ambiguous sound, they will tend to categorise the sounds more often in the direction of the biasing context. By manipulating the number of times an ambiguous sound between /s/ and /sh/ was heard between participants and within each biasing context (1, 4, 10 or 20 occurences) they showed that the size of the putative perceptual recalibration effect correlated with the frequency of exposure. This kind of statistically modulated change is rarely demonstrated in perceptual recalibration not least because of how those studies are typically designed. It is more common to dispense exposure in one block followed by a block of test trials because the effect of interest is simply whether the average categorisation functions of the two groups diverge or not post-exposure. While @cummings-theodore2023 found graded effects between-participants, it should in principle be possible to detect graded change within participants as well had they been given the full extent of critical trials but tested after each incremental input.
Such a build-up in perceptual recalibration was in fact tested in @vroomen2007. Participants were exposed to an ambiguous sound between /aba/ and /ada/ paired with videos of a person either clearly articulating "aba" or "ada". Interspersed between the first 64 exposure trials of the experiment were test trials to check for incremental build-up in the recalibration effect. In their modelling of this study @kleinschmidt-jaeger2011 found the ideal adaptor to fit the data well indicating that listeners integrate each observed piece of evidence with their priors and updated their beliefs in a way that was quantitatively predicted by the model.

So far we have discussed findings of gradient changes in perceptual recalibration studies that repeatedly expose listeners to tokens made from a single synthesised ambiguous token, or in the case of @cummings-theodore2023, tokens that are a blend of the cues from the /s/ and /sh/ utterances of the critical exposure words. The main manipulation is the number of exposures to the ambiguous sounds with the predicted effect being the quantity of difference between groups or in the case of incremental testing, the quantity of difference between trials. 

In the distributional learning experimental paradigm clusters of cue values are constructed and centered around specific mean values with specific variances typically along a single acoustic dimension to simulate within talker variability. These cues are mapped onto two adjacent sound categories, giving a bimodal distribution. Listeners are exposed to the distribution in the context of minimal pair words and are tasked to categorise what they believe they heard. It is the the locations of the means of these distributions or category variances that are manipulated between groups. Through exposure to these distributions, supervised or unsupervised, and with sufficient numbers of trials listeners are expected to learn the talker's distributions and therefore show categorization patterns that correspond to the exposure distribution.
@kleinschmidt-jaeger2016 for instance exposed L1-US English listeners to recordings of /b/-/p/ minimal pair words like *beach* and *peach* that were acoustically manipulated. Separate groups of listeners were exposed to distributions of voice onset times (VOTs)---the primary cue distinguishing  words like *beach* and *peach*---that were shifted by up to +30 ms, relative to what one might expect from a 'typical' talker (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)A). In line with the distributional learning hypothesis, listeners' category boundary or point of subjective equality (PSE)---i.e., the VOT for which listeners are equally likely to respond "b" or "p"---shifted in the same direction as the exposure distribution (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)B). Also in line with the distributional learning hypothesis, these shifts were larger the further the exposure distributions were shifted. An incremental Bayesian belief updating model was fit to the data which showed that within each exposure group listeners gradually increased the shift of their categorisations as they heard more trials [@kleinschmidt2020].

This initial finding of incremental belief updating within the distributional learning paradigm while noteworthy, is not without its limitations. Distributional learning studies have not typically been set up as exposure-test configurations. All exposure trials are in a sense test trials as well. This means that the estimated categorisation function is fit over a number of trials decided on by the researcher -- for e.g. @kleinschmidt-jaeger2016 apportioned their 222 trials into 6 segments. Accordingly, past studies have not implemented standard test trials across groups. 

The present study aims to build on the pioneering work of @clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019; @kleinschmidt2020 with some design innovations that we believe affords a productive test of the core claims of the ideal adaptor. We aim to replicate the findings of @kleinschmidt-jaeger2016 by manipulating exposure distributions between groups while empirically investigating incremental learning by introducing a novel test-exposure-test design. Annother novelty this study possibly brings is a document of how soon, from the onset of exposure, does the distributional learning effect emerge. This is something that remains opaque in previous studies because of their lack of test blocks. Given the substantial amount of evidence that adaptation takes place rapidly (e.g. 5 mins from exposure in L2 accent adaptation; 4 - 10 trials in lexically guided perceptual recalibration) we might expect listeners to also show learning effects very early on in an experiment. On the other hand, it is possible that distributional learning effects may show up later than one would expect in perceptual recalibration since listeners have a presumably more challenging task of inferring the means of two categories over a range of cues.

In experimental work researchers often have to consider the generalizability of their results which leads to questions about ecological validity. There is a trade-off between ecological validity of experimental design and the desired degree of control over the variables. Questions about ecological validity of prior work in distributional learning pertains to 2 features. First, the stimuli which were generated with a synthesiser, had an obvious machine-like quality [@kleinschmidt-jaeger2016; @clayards2008]. Secondly, the pairs of distributions of voiced and voiceless categories were always identical in their variances [see also @theodore-monto2019] which adds to the artificiality of the experiment. In our design description we show how we can begin to improve on these features through the stimuli and the setting of exposure conditions. 

**END OF INTRODUCTION



<!-- 
However, Kleinschmidt and Jaeger also observed a previously undocumented property of these adaptive changes: shifts in the exposure distribution had less than proportional (sublinear) effect on shifts in PSE (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)C). While this finding is broadly compatible with the hypothesis of distributional learning, it points to important not well-understood constraints on adaptive speech perception.-->


(ref:kleinschmidt-jaeger-2016-replotted) Design and results of @kleinschmidt-jaeger2016 replotted. **Panel A:** Different groups of participants were exposed to different shifts in the mean VOT of /b/ and /p/. **Panel B:** categorization functions of individual participants depending on the exposure condition (shift in VOT means of /b/ and /p/). For reference, the black dashed line shows the categorization function of the 0-shift condition. The colored dashed lines shows the categorization function expected for an ideal observer that has fully learned the exposure distributions. **Panel C:** Mean and 95% CI of participants' points of subjective equality (PSEs), relative to the PSE of the ideal observers.

```{r kleinschmidt-jaeger-2016-refitted}
# load K&J2016 data and filter to semi-supervised rows
d.KJ16 <- 
  supunsup_clean %>%
  filter(supCond == "mixed" | supCond == "supervised") %>%
  # rename variables so that they match the naming conventions employed in the remainder
  # of this paper.
  rename(
    ParticipantID = subject,
    Item.VOT = vot,
    Condition.Exposure = bvotCond,
    Response.Voiceless = respP,
    Item.MinimalPair = wordClass,
    category = respCategory) %>%
  mutate(
    Condition.Exposure = factor(case_when(
      Condition.Exposure == 0 ~ "+0ms",
      Condition.Exposure == 10 ~ "+10ms",
      Condition.Exposure == 20 ~ "+20ms",
      Condition.Exposure == 30 ~ "+30ms")),  
    Condition.Exposure = fct_relevel(
      Condition.Exposure, c("+0ms", "+10ms", "+20ms", "+30ms"))) 

d.KJ16_unlabeled <- 
  d.KJ16 %>% 
  filter(labeled == "unlabeled") %>%
  group_by(ParticipantID) %>%
  # filter to final 6th of total trials
  slice_tail(n = 37)

VOT.mean_d.KJ16_unlabeled <- mean(d.KJ16_unlabeled$Item.VOT)
VOT.sd_d.KJ16_unlabeled <- sd(d.KJ16_unlabeled$Item.VOT)
d.KJ16_unlabeled %<>% mutate(VOT_gs = (Item.VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled))

contrasts(d.KJ16_unlabeled$Condition.Exposure) <-
  cbind(" 10vs0" = c(-3/4, 1/4, 1/4, 1/4),
        " 20vs10" = c(-1/2, -1/2, 1/2, 1/2),
        " 30vs20" = c(-1/4, -1/4, -1/4, 3/4))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 1 + VOT_gs * Condition.Exposure +
        (1 + VOT_gs | ParticipantID) +
        (1 + VOT_gs * Condition.Exposure | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .99),
    file = "../models/KJ16-semisupervised-GLMM.rds")

# fit nested model to extract slopes and intercepts
fit_nested_KJ16 <- 
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~  0 + (Condition.Exposure) / VOT_gs +
        (0 + VOT_gs | ParticipantID) +
        (0 + (Condition.Exposure) / VOT_gs | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000, 
    warmup = 2000, 
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .995),
    file = "../models/KJ16-semisupervised-nested-GLMM.rds")

cond_fit_KJ16 <- fit_KJ16 %>% 
  conditional_effects(
  effects = "VOT_gs:Condition.Exposure",
  method = "posterior_epred",
  ndraws = 500,
  re_formula = NA) %>% 
  .[[1]] %>% 
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled))

d.KJ16_PSE <- 
  fit_nested_KJ16 %>% 
  gather_draws(`b_mu2_Condition.Exposure.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(
    term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept"),
    .variable = gsub("b_mu2_Condition.ExposureP(\\d{1,2}ms).*$", "\\1", .variable)) %>% 
  pivot_wider(names_from = term, values_from = ".value") %>%
  rename(Condition.Exposure = .variable) %>% 
  relocate(c(Condition.Exposure, Intercept, slope, .chain, .iteration, .draw)) %>% 
  mutate(
    PSE = descale(-Intercept/slope, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled),
    Condition.Exposure = paste0("+", Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), mean = mean, upper = ~ quantile(.x, probs = .975))))
```


```{r io-categorization-kleinschmidt-jaeger-2016}
# Test points by condition
x <- 
  d.KJ16_unlabeled %>% 
  group_by(Condition.Exposure) %>% 
  distinct(Item.VOT) %>% 
  rename(x = Item.VOT) 

# get io categorizations of the test points by condition
io.d.KJ16 <- 
  make_MVG_ideal_observer_from_data(
  d.KJ16 %>% 
    rename(VOT = Item.VOT) %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure", 
    cues = c("VOT"), 
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>% 
  nest(io = -c(Condition.Exposure)) %>% 
  left_join(x) %>% 
  mutate(x = map(x, ~ c(.x))) %>% 
  nest(x = x) %>% 
  mutate(categorization = map2(
    x, io,
    ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% 
    mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>% 
  unnest(cols = categorization, names_repair = "unique") %>% 
  pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_b = round(`response_b` * 10^12), n_p = 10^12 - n_b) %>% 
  group_by(Condition.Exposure) %>% 
  nest(data = -c(Condition.Exposure)) %>% 
  mutate(
    model_unscaled = map(
      data, ~ glm(
      cbind(n_p, n_b) ~ 1 + VOT, 
      family = binomial, 
      data = .x)),
    intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_unscaled = map_dbl(
      model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
    model_scaled = map(data, ~ glm(
      cbind(n_p, n_b) ~ 1 + I((VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled)), 
      family = binomial, 
      data = .x)),
    intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
    PSE = -intercept_unscaled/slope_unscaled)

# get io with lapse-accounted categorization 
io.d.KJ16.lapse_rate <- 
  make_MVG_ideal_observer_from_data(
    data = d.KJ16 %>% 
      rename(VOT = Item.VOT) %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure", 
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT")),
    lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>% 
  nest(io = -c(Condition.Exposure)) %>% 
  crossing(x = seq(-10, 70, .5)) %>% 
  nest(x = x) %>% 
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>% 
           filter(category == "p") %>% 
           mutate(VOT = map(x, ~.x[1]) %>% unlist()))) %>% 
  unnest(categorization, names_repair = "unique")

# get io of typical talker and its categorization (+0 condition)
d.typical_talker <- 
  io.d.KJ16[[2]][[1]][1, 1] %>% 
  unnest(io) %>% 
  mutate(lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>% 
  nest(io = everything()) %>% 
  crossing(x = seq(-10, 70, .5)) %>% 
  nest(x = x) %>% 
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>% 
           filter(category == "p") %>% 
           mutate(VOT = map(x, ~.x[1]) %>% unlist())),
    line = map(
      categorization, 
      ~ geom_line(
          data = .x,
          mapping = aes(x = VOT, y = response),
          linetype = 2,
          linewidth = 0.6,
          alpha = .8,
          colour = "black")))
```




```{r kleinschmidt-jaeger-2016-replotted, fig.height=base.height*3.5, fig.width=base.width*5.5, fig.cap="(ref:kleinschmidt-jaeger-2016-replotted)"}
# make histograms of exposure distributions
p.KJ16.histogram <-
  d.KJ16 %>% 
  group_by(Condition.Exposure) %>%
  slice_head(n = 222) %>% 
  ggplot(aes(x = Item.VOT,
             fill = paste(Condition.Exposure, trueCat))) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous(breaks = c(0, 25, 50)) +
  scale_fill_manual(
    "Category", 
    values = c("+0ms b" = "#f8766d",
               "+0ms p" = "#fdd1ce",
               "+10ms b" = "#7cae00",
               "+10ms p" = "#d4ff66",
               "+20ms b" = "#00bfc4",
               "+20ms p" = "#99fcff",
               "+30ms b" = "#c77cff",
               "+30ms p" = "#e9ccff"),
    aesthetics = "fill",
    labels = c("/b/", "/p/", "", "", "", "", "", "")) +
   guides(
    fill = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", NA, NA, NA, NA, NA, NA),
        values = c("b", "p", NA, NA, NA, NA, NA, NA)), nrow = 1)) +
  facet_wrap(~ Condition.Exposure, nrow = 1) +
  theme(legend.justification = "left") +
  remove_x_guides
  
p.KJ16.fit <- 
  cond_fit_KJ16 %>% 
  rename(Condition = Condition.Exposure) %>% 
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    ymin = lower__, ymax = upper__, fill = Condition), 
    alpha = .1,
    show.legend = F) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    color = Condition),
    linewidth = .7,
    alpha = 0.6,
    show.legend = F) +
  stat_summary(
    data = d.KJ16_unlabeled %>% 
      rename(Condition = Condition.Exposure) %>% 
      group_by(Condition),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      group = Condition,
      colour = Condition),
    geom = "pointrange",
    size = 0.15,
    show.legend = F) +
  geom_line(
    data = io.d.KJ16.lapse_rate %>% 
      rename(Condition = Condition.Exposure) %>% 
      group_by(Condition),
    mapping = aes(x = VOT, y = response, group = Condition, colour = Condition),
    linetype = 2,
    linewidth = 1,
    alpha = .6,
    inherit.aes = F,
    show.legend = F) +
  d.typical_talker$line +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Proportion \"p\"-responses", breaks = c(0, .5, 1)) +
  facet_wrap(~ Condition, nrow = 1)

p.KJ16.PSE <- 
  d.KJ16_PSE %>% 
  left_join(io.d.KJ16) %>% 
  rename(PSE.io = PSE) %>% 
  ggplot(aes(y = PSE_mean, x = Condition.Exposure, colour = Condition.Exposure)) +
  geom_hline(
    yintercept = c(20, 30, 40, 50), 
    linewidth = 1.5,
    alpha = .4,
    linetype = 2, 
    colour = scales::hue_pal()(4)) +
  geom_point(size = 2, show.legend = F) +
  geom_linerange(
    aes(ymin = PSE_lower, ymax = PSE_upper), size = 1, alpha = .8, show.legend = F) +
  scale_x_discrete("Condition") +
  scale_y_continuous("PSE") +
  theme(axis.text.x = element_text(angle = 22.5, hjust = .8))

layout <- "
AAAA#
BBBBC"

p.KJ16.histogram + p.KJ16.fit + p.KJ16.PSE +
  plot_layout(design = layout,
              guides = "collect") +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(legend.position = "top",
        plot.tag = element_text(face = "bold"))
```

For example, influential models of adaptive speech perception predict proportional, rather than sublinear, shifts (for proof, see SI \@ref(sec:ibbu-proof)). This is the case both for incremental Bayesian belief-updating model [@kleinschmidt-jaeger2011] and general purpose normalization accounts [@mcmurray-jongman2011]---models that have been found to explain listeners' behavior well in experiments with less substantial changes in exposure. There are, however, proposals that can accommodate this finding. Some proposals distinguish between two types of mechanisms that might underlie representational changes, <!--- link representational change and distributional learning --> *model learning* and *model selection* [@xie2018, p. 229]. The former refers to the learning of a new category representations---for example, learning a new generative model for the talker [@kleinschmidt-jaeger2015, Part II] or storage of new talker-specific exemplars [@johnson1997; @sumner2011]. Xie and colleagues hypothesized that this process might be much slower than is often assumed in the literature, potentially requiring multiple days of exposure and memory consolidation during sleep [see also @fenn2013; @tamminen2012; @xie2018sleep]. Rapid adaptation that occurs within minutes of exposure might instead be achieved by selecting between *existing* talker-specific representations that were learned from previous speech input---e.g., previously learned talker-specific generative models [see mixture model in @kleinschmidt-jaeger2015, p. 180-181] or previously stored exemplars from other talkers [@johnson1997]. Model learning and model selection both offer explanations for the sublinear effects observed in @kleinschmidt-jaeger2016. But they suggest different predictions for the evolution of this effect over the course of exposure.

Under the hypothesis of model learning, sublinear shifts in PSEs can be explained by assuming a hierarchical prior over talker-specific generative models [$p(\Theta)$ in @kleinschmidt-jaeger2015, p. 180]. This prior would 'shrink' adaptation towards listeners' priors---similar to the effect of random by-subject or by-item effects in generalized linear mixed-effect models, which shrink group-level effect estimates towards the population mean of the data [@baayen2008]. Critically, as long as these priors attribute non-zero probability to even extreme shifts (e.g., the type of Gaussian prior used in mixed-effects models), this predicts listeners' PSEs will continue to change with increasing exposure until they have converged against the PSE that is ideal for the exposure statistics. In contrast, the hypothesis of model selection predicts that rapid adaptation is more strictly constrained by previous experience: listeners can only adapt their categorization functions up to a point that corresponds to (a mixture of) previously learned talker-specific generative models. This would imply that at least the earliest moments of adaptation are subject to a hard limit (Figure \@ref(fig:prediction)): exposure helps listeners to adapt their interpretation to more closely aligned with the statistics of the input, but only to a certain point.

(ref:prediction) Contrasting predictions of model learning and model selection hypotheses about the incremental effects of exposure on listeners' categorization function. Both hypothesis predict incremental adaptation towards the statistics of the input, as well as constraints on this adaptation. The two hypotheses differ, however, in that model selection predicts a hard limit on how far listeners' can adapt during initial encounters with an unfamiliar talker.

```{r prediction, fig.height=base.height+1/4, fig.width=base.width*2, fig.cap="(ref:prediction)"}
k <- 10^-1

crossing(
  Exposure = 0:100,
  Shift = c(20, 40),
  Hypothesis = c("model learning", "model selection")) %>%
  mutate(
    PSE = 25 + ifelse(Hypothesis == "model learning", Shift, pmin(Shift, 25)) * 
      (1 - exp(-k * Exposure))) %>%
  ggplot(aes(x = Exposure, y = PSE, color = factor(Shift))) +
  geom_hline(
    data = crossing(Shift = c(0, 20, 40), Hypothesis = c("model learning", "model selection")),
    aes(yintercept = Shift + 25, color = factor(Shift)), linetype = 2) +
  geom_line(alpha = .5) +
  scale_y_continuous("PSE (in ms VOT)") +
  scale_color_manual(breaks = c("0", "20", "40"), values = c("gray", "green", "blue")) +
  facet_wrap(~ Hypothesis) +
  guides(color = "none") +
  theme(panel.grid = element_blank())
```

The present study employs a novel incremental exposure-test paradigm to address two questions. We test whether the sublinear effects of exposure observed in recent work replicate for exposure that (somewhat) more closely resembles the type of speech input listeners receive on a daily basis. And, we evaluate the predictions of the model learning and selection hypotheses against human perception. We take this question to be of interest beyond the specific hypotheses we contrast: whether there are hard limits to the benefits of exposure to unfamiliar speech patterns ultimately has consequences for education and medical treatment. 

All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models (see SI, \@ref(sec:software)).





