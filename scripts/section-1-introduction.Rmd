```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Successful speech recognition requires that listeners map the acoustic signal onto words and meanings. But this signal-to-meaning mapping varies across talkers and context. The same word spoken by different talkers can sound quite different; and conversely, the same acoustic signal can imply different words depending on the talker. Yet, listeners typically recognize speech quickly and accurately across a wide range of talkers and acoustic conditions (after decades of advances, automatic speech recognition is now approaching the accuracy that most listeners display effortlessly during everyday speech perception).

Research has identified *adaptivity* as a key component to the robustness of human speech perception. Although first encounters with an unfamiliar accent can cause initial processing difficulty, this difficulty can diminish rapidly with exposure [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. For instance, twenty or so short sentences from an unfamiliar second language accent can significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Under ideal conditions, deviations from expected pronunciations along familiar phonetic dimensions can be detected after even shorter exposure [@cummings-theodore2023; @kleinschmidt-jaeger2012; @liu-jaeger2018], sometimes as few as a single trial [@vroomen2007]. Findings like these suggest that speech perception is highly adaptive, allowing listeners to quickly adjust the mapping from acoustics to phonetic categories and word meanings. While we now may take such rapid adaptation for granted, its first demonstrations challenged long-held assumptions, and spurred the development of new paradigms and theories [reviewed in @bent-baeseberk2021; @cummings-theodore2023; @schertz-clare2020; @zheng-samuel2023]. Researchers had, of course, long known that exposure can affect speech perception in adults---with enough time, we can after all learn new languages. However, the rapid adaptive processes described here unfold over much shorter times scales (second and minutes), raising questions about the underlying mechanisms.

Indeed, it remains unclear *how* such rapid adaptation is achieved. How do listeners integrate information from a new talker, and how does this come to incrementally change their interpretation of that talker's speech? This is the question we seek to contribute to here. To appreciate our approach to this question, it is helpful to briefly reflect on the state of the field. Research on adaptive speech perception tends to discuss informal---often descriptive, rather than explanatory---hypotheses [see discussion in @norris-cutler2021]. This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Such descriptions do not specify what mechanisms support adaptive speech perception, nor do they make predictions about how adaptation unfolds incrementally with each new observation from an unfamiliar talker (the same criticism applies to some of our own work). 

Viewed from the perspective of such informal hypotheses, research on adaptive speech perception can be an open-ended list of questions. Is adaptation more or less immediate, or does it unfold gradually? If the latter, do changes in listeners' behavior accumulate additively, leading to more or less linear changes in behavior? If changes are non-linear, are they first slow and then fast, or first fast and then slow? And, how does listeners' prior experience affect how listeners adapt? Are there limits to listeners' ability to fully adapt to a new talker? Or can we adapt to more or less any accent provided sufficient input? Under a question- rather than theory-driven approach, each such question can be---and often is---viewed in isolation. <!-- Integration of findings across studies is limited to intuitions of the type described in the preceding paragraph---intuitions that can be misleading [see discussion of "category expansion" in @hitczenko-feldman2016; "boundary shift" in @kleinschmidt-jaeger2015]. --> Integration of findings across studies is left to reviews, which then are limited to qualitative integration. This makes it difficult develop and test theoretical models that are sufficiently predictive to detect *informative incompatibility* with the data. As Allen Newell wrote more than 50 years ago "you can't play 20 questions with nature and win" [@newell1973, p. 1].

Critically, there *are* theories that make clear, quantifiable predictions about all of the questions in the preceding paragraph, including basic predictions that remain untested. One of the most developed of these theories is the hypothesis that adaptive speech perception draws on *distributional learning* [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assmann2007; @sohoglu-davis2016]. While distributional learning models differ from each other in important aspects, they share the central assumption that listeners incrementally learn and store information about talkers' speech. This includes information about the phonetic distributions that characterize the talker's speech, such as the average values of phonetic cues, their variability, or even the full phonetic distributions of all speech categories. These statistical properties are then used to interpret subsequent speech from the talker, supporting robust speech recognition across talkers [reviewed in @schertz-clare2020; @xie2023]. 

With these assumptions come shared predictions that we introduce next. Then we discuss why recent reviews conclude that we do not know whether distributional learning explains rapid adaptation [@bent-baeseberk2021; @xie2023], motivating the present work. In brief, it is one thing to show that rapid changes in speech perception are *qualitatively* compatible with models of distributional learning---e.g., that prior experience with an accent facilitates perception [e.g., @porretta2017; @witteman2013], or that additional exposure further improves subsequent speech perception [e.g., @escudero-williams2014; @REF-ANY-OTHER-FROM-REVIEWERS]. It is a rather different thing to test whether distributional learning can predict the *quantitative* changes in listeners' perception *well*, accounting for a substantial share of listeners' behavior---as expected if distributional learning is indeed a critical mechanism for rapid changes in speech perception. It is this latter question we seek to contribute to, taking a step towards the integrative, theory-driven vision outlined in @newell1973, "forcing enough detail and scope to tighten the inferential web that ties our experimental studies together" [p. 24]. As we discuss below, this need for stronger, more fine-grained, quantitative evaluations of existing theories requires new combinations of paradigms, analyses and model-guided interpretation [see discussion in @coretta2023; @yarkoni-westfall2017]. But this effort can pay off: to anticipate one of our results, the 'stress test' that we submit distributional learning models to suggests an important, previously unrecognized, limit of existing models.

## Predictions of distributional learning
Consider a listener's initial encounter with an unfamiliar talker who produces some sounds, e.g. /d/ and /t/, in an unexpected way (Figure \@ref(fig:predictions)A). Listeners' perception is predicted to change incrementally with exposure to the talker's speech (Figure \@ref(fig:predictions)B-C). Distributional learning models make four predictions about how these changes unfold incrementally. First, the direction and magnitude of that change should gradiently depend on listeners' prior expectations based on relevant previously experienced speech input from other talkers (**prediction 1 - *prior expectations***), and both the amount (**prediction 2a - *exposure amount***) and distribution of phonetic cues in the exposure input from the unfamiliar talker [**prediction 2b - *exposure distribution* **, for review, see @xie2023]. Specifically, listeners' categorization functions---the mapping from acoustics to phonetic categories and words---should gradually shift from a starting point that reflects the statistics of previously experienced speech towards a target that reflects the statistics of the new talker's speech. Standard distributional learning models further predict that this shift proceeds until the listener has fully learned the statistics of the new talker's speech (**prediction 3 - *learn to convergence***), though alternative models exist [e.g., selection or reweighting between already learned representations, @kleinschmidt-jaeger2015, Part II]. Finally, some distributional learning models further commit to specific learning mechanisms that constrain how exactly adaptation is expected to accumulate incrementally: both error-driven theories [@harmon2019; @olejarczuk2018; @sohoglu-davis2016] and theories of ideal information integration [@kleinschmidt-jaeger2015; @kleinschmidt2020] predict that adaptation initially proceeds quickly and then slows down as the listener approaches the correct mapping from the acoustic signal to phonetic categories (**prediction 4 - *diminishing returns***).^[Predictions 1-4 assume that listeners *know* that they are listening to the same new talker. Talker recognition is itself an active inference process that we do not further discuss here [but see @kleinschmidt-jaeger2015; @magnuson-nusbaum2007].] Such diminishing returns---a.k.a. the power law of learning [@crossman1959]---are predicted by many learning theories, and have been demonstrated across a wide range of learning phenomena [e.g., @anderson1990; @logan1988; @palmeri1997; @rescorla-wagner1972]. It is, however, unknown whether rapid changes in speech perception reflect the same type of learning, or any learning in the more narrow sense at all [see discussion in @xie2018].

Figure \@ref(fig:predictions)D illustrates predictions 3 and 4 and contrasts them with other possible scenarios, such as immediate talker switching; linear, rather than sublinear, changes to the point of convergence; or convergence against partial adaptation. These scenarios are not meant as an exhaustive list, but rather to illustrate that there are many possible ways that adaptive speech perception might unfold---some more plausible than others. 'Premature convergence' against partial adaptation, for example, could result from strong constraints on distributional learning, as expected if the early moments of adaptation are limited to the reweighting of previously learned dialect or sociolect representations [see discussion in @wade2022; @xie2018]. 

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally, using the pronunciation of US English word-initial /d/ and /t/ as an example (as in "dip" vs. "tip"). **Panel A:** Transparent lines indicate cross-talker variability in the realization of /d/ and /t/ along the primary cue used to distinguish them (voice onset timing or VOT). Shown are 20 random talkers from a database of connected speech [@chodroff-wilson2018]. The thicker solid lines indicate a 'typical' talker (averaging over all talkers in the database). Dashed lines indicate a hypothetical unfamiliar talker with a noticeably different distribution of VOT values. **Panel B:** Ideal categorization functions along the phonetic VOT continuum for speech from a typical talker (*idealized pre-exposure listener*, SI \@ref(sec:idealized-prior-listeners)) and speech from the unfamiliar talker (*idealized learner* that has fully learned that talkers distributions, SI \@ref(sec:idealized-learners)). Arrows point to the points of subjective equality (PSE), the point along the phonetic VOT continuum at which listeners are equally likely to identify a sound as an instance of /d/ or /t/. **Panel C:** The same as in Panel B but just showing the PSE. **Panel D:** Different ways in which listeners' PSEs along the phonetic VOT continuum might incrementally change with increasing exposure to the unfamiliar talker (from more transparent to less transparent). The horizontal lines indicates the ideal PSEs from Panel C. 

```{r predictions, fig.height=base.height*3+2/3, fig.width=base.width*5, fig.cap="(ref:predictions)", fig.pos="H"}
# Create all talker-specific IOs for connected speech data, using only the VOT cue
d.talker_IO.VOT <- 
  make_IOs_from_data (
    data = d.chodroff_wilson.connected,
    cues = c("VOT"),
    groups = "Talker") 

# Add x, PSE, categorization, and get gaussian geoms
d.talker_IO.VOT %<>%
  nest(io = -Talker) %>%
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  add_gaussians_as_geoms_to_io(alpha = .2, linetype = 1, linewidth = .5) %>%
  bind_rows(
    # Do the same after aggregating all talker-specific IOs into a single 'typical' talker IO
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(Talker = "typical") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = 1, linewidth = 1.2),
    # Do the same after aggregating all talker-specific IOs into a single talker that is shifted by 35ms relative to the 'typical' talker
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(
        mu = map(mu, ~ .x + 35),
        Talker = "unfamiliar") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = "twodash", linewidth = 1.2))

p.gaussian <- 
  ggplot() +
  (d.talker_IO.VOT %>% 
     filter(
       !(Talker %in% c("typical", "unfamiliar")),
       Talker %in% sample(unique(Talker), 20)) %>% 
     pull(gaussian)) + 
  (d.talker_IO.VOT %>% 
     filter(Talker == "typical") %>% .$gaussian) +
  scale_colour_manual(values = c(colours.category_greyscale)) +
  guides(colour = "none") +
  new_scale_colour() +
  (d.talker_IO.VOT %>% 
     filter(Talker == "unfamiliar") %>% .$gaussian) +
  scale_colour_manual("Category", values = c("#02427e", "#b4dafe")) +
  guides(color = guide_legend(override.aes = list(size = 0.5, colour = c(colours.category_greyscale), values = c("/d/", "/t/")))) +
  labs(x = "VOT (ms)", y = "Density") +
  theme(
    legend.background = element_rect(fill='transparent'),
    legend.box.background = element_rect(fill='transparent'),
    legend.key = element_rect(fill = "transparent"),
    legend.key.size = unit(0.5, "cm"),
    legend.key.spacing = unit(0.05, "cm"),
    legend.key.height = unit(0.01, "cm"),
    legend.title = element_text(size = 7), 
    legend.text = element_text(size = 6),
    legend.position = "inside",
    legend.position.inside = c(0.79,0.8))

p.cat <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>% 
  select(Talker, category, categorization, PSE) %>% 
  filter(category == "/t/") %>% 
  unnest(categorization, names_repair = "unique") %>% 
  ggplot(aes(x = VOT, y = response, group = Talker, colour = Talker, linetype = Talker)) +
  geom_line(linewidth = 1.2) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  scale_linetype_manual(values = c("solid", "twodash")) +
  #scale_x_continuous("VOT (ms)", limits = c(-25, 130), breaks = c(0, 37, 72, 100)) +
  scale_y_continuous('Proportion "t"-responses', breaks = c(0, .5, 1)) +
  guides(linetype = "none", colour = "none") +
   geom_segment(
     data = 
       d.talker_IO.VOT %>% 
       filter(Talker %in% c("typical", "unfamiliar")) %>%
       mutate(x = PSE, xend = PSE, y = .5, yend = .01),
    mapping = aes(x = x, xend = xend, y = y , yend = yend, color = Talker),
    alpha = 0.5,
    arrow = arrow(type = "open" , length = unit(0.04, "npc")),
    inherit.aes = F) +
  scale_x_continuous(limits = c(15, 90), breaks = c(37, 72)) +
  scale_colour_manual(values = c("grey", "#02427e"))


p.PSE <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>%
  select(Talker, category, PSE) %>% 
  group_by(Talker) %>% 
  filter(category == "/t/") %>% 
  ggplot(aes(x = PSE, y = Talker, colour = Talker)) +
  geom_point(size = 1.5) +
  scale_colour_manual(values = c("grey", "#02427e"), guide = "none") + 
  scale_x_continuous("Ideal PSE (ms VOT)", limits = c(30, 75), breaks = c(37, 72)) +
  scale_y_discrete("Talker") +
  theme(axis.text.y = element_text(angle = 90, vjust = .5, hjust = 0.5))
 
PSE_change <- 
  expand_grid(
    PSE = c(),
    learning_pattern = factor(c("immediate", "linear", "diminished", "premature")),
    exposure_amount = factor(c("none", "1/3", "2/3", "complete"))) %>%
  mutate(
    learning_pattern = fct_relevel(learning_pattern, "immediate", "linear", "diminished", "premature"),
    exposure_amount = fct_relevel(exposure_amount, "none", "1/3", "2/3", "complete"),
    PSE = case_when(
      exposure_amount == "none" ~ 37,
      exposure_amount == "complete" & learning_pattern %in% c("immediate", "linear") ~ 72,
      exposure_amount %in% c("1/3", "2/3") & learning_pattern == "immediate" ~ 72,
      exposure_amount == "1/3" & learning_pattern == "linear" ~ 37 + ((72 - 37 + 1)/3),
      exposure_amount == "2/3" & learning_pattern == "linear" ~ 37 + 2 * ((72 - 37 + 1)/3),
      exposure_amount == "1/3" & learning_pattern == "diminished" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount == "2/3" & learning_pattern == "diminished" ~ 37 + 1.6 * ((72 - 37 + 1)/2),
      exposure_amount == "complete" & learning_pattern == "diminished" ~ 37 + 1.85 * ((72 - 37 + 1)/2) ,
      exposure_amount == "1/3" & learning_pattern == "premature" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount %in% c("2/3", "complete") & learning_pattern == "premature" ~ 37 + 25))

p.PSE_change <- 
  PSE_change %>% 
  ggplot(aes(exposure_amount, PSE, alpha = exposure_amount, group = 1), colour = "#02427e") +
  geom_rect(
    data = PSE_change  %>% 
      nest(data = c(exposure_amount, PSE)) %>% 
      group_by(learning_pattern) %>% 
      slice_sample(n = 1) %>% 
      select(learning_pattern) %>% 
      mutate(ymin = ifelse(learning_pattern == "diminished", -Inf, NA),
             ymax = ifelse(learning_pattern == "diminished", Inf, NA)),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "yellow",
    alpha = .15,
    inherit.aes = F) +
  geom_point(size = 1.8, colour = "#02427e") +
  geom_line(alpha = .3) +
  geom_hline(aes(yintercept = 37), linetype = 1, alpha = .8, colour = "grey") +
  geom_hline(aes(yintercept = 72), linetype = 2, alpha = .6, colour = "#02427e") +
  scale_x_discrete("Exposure amount", breaks = c("none", "complete"), labels = c("none", "full")) +
  scale_y_continuous("PSE (ms VOT)", limits = c(25, 75), breaks = c(30, 40, 50, 60, 70)) +
  guides(alpha = "none") +
  facet_wrap(~learning_pattern, nrow = 1, labeller = labeller(learning_pattern = c("immediate" = "immediate switch", "linear" = "linear", "diminished" = "convergence w/\ndiminishing returns", "premature" = "premature convergence\nw/ diminishing returns"))) +
  theme(strip.text = element_text(size = 9))


((p.gaussian | p.cat | p.PSE) / p.PSE_change) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

## What is (not) known about these predictions?
We discuss the available evidence for prediction 1-4 (summarized in Table \@ref(tab:predictions) below) in more depth after presenting our own results. For now, we briefly illustrate why recent reviews have called for new experimental paradigms and analysis approaches that can more strongly constrain theories of adaptive speech perception [@bent-baeseberk2021; @schertz-clare2020; @xie2023]. Based on computational simulations, Xie and colleagues argue that such strong tests require (a) information about the distribution of phonetic cues in both listeners' prior experience and during exposure, (b) paradigms that measure incremental changes in listeners' behavior both within and across exposure conditions, and, critically, (c) analyses that quantitatively link the latter to the former---ideally, while comparing those changes in listeners' behavior to quantitative predictions of distributional learning models. 

<!-- TO DO: think about where references proposed by reviewers should be added in the paras below -->
Few existing studies meet these criteria. The most common paradigms expose one group of listeners to one speech pattern, and a second group of listeners to another speech pattern. Following exposure, both groups are tested on identical stimuli to see how exposure affected perception. Such designs were effective in establishing the *existence* of adaptive speech perception. They do, however, offer only weak tests of distributional learning models [see discussion in @xie2023; @cummings-theodore2023]: it is one thing to show that differences between two exposure conditions lead to differences in behavior; it is another thing to test whether the direction and magnitude of changes in behavior can be consistently explained as a function of listeners' prior experience, and the amount and type of exposure they receive in the experiment.

For example, there is now substantial evidence that long-term prior experience with a particular second language accent over months and years can facilitate more accurate understanding, as well as faster adaptation, to unfamiliar talkers of that accent [@porretta2017; @witteman2013]. Findings like these demonstrate that prior experience affects speech perception, in line with prediction 1 of distributional learning theories. These findings leave open, however, whether the specific benefits of prior experiences are predicted by the *distributions of phonetic cues* that listeners have previously experienced, rather than some other non-distributional learning algorithm. Only a handful of studies have begun to address this question, testing whether differences in the adaptation *outcome* between participant groups can at least qualitatively be accounted for by an estimate of the distributions of phonetic cues in participants' prior experience [@kang-schertz2021; @schertz2016; @tan2021; @xie2021cognition]. Here we build on those studies, assessing how well distributional learning models can explain the quantitative effects of listeners' prior experience both before and during exposure to different phonetic distributions. 

Similar considerations apply to findings that repeated exposure can result in a gradient, cumulative build-up of changes in listeners perception---in line with prediction 2a [e.g., @cummings-theodore2023; @liu-jaeger2018; @poellmann2011; @vroomen2007; @REF-ANY-OTHER-FROM-REVIEWERS]. None of these studies analyzed the phonetic distributions listeners were exposed to, or linked those distributions to the observed incremental changes in listener's behavior. This leaves open whether the observed changes are due to distributional learning rather than alternative mechanism, such as changes in decision biases [see discussion in @xie2023]. Even if we were to take for granted that distributional learning is the most plausible explanation for these results, previous studies leave open whether distributional learning can explain a non-trivial share of the observed incremental changes in listeners' perception---as predicted if distributional learning is an important component of rapid adaptation to unfamiliar talkers. A better understanding of the extent to which changes in listeners' perception quantitatively follow the predictions of distributional learning models also will facilitate the interpretation of otherwise arbitrary seeming findings. For example, some recent findings suggest that adaptation only accumulated up to a certain number of exposure stimuli, after which no further changes were observed [@cummings-theodore2023; @liu-jaeger2018]. Does this 'ceiling' effect reflect convergence against the phonetic distributions of the exposure stimuli, or does it point to constraints on adaptive speech perception that are *not* predicted by distributional learning [see discussion in @kleinschmidt-jaeger2015]? To answer questions like these, it is necessary to compare incremental changes in listeners' perception against quantitative predictions of distributional learning models. This in turn requires paradigms that deliver more fine-grained data, providing sufficiently constraining evidence to evaluate such models.

"Distributional learning" experiments take a step in this direction by explicitly manipulate the phonetic distributions during exposure [e.g., @chladkova2017; @clayards2008; @escudero2011; @goudbeek2008; @idemaru-holt2011; @maye2002; @pisoni1982; @zhang-holt2018]. These experiments have found that the outcome of exposure is qualitatively compatible with prediction 2b. <!-- In some cases, this includes comparisons of more than two exposure conditions, making those results more informative (the probability that outcomes of different exposure conditions qualitatively order in the way predicted by distributional learning decreases with the number of conditions). --> Of particular note are studies that further applied distributional learning models to the exposure inputs of those experiments, and qualitatively or quantitatively compared the predictions of these models to listeners' behavior [@bejjanki2011; @clayards2008; @hitczenko-feldman2016; @harmon2019; @kleinschmidt2015]. <!-- Could be omitted up to next para: --> For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Using a distributional learning model similar to the idealized learners we presented below, Clayards and colleagues predicted that listeners in the wide variance group would exhibit a more shallow categorization function than the narrow variance group. This is precisely what they found, providing support for prediction 2b that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior [see also @nixon2016; @theodore-monto2019]. Findings like these suggests that the outcome of adaptation is compatible with prediction (2b) of distributional learning models.<!-- ^[A related line of work has used distributional learning or explicit training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mccandliss2002; @pajak-levy2012; @pisoni1982]. These studies, too, have observed outcomes predicted by distributional learning models [for review, see @pajak2016].] -->

Studies like those by Clayards and colleagues come particularly close to the standard called for by @xie2023, and they form the inspiration for the present work. Previous studies have, however, used much longer exposure than what we aim for here [an order of magnitude more than the earliest test in our study, @chladkova2017; @clayards2008; @colby2018; @escudero2011; @goudbeek2008; @logan1991]. Next, we describe how we build on these works, including stronger joint tests of predictions 1 and 2a,b, as well as the first test of predictions 3 and 4 for adaptive speech perception. 

\begin{table}[!ht]
\begin{small}
\begin{tabular}{p{0.28\textwidth}p{0.7\textwidth}}
\hline
Prediction & Supported by evidence from \\
\hline
(1) - {\em prior expectations} & (Kang \& Schertz, 2021; Schertz et al., 2016; Tan et al., 2021; Xie et al., 2021) \\

(2a) - {\em exposure amount}  & (Vroomen et al., 2007; Cummings \& Theodore, 2023; Kleinschmidt \& Jaeger, 2011; Liu \& Jaeger, 2018) \\

(2b) - {\em exposure distribution} & (Chl\'adkov\'a et al., 2017; Clayards et al., 2008; Colby et al., 2018; Hitczenko \& Feldman, 2016; Idemaru \& Holt, 2011; Kleinschmidt, 2020; Theodore \& Monto, 2019) \\

(3) - {\em learn to convergence}  & \textsc{Not previously tested for rapid changes in speech perception} \\

(4) - {\em diminishing returns} & \textsc{Not previously tested for rapid changes in speech perception}. \\

\hline
\end{tabular}
\caption{Predictions of distributional learning models about incremental adaptation to an unfamiliar talker. Only prediction 2a has been tested against {\em incremental} changes in listeners' behavior.}
\label{tab:predictions}
\end{small}
\end{table}

<!-- Prediction 1 ({\bf prior expectations}) & Listeners' categorization function prior to informative exposure to an unfamiliar talker's speech is determined by the statistics of the speech input they have previously experienced from other talkers. & AA, \cite{kang-schertz2021, schertz2016, tan2021, xie2021cognition} \\ -->

<!-- Prediction 2a ({\bf exposure amount}) & \multirow{2}{.6\textwidth}{With increasing exposure to the new talker, listeners' adapt their prior expectations by integrating information about the talker's phonetic distributions. The direction and magnitude of changes in listeners' categorization function relative to their pre-exposure behavior are determined by the amount and distribution of phonetic cues relative the statistics of previously experienced speech input}. & LGPL/VGPL, \cite{cummings-theodore2023, kleinschmidt-jaeger2012, liu-jaeger2018, vroomen2007} \\ -->

<!-- Prediction 2b ({\bf exposure distribution}) & & AA, \cite{xie2021cognition, tan2021}; DL, \cite{clayards2008, chladkova2017, colby2018, idemaru-holt2011, kleinschmidt-jaeger2016, theodore-monto2019} \\ -->

<!-- Prediction 3 ({\bf learn to convergence}) &  With additional exposure, listeners will continue to adapt until they have fully learned the exposure distribution. & \\ -->

<!-- Prediction 4 ({\bf diminishing returns}) & With each new observation, changes in listeners' behavior depend on the prediction error (or equivalently, the amount of new information) associated with that observation. As a consequence, listeners' behavior should initially change quickly and then less and less as listeners converge against the exposure distribution. & LGPL \cite{liu-jaeger2018} \\ -->

## The present study
<!-- TO DO: potentially link to (a)-(c) above -->
We combine (i) the incremental exposure-test distributional learning paradigm shown in Figure \@ref(fig:block-design-figure) with (ii) incremental Bayesian mixed-effects psychometric models, and (iii) model-guided interpretation. Between groups of participants, we manipulate the distributions of phonetic cues in the exposure input. Specifically, the exposure distributions are shifted to different degrees both relative to each other, and relative to listeners' prior expectations. Within each participant, we measure changes in the categorization functions at multiple points during exposure. This combination of exposure conditions and incremental testing allows us to obtain substantially more fine-grained, quantitative data about the early effects of distributional exposure than in previous work. The mixed-effects psychometric analysis we present provides a theory-agnostic way to quantify these incremental changes in listeners' categorization function---the mapping from phonetic cues to speech categories. Finally, we use model-guided interpretation through comparison to ideal observer and ideal adaptor models to determine how closely listeners' behavior before, during, and following exposure matches the predictions of distributional learning [@feldman2009; @kleinschmidt-jaeger2015; @massaro1989; @xie2023]. This sheds light on whether distributional learning can plausibly account for a non-trivial share of the rapid changes in speech perception that come with exposure to an unfamiliar talker. As already anticipated above, it is this model-guided interpretation that allows us to identify *unexpected* constraints on distributional learning---constraints that can be shown to *not* follow from state-of-the-art models.^[Such unexpected limitations are more informative for theory development than findings that are 'merely' counter-intuitive. For instance, contrary to intuitions, some failures to find adaptation for some speech stimuli [@floccia2006; @zheng-samuel2020] are compatible with distributional learning [@tan2021], and the same has been argued for some seemingly arbitrary properties of adaptive speech perception [e.g., the 'undoing' of adaptation after prolonged exposure to the exact same stimulus, @vroomen2007; @kleinschmidt-jaeger2012]. The model-guided approach we take here is intended to reduce the reliance on intuition.] Together, (i)-(iii) thus allow us to put the four predictions in Table \@ref(tab:predictions) to a much stronger quantitative test than in previous work. 

<!-- TO DO: name exposure conditions in figure (vertical name in color matching the row, placed at left side of figure?) -->
```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three {\\between}-participant exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

<!-- Our paradigm integrates, and builds on, advances in separate lines of research on unsupervised distributional learning during speech perception [@clayards2008; @colby2018; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [LGPL/VGPL, @cummings-theodore2023; @kleinschmidt-jaeger2012; @vroomen2007], and adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition]. We return to these and related works in the general discussion. For readers unfamiliar with this literature, we briefly make two observations that motivate the paradigm in Figure \@ref(fig:block-design-figure). -->

<!-- First, as indicated in Table \@ref(tab:predictions), previous research has focused on the *outcome* of learning, leaving open whether adaptive speech perception unfolds over time in ways consistent with distributional learning models. For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Using a distributional learning model similar to the idealized learners we presented below, Clayards and colleagues predicted that listeners in the wide variance group would exhibit a more shallow categorization function than the narrow variance group. This is precisely what they found, providing support for prediction 2b that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior [see also @nixon2016; @theodore-monto2019]. Findings like these suggests that the outcome of adaptation is qualitatively compatible with predictions (2a) and (2b) of distributional learning models [see also @hitczenko-feldman2016; @tan2021; @xie2021cognition].^[A related line of work has used distributional learning or explicit training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mccandliss2002; @pajak-levy2012; @pisoni1982]. These studies, too, have observed outcomes predicted by distributional learning models [for review, see @pajak2016].] 

These findings are, however, based on tests that averaged over, and/or followed, hundreds of exposure trials---exposure amounts that exceed what is available during many everyday interactions with unfamiliar talkers. This leaves open then whether the learning mechanism identified in distributional learning studies are sufficiently rapid to have a meaningful impact on those interactions. The strong focus on the outcome of adaptation also means that we do not know how listeners incrementally interpolate between their prior expectations and new phonetic input (the joint effects of predictions 1 and 2a,b). And it explains why predictions (3 - *learn to convergence*) and (4 - *diminishing returns*) have remained untested: tests of these two predictions require a repeated exposure-test paradigm like the one we present here [for discussion, see @cummings-theodore2023; @kleinschmidt2020]. -->

Beyond our primary goals, the present work is motivated by one additional consideration. The need for quantifiable predictions---and the control this requires over the relevant phonetic distributions---can conflict with ecological validity. <!---  (though advances in automatic speech recognition and large language models might ultimately help resolve this tension)--> This has made some researchers question whether the mechanisms that explain adaptation during that are characteristic of everyday speech perception [see discussion in @baeseberk2018]. As detailed under *Methods*, we take several modest steps towards addressing these concerns. This includes concerns about both the stimuli and their distributions in the experiment. 

<!-- Second, there often is a tension between ecological validity and the ability to make strong, quantitative predictions (though recent advances in automatic speech recognition and large language models might ultimately help resolve this tension). For these reasons, it has remained challenging to test distributional learning models against fully natural speech. This makes it difficult to test, on such stimuli, predictions (1) and (2b) about the effects of phonetic distributions listeners experience throughout their lifetime and during the experiment. Even recent tests against exposure to fully natural speech have thus focused on broad qualitative comparisons [e.g., @schertz2016; @xie2017; see also, @schertz-clare2020]. This leaves open whether the direction and magnitude of changes in listeners' behavior can be explained by existing models [but see @hitczenko-feldman2016; @tan2021; @xie2021cognition].  -->

<!-- Tests of distributional learning models have thus largely relied on paradigms that afford researchers with fine-grained control over the distribution of phonetic properties that listeners experience in the experiment [e.g., @chladkova2017; @clayards2008; @colby2018; @idemaru-holt2011; @kleinschmidt2020; @theodore-monto2019]. As we aim to demonstrate below, such control is necessary for stronger tests of existing theories, but it often comes with sacrifices in ecological validity (for now, at least). We follow this approach here. As detailed under *Methods*, we do, however, take several modest steps towards addressing concerns about ecological validity. This includes concerns about both the stimuli and their distribution in the experiment [see discussion in @baeseberk2018].  -->

<!-- Could be omitted up to next para: -->
To anticipate our results, we find that the changes in listeners' categorization behavior *largely* follow the predictions of distributional learning models. In particular, we present the first direct evidence that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (prediction 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find initial---though not decisive---evidence that changes in rate of adaptation throughout exposure are consistent with the predictions of error-driven learning theories and theories of ideal information integration (prediction 4). We show that a Bayesian model of adaptation that is based on principles of ideal information integration [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses with very high accuracy ($R^2 = 96\%$). However, not all observations we make are predicted by existing models, providing new insights into previously unrecognized limits of adaptation. In particular, we do not find support for prediction 3 (*learn to convergence*). Rather, changes in listeners' behavior seem to plateau before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions (cf. the *premature convergence* panel of Figure \@ref(fig:predictions)D). We also find that this constraint on adaptation might be asymmetric, depending on the direction of the shift in the exposure input relative to listeners' prior expectations. We discuss the implications of our findings for theories of adaptive speech perception, and suggest how future variants of our approach can be used to further contrast different models of adaptive speech perception. 

## Open science
All data and code for this article are available on OSF at [https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30](https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30). The OSF repo also contains detailed supplementary information (SI) that we refer to throughout this article. Following @xie2023, both this article and its SI are written in R Markdown. This allows other researchers to replicate and revise our analyses with the press of a button using freely available software [@R-base; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an undergraduate class at the University of Rochester (BCS206/207). The experiment was originally designed to address predictions 1-3. Our analyses of prediction (4 - *diminishing returns*) are thus post-hoc, as are some of the analyses we present to understand the evidence against prediction 3 (*learn to convergence*). All post-hoc analyses are indicated as such. Finally, the ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work ### **ommitted for review** ###<!-- [@kleinschmidt-jaeger2015; @tan2021; @xie2023]-->. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.

```{r}
rm(
  d.talker_IO.VOT,
  p.cat,
  p.PSE,
  p.PSE_change,
  p.gaussian)
```

