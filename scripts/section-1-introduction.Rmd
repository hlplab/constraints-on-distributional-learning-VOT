```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Successful speech recognition requires that listeners map the acoustic signal onto words and meanings. But this signal-to-meaning mapping varies across contexts and talkers. The same word spoken by different talkers can sound quite different; and conversely, the same acoustic signal can imply different words depending on the talker. Yet, healthy young adult listeners typically recognize speech quickly and accurately across a wide range of talkers and acoustic conditions (after decades of advances, automatic speech recognition is now approaching the accuracy that most listeners display effortlessly during everyday speech perception).

Research has identified *adaptivity* as a key component to the robustness of human speech perception. Although first encounters with an unfamiliar accent can cause initial processing difficulty, this difficulty can diminish rapidly with exposure [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. Twenty short sentences from an unfamiliar second language accent can significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Under ideal conditions, deviations from expected pronunciations along familiar phonetic dimensions can be detected after even shorter exposure [e.g., @cummings-theodore2023; @liu-jaeger2018], sometimes after a single trial [@vroomen2007]. Findings like these suggest that speech perception is highly adaptive, allowing listeners to quickly adjust the mapping from acoustics to phonetic categories and word meanings. While such rapid adaptation is now well-documented, its discovery challenged long-held assumptions, and spurred the development of new paradigms and theories [reviewed in @bent-baeseberk2021; @schertz-clare2020]. Of course, exposure had long been known to affect speech perception---after all, we can learn new languages with enough exposure. However, rapid adaptation to unfamiliar speech patterns unfolds over much shorter times scales (seconds and minutes).

What remains unclear is *how* rapid adaptation is achieved: how do listeners integrate information from a new talker, and how does this come to incrementally change listeners' interpretation of that talker's speech? This is the question we seek to contribute to here. To appreciate our approach to this question, it is helpful to reflect on the state of the field. Similar to many other fields in the cognitive sciences, research on adaptive speech perception tends to discuss informal---often descriptive, rather than explanatory---hypotheses [see discussion in @norris-cutler2021; @xie2023]. This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Such descriptions do not specify what mechanisms support adaptive speech perception, nor do they make predictions about how adaptation unfolds incrementally with each new observation from an unfamiliar talker. 

Viewed from the perspective of such informal hypotheses, research on adaptive speech perception can be an open-ended list of questions. Is adaptation more or less immediate, or does it unfold gradually? If the latter, do changes in listeners' behavior accumulate additively, leading to more or less linear changes in behavior? And, how does listeners' prior experience affect how listeners adapt? Are there limits to listeners' ability to fully adapt to a new talker? Or can we adapt to more or less any accent provided sufficient input? Under a question- rather than theory-driven approach, each such question can be---and often is---viewed in isolation. This makes it difficult develop and test theoretical models that are sufficiently constrained to detect *informative incompatibility* with the data. As Allen Newell famously put it more than 50 years ago "you can't play 20 questions with nature and win" [@newell1973, p. 1].

Critically, there *are* integrative theories that make clear, quantifiable predictions about all of the questions in the preceding paragraph, including basic predictions that remain untested. One of the most developed of these theories is the hypothesis that adaptive speech perception draws on *distributional learning* [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assmann2007; @sohoglu-davis2016]. While distributional learning models differ from each other in important aspects, they share the central assumption that listeners incrementally learn and store information about the phonetic distributions that characterize the talker's speech. This includes information like the average values of phonetic cues, their variability, or even the full phonetic distributions of all speech categories. These statistical properties are then used to interpret subsequent speech from the talker, supporting robust speech recognition across talkers [reviewed in @schertz-clare2020; @xie2023]. 

With these assumptions of distributional learning theories come shared predictions, which we introduce next. The present study is the first to test some of these predictions, and the first to systematically test all of the predictions together in a single experiment. Going beyond qualitative tests, we also assess whether a distributional learning model [the ideal adaptor, @kleinschmidt-jaeger2015] can explain (1) a non-trivial share of the rapid changes in listeners' perception that come with exposure, as well as (2) potential limits of such adaptation. As we describe below, this requires a paradigm that elicits rich, model-constraining, data, as well as quantitative model-guided analyses that take advantage of such data [see also discussion in @coretta2023; @yarkoni-westfall2017]. 

## Predictions of distributional learning
Consider a listener's initial encounter with an unfamiliar talker who produces some familiar sound categories, e.g. /d/ and /t/ in US English, in an unfamiliar way (dashed phonetic distributions in Figure \@ref(fig:predictions)A). Figure \@ref(fig:predictions)B-C show how the *ideal* categorization functions for this unfamiliar talker would differ from those for a 'typical' talker of US English. Distributional learning theories predict that listeners' perception should change incrementally with exposure to the talker's speech. Specifically, the direction and magnitude of that change should gradiently depend on listeners' prior expectations based on relevant previously experienced speech input (**prediction 1 - *prior expectations***), and both the amount (**prediction 2a - *exposure amount***) and distribution of phonetic cues in the exposure input from the unfamiliar talker (**prediction 2b - *exposure distribution***). Listeners' categorization functions---the mapping from acoustics to phonetic categories and words---should gradually shift from a starting point that reflects the phonetic distributions of previously experienced speech towards a target that reflects the phonetic distributions of the new talker's speech. Some distributional learning models further predict that adaptation initially proceeds quickly and then slows down as the listener's expectations come to more closely approximate the phonetic distributions of the unfamiliar talker [**prediction 3 - *diminishing returns***, e.g., @harmon2019; @kleinschmidt-jaeger2015; @olejarczuk2018; @sohoglu-davis2016]. Finally, standard distributional learning models further make the critical prediction that this shift proceeds until the listener has fully learned the statistics of the new talker's speech (**prediction 4 - *learn to convergence***).^[Predictions 1-4 assume that listeners *know* that they are listening to the same new talker throughout exposure. Talker recognition is itself an inference process that we do not further discuss here [but see @kleinschmidt-jaeger2015; @magnuson-nusbaum2007].] 

Figure \@ref(fig:predictions)D illustrates these predictions. All panels show predictions 1 and 2a,b: the point of subjective equality (PSE) of listeners' categorization function initially is based on listeners' expectation for a 'typical' talker of US English (bottom gray line, prediction 1). With exposure, listeners acquire information about the unfamiliar talker's speech, and thus shift their categorization function towards the PSE that would be ideal for that talker's phonetic distributions (prediction 2b). With more exposure, this shift increases (prediction 2a). The four panels of Figure \@ref(fig:predictions)D  differ, however, in whether show predictions 3 and 4 (shaded background) or alternative scenarios, such as immediate talker switching; linear, rather than sublinear, changes; or convergence against partial adaptation. These scenarios do not constitute an exhaustive list. Rather, they are meant to illustrate that there are many ways in which rapid adaptation during speech perception might unfold, most of which have not been ruled by previous work. We return to this shortly.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally, using the pronunciation of US English word-initial /d/ and /t/ as an example (as in "dip" vs. "tip"). **A)** Thin gray lines indicate cross-talker variability in the realization of /d/ and /t/ along the primary phonetic cue that distinguishes them in US English (voice onset timing or VOT). Shown are 20 random talkers from a database of connected speech [@chodroff-wilson2018]. Thicker gray lines indicate the VOT distributions expected for a 'typical' talker (averaging over all talkers in the database). Dashed lines indicate a hypothetical unfamiliar talker with a noticeably different VOT distribution. **B)** Ideal categorization functions (described under *Methods*) along the VOT continuum for speech from a typical talker (solid gray) and speech from the unfamiliar talker (dashed blue). Arrows point to the point of subjective equality (PSE), the VOT that listeners are equally likely to identify as /d/ or /t/ (aka "category boundary"). **C)** Same as B) but just showing the PSE, now on y-axis, which is how we plot changes in PSE with increasing exposure in the next panel. **D)** Different ways in which listeners' PSEs might incrementally change with increasing exposure to the unfamiliar talker (from more transparent to less transparent). Horizontal lines indicates the ideal PSEs from C). 

```{r predictions, fig.height=base.height*3+2/3, fig.width=base.width*5, fig.cap="(ref:predictions)", fig.pos="H"}
# Create all talker-specific IOs for connected speech data, using only the VOT cue
d.talker_IO.VOT <- 
  make_IOs_from_data (
    data = d.chodroff_wilson.connected,
    cues = c("VOT"),
    groups = "Talker") 

# Add x, PSE, categorization, and get gaussian geoms
d.talker_IO.VOT %<>%
  nest(io = -Talker) %>%
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  add_gaussians_as_geoms_to_io(alpha = .2, linetype = 1, linewidth = .5) %>%
  bind_rows(
    # Do the same after aggregating all talker-specific IOs into a single 'typical' talker IO
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(Talker = "typical") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = 1, linewidth = 1.2),
    # Do the same after aggregating all talker-specific IOs into a single talker that is shifted by 35ms relative to the 'typical' talker
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(
        mu = map(mu, ~ .x + 35),
        Talker = "unfamiliar") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = "twodash", linewidth = 1.2))

p.gaussian <- 
  ggplot() +
  (d.talker_IO.VOT %>% 
     filter(
       !(Talker %in% c("typical", "unfamiliar")),
       Talker %in% sample(unique(Talker), 20)) %>% 
     pull(gaussian)) + 
  (d.talker_IO.VOT %>% 
     filter(Talker == "typical") %>% .$gaussian) +
  scale_colour_manual(values = c(colours.category_greyscale)) +
  guides(colour = "none") +
  new_scale_colour() +
  (d.talker_IO.VOT %>% 
     filter(Talker == "unfamiliar") %>% .$gaussian) +
  scale_colour_manual("Category", values = c("#02427e", "#b4dafe")) +
  guides(color = guide_legend(override.aes = list(size = 0.5, colour = c(colours.category_greyscale), values = c("/d/", "/t/")))) +
  labs(x = "VOT (ms)", y = "Density") +
  theme(
    legend.background = element_rect(fill='white'),
    legend.box.background = element_rect(fill='transparent'),
    legend.key = element_rect(fill = "transparent"),
    legend.key.size = unit(0.5, "cm"),
    legend.key.spacing = unit(0.05, "cm"),
    legend.key.height = unit(0.01, "cm"),
    legend.title = element_text(size = 7), 
    legend.text = element_text(size = 6),
    legend.position = "inside",
    legend.position.inside = c(0.79,0.8))

p.cat <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>% 
  select(Talker, category, categorization, PSE) %>% 
  filter(category == "/t/") %>% 
  unnest(categorization, names_repair = "unique") %>% 
  ggplot(aes(x = VOT, y = response, group = Talker, colour = Talker, linetype = Talker)) +
  geom_line(linewidth = 1.2) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  scale_linetype_manual(values = c("solid", "twodash")) +
  #scale_x_continuous("VOT (ms)", limits = c(-25, 130), breaks = c(0, 37, 72, 100)) +
  scale_y_continuous('Proportion "t"-responses', breaks = c(0, .5, 1)) +
  guides(linetype = "none", colour = "none") +
   geom_segment(
     data = 
       d.talker_IO.VOT %>% 
       filter(Talker %in% c("typical", "unfamiliar")) %>%
       mutate(x = PSE, xend = PSE, y = .5, yend = .01),
    mapping = aes(x = x, xend = xend, y = y , yend = yend, color = Talker),
    alpha = 0.5,
    arrow = arrow(type = "open" , length = unit(0.04, "npc")),
    inherit.aes = F) +
  scale_x_continuous(limits = c(15, 90), breaks = c(37, 72)) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

p.PSE <- 
d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar"), category == "/t/") %>%
  select(Talker, category, PSE) %>% 
  mutate(PSE = round(PSE)) %>% 
  ggplot() +
  geom_hline(data = . %>% filter(Talker == "typical"), aes(yintercept = PSE), 
             color = "grey", linewidth = 1.5, linetype = 1) +
  geom_hline(data = . %>% filter(Talker == "unfamiliar"), aes(yintercept = PSE), 
             color = "#02427e", linewidth = 1.5, linetype = "twodash") +
  scale_y_continuous("Ideal PSE (ms VOT)", limits = c(30, 75), breaks = c(37, 72)) +
  scale_x_continuous("", limits = c(0, .5), breaks = NULL) +
   annotate(
       geom = "text",
       x = 0.1, 
       y = c(41, 69),
       justify = 1,
       label = c("typical", "unfamiliar"),
       color = c("grey", "#02427e"),
       fontface = "bold",
       size = 2.5) +
  theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
 
PSE_change <- 
  expand_grid(
    PSE = c(),
    learning_pattern = factor(c("immediate", "linear", "diminished", "premature")),
    exposure_amount = factor(c("none", "1/3", "2/3", "complete"))) %>%
  mutate(
    learning_pattern = fct_relevel(learning_pattern, "immediate", "linear", "diminished", "premature"),
    exposure_amount = fct_relevel(exposure_amount, "none", "1/3", "2/3", "complete"),
    PSE = case_when(
      exposure_amount == "none" ~ 37,
      exposure_amount == "complete" & learning_pattern %in% c("immediate", "linear") ~ 72,
      exposure_amount %in% c("1/3", "2/3") & learning_pattern == "immediate" ~ 72,
      exposure_amount == "1/3" & learning_pattern == "linear" ~ 37 + ((72 - 37 + 1)/3),
      exposure_amount == "2/3" & learning_pattern == "linear" ~ 37 + 2 * ((72 - 37 + 1)/3),
      exposure_amount == "1/3" & learning_pattern == "diminished" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount == "2/3" & learning_pattern == "diminished" ~ 37 + 1.6 * ((72 - 37 + 1)/2),
      exposure_amount == "complete" & learning_pattern == "diminished" ~ 37 + 1.85 * ((72 - 37 + 1)/2) ,
      exposure_amount == "1/3" & learning_pattern == "premature" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount %in% c("2/3", "complete") & learning_pattern == "premature" ~ 37 + 25))

p.PSE_change <- 
  PSE_change %>% 
  ggplot(aes(exposure_amount, PSE, alpha = exposure_amount, group = 1), colour = "#02427e") +
  geom_rect(
    data = PSE_change  %>% 
      nest(data = c(exposure_amount, PSE)) %>% 
      group_by(learning_pattern) %>% 
      slice_sample(n = 1) %>% 
      select(learning_pattern) %>% 
      mutate(ymin = ifelse(learning_pattern == "diminished", -Inf, NA),
             ymax = ifelse(learning_pattern == "diminished", Inf, NA)),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "yellow",
    alpha = .15,
    inherit.aes = F) +
  geom_point(size = 1.8, colour = "#02427e") +
  geom_line(alpha = .3) +
  geom_hline(aes(yintercept = 37), linetype = 1, alpha = .8, colour = "grey") +
  geom_hline(aes(yintercept = 72), linetype = 2, alpha = .6, colour = "#02427e") +
  scale_x_discrete("Exposure amount", breaks = c("none", "complete"), labels = c("none", "full")) +
  scale_y_continuous("PSE (ms VOT)", limits = c(25, 75), breaks = c(30, 40, 50, 60, 70)) +
  guides(alpha = "none") +
  facet_wrap(~learning_pattern, nrow = 1, labeller = labeller(learning_pattern = c("immediate" = "immediate switch", "linear" = "linear", "diminished" = "convergence w/\ndiminishing returns", "premature" = "premature convergence\nw/ diminishing returns"))) +
  theme(strip.text = element_text(size = 9))


((p.gaussian | p.cat | p.PSE) / p.PSE_change) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

## Contributions: What is (not) known about distributional learning as mechanism for rapid adaptive speech perception?
The present study makes three novel contributions to our understanding of adaptive speech perception. Our first contribution is the most straightforward. We present the **first tests of predictions 3 and 4 for rapid adaptive speech perception**. Diminishing returns (prediction 3)---a.k.a. the power law of learning [@newell-rosenbloom1981]---are predicted by many theories of learning [e.g., associative learning, @rescorla-wagner1972] and have been demonstrated across a wide range of learning phenomena [e.g., @anderson1990; @logan1988; @palmeri1997]. It is, however, unknown whether rapid changes in speech perception actually reflect learning in the more narrow sense at all [see discussion in @xie2018]. For instance, in neuroimaging studies on adaptive speech perception, it is common to attribute rapid adaptation to changes in decision-making, rather than changes in the distributional mapping from phonetics to speech categories [e.g., @blanco-elorrieta2021; @myers-mesite2014; reviewed in @xie2023]. 

Even less is known about prediction 4 (*learn to convergence*): despite its importance to distributional learning theories, it is not yet known whether rapid adaptation actually converges against the phonetic distributions in the input. For instance, as shown in Figure \@ref(fig:predictions)D, listeners might simply *immediately switch* to entirely different mechanisms for unfamiliar speech, compared to familiar speech [e.g. "criterion relaxation", @zheng-samuel2020]. Alternatively, listeners might gradually adapt towards, but never fully arrive at, the ideal categorization function that would be expected from a listener that has fully learned the exposure distributions. Such *premature convergence* could result from some form of satisficing behavior, perhaps because partial adaptation is 'good enough' in achieving highly accurate speech recognition for the unfamiliar talker. Or it could reflect strong constraints on the early moments of adaptive speech perception. Such constraints are expected, for example, if the early moments of adaptation are limited to the reweighting of previously learned dialect or sociolect representations, rather than the acquisition of new representations for the unfamiliar talkers' speech [see discussion in @kleinschmidt-jaeger2015; @wade2022; @xie2018]. 

\begin{table}[!ht]
\begin{small}
\begin{tabular}{p{0.25\textwidth}p{0.75\textwidth}}
\hline
Prediction & Supported by evidence from \\
\hline
(1) - {\em prior expectations} &  (e.g., Kang \& Schertz, 2021; Schertz et al., 2016; Tan et al., 2021; Xie et al., 2021) \\

(2a) - {\em exposure amount}  &  (e.g, Vroomen et al., 2007; Cummings \& Theodore, 2023; Kleinschmidt \& Jaeger, 2011; Liu \& Jaeger, 2018) \\

(2b) - {\em exposure distribution} &  (e.g., Chl\'adkov\'a et al., 2017; Clayards et al., 2008; Colby et al., 2018; Hitczenko \& Feldman, 2016; Idemaru \& Holt, 2011; Theodore \& Monto, 2019) \\

(3) - {\em diminishing returns} & \textsc{Not previously tested for rapid changes in speech perception}. \\

(4) - {\em learn to convergence}  & \textsc{Not previously tested for rapid changes in speech perception} \\

\hline
\end{tabular}
\caption{Predictions of distributional learning theories about incremental adaptation. Existing evidence is compatible with predictions 1, 2a,b, though it does not necessarily provide strong support for those predictions (see text). Prediction 3 and 4 remain untested for adaptive speech perception.}
\label{tab:predictions}
\end{small}
\end{table}

<!-- Prediction 1 ({\bf prior expectations}) & Listeners' categorization function prior to informative exposure to an unfamiliar talker's speech is determined by the statistics of the speech input they have previously experienced from other talkers. & AA, \cite{kang-schertz2021, schertz2016, tan2021, xie2021cognition} \\ -->

<!-- Prediction 2a ({\bf exposure amount}) & \multirow{2}{.6\textwidth}{With increasing exposure to the new talker, listeners' adapt their prior expectations by integrating information about the talker's phonetic distributions. The direction and magnitude of changes in listeners' categorization function relative to their pre-exposure behavior are determined by the amount and distribution of phonetic cues relative the statistics of previously experienced speech input}. & LGPL/VGPL, \cite{cummings-theodore2023, kleinschmidt-jaeger2012, liu-jaeger2018, vroomen2007} \\ -->

<!-- Prediction 2b ({\bf exposure distribution}) & & AA, \cite{xie2021cognition, tan2021}; DL, \cite{clayards2008, chladkova2017, colby2018, idemaru-holt2011, kleinschmidt-jaeger2016, theodore-monto2019} \\ -->

<!-- Prediction 3 ({\bf diminishing returns}) & With each new observation, changes in listeners' behavior depend on the prediction error (or equivalently, the amount of new information) associated with that observation. As a consequence, listeners' behavior should initially change quickly and then less and less as listeners converge against the exposure distribution. & LGPL \cite{liu-jaeger2018} \\ 

<!-- Prediction 4 ({\bf learn to convergence}) &  With additional exposure, listeners will continue to adapt until they have fully learned the exposure distribution. & \\ -->

Our second contribution is that we **test all four predictions together in a single experiment, borrowing from different existing paradigms**. Previous work has to some extent tested predictions 1 and 2a,b, as summarized in Table \@ref(tab:predictions). However, the different predictions have typically been pursued in separate lines of research, using very different paradigms. This has raised questions as to whether the different paradigms tap into the same mechanisms [e.g., @zheng-samuel2020; @xie2023], and the extent to which results of different paradigms are likely to generalize to everyday speech perception [e.g., @baeseberk2018]. For instance, prediction 1 has typically been tested by contrasting different groups of listeners in their ability to correctly understand natural accents, depending on the amount of long-term exposure listeners had with the accent [e.g., @porretta2017; @witteman2013]. This line of work usually investigates exposure to natural accents, typically using sentence recordings or even longer narrative recordings, with all their phonetic and other linguistic complexities. In contrast, research on prediction 2a (*exposure amount*) primarily comes from paradigms that use manipulated---rather than natural---speech, typically isolated non-word or word recordings [e.g., lexically-guided perceptual learning, @cummings-theodore2023; @liu-jaeger2018; @poellmann2011], sometimes repeating the exact same stimulus for many dozens or hundreds of times [e.g., visually-guided perceptual learning, @vroomen2007]. These studies have found that repeated exposure results in a gradient, cumulative changes in listeners perception---in line with prediction 2a. 

Similarly, prediction 2b has almost exclusive been tested in so-called distributional learning paradigms [e.g. @clayards2008; @escudero2011; @goudbeek2008; @idemaru-holt2011; @logan1991; @maye2002; @pisoni1982; @zhang-holt2018]. These paradigms, too, tend to repeat a small set of stimuli many times. Unlike visually- or lexically-guided perceptual learning paradigms, however, distributional learning experiments explicitly manipulate the distribution of phonetic cues. The paradigms also differ in whether the input is labeled or not---i.e., whether listeners can easily infer which sound categories the talker intended to produce. Whereas labeling information is typically absent in distributional learning experiments, [but see e.g., @chladkova2017; @goudbeek2008; @kleinschmidt2015], exposure stimuli in perceptual learning experiments are *always* labeled. As pointed out by an anonymous reviewer, it is thus possible that the two experiments tap into different mechanisms, specifically unsupervised vs. supervised learning. Everyday speech perception, however, plausibly employs both supervised and supervised learning: in naturally-accented, connected speech, lexical and other linguistic context will provide effectively label some, but not all, sound segments [@gibson2013].

In short, existing evidence for predictions 1 and 2a,b comes from different paradigms, each trading off experimental control and ecological validity in different ways. Here, we use a revised incremental distributional learning paradigm to test all four predictions together. Unlike previous distributional learning experiments, we assess the effects of exposure incrementally from pre-exposure onward, including tests after *much* shorter exposure than in previous experiments of this type [an order of magnitude fewer trials, @chladkova2017; @clayards2008; @colby2018; @escudero2011; @goudbeek2008; @goudbeek2009; @logan1991]. As detailed under Methods, we also take several modest steps to improve the ecological validity of the distributional learning paradigm, without loss of control over the relevant phonetic distributions [see discussion in @baeseberk2018; @zheng-samuel2020]. This includes the use of stimuli that sound natural and exhibit natural correlations between phonetic cues; the use of *sampled*, rather than designed (unnaturally symmetric), exposure distributions; and a mix of both labeled and unlabeled inputs, to reflect that linguistic context often but not always labels in the input during everyday interactions with unfamiliar talkers. 

Our third and final contribution is the one we are most excited about. As highlighted in recent reviews [e.g., @bent-baeseberk2021; @xie2023], previous work has largely left open whether the phonetic distributions---i.e., the theoretical construct that distributional learning theories predict to be the primary driver of adaptation---can actually quantitatively predict listeners' behavior. It is thus unknown whether distributional learning is the primary mechanism underlying rapid adaptation during speech perception. If it is, **distributional learning should explain a substantial share of the changes in listeners' perception.** Here, we aim to address this question. 

Based on computational simulations, @xie2023 argue that such a strong, quantitative tests of distributional learning theories requires (a) information about the distribution of phonetic cues in both listeners' prior experience and during exposure, (b) paradigms that measure incremental changes in listeners' behavior both within and across exposure conditions, and, critically, (c) analyses that quantitatively link the latter to the former. Few existing studies meet these criteria. For example, only a handful of studies have begun to ask whether the effects of prior experience (prediction 1) can at least *qualitatively* be predicted from the phonetic distributions listeners are likely to have experienced previously [@kang-schertz2021; @schertz2016; @tan2021; @xie2021cognition]. This goes beyond demonstrations that, for instance, long-term familiarity with an accent leads to more accurate recognition of that accent (see above). But it leaves open *how much* of listeners' perception, and their ability to adapt to an unfamiliar talker, is explained by the phonetic distributions in their previously experienced input. Similar considerations apply to recent qualitative tests of prediction 2a about the amount of exposure [e.g., @cummings-theodore2023; @liu-jaeger2018; @poellmann2011]. These studies leave open whether distributional learning can account for a non-trivial share of the cumulative build-up in listeners' adaptation. 
<!-- Qualitative tests also are limited in what they can tell us about potential constraints on adaptation. For instance, some recent experiments on lexically-guided perceptual learning suggest that adaptation only accumulates up to a certain number of exposure stimuli, after which no further changes were observed [@cummings-theodore2023; @liu-jaeger2018]. Does this 'ceiling' effect reflect the specific phonetic properties of the stimuli used in the experiment, or does it point to constraints on adaptive speech perception that are *not* predicted by distributional learning? Existing approaches are not suited to address this question. -->

The present study is inspired by a very small number of studies---reviewed in more detail later---that has begun to test distributional learning models quantitatively. This includes findings that a distributional learning model, the ideal adaptor, can account for a large share of the incremental effects of repeated exposure to the exact same visually-labeled non-word stimulus [@kleinschmidt-jaeger2015]. This provides strong quantitative support for prediction 2a, at least when listeners receive maximally informative, non-varying input many times in a row. Most notably though, the present study is motivated by distributional learning experiments that have tested prediction 2b by directly manipulating the phonetic distributions listeners are exposed to [e.g., @bejjanki2011; @clayards2008; @harmon2019; @hitczenko-feldman2016; @kleinschmidt2015]. For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Clayards and colleagues first demonstrated that an idealized learner---an ideal observer that has perfect knowledge of the relevant exposure distributions---would exhibit a more shallow categorization function in the wide, compared to the narrow, variance exposure. Clayards and colleagues then showed that this is precisely what human listeners do, too [see also @nixon2016; @theodore-monto2019]. More recent work has compared how well distributional learning can account for the effects of different exposure distributions (prediction 2b) after prolonged exposure of over 200 trials [e.g., @kleinschmidt-jaeger2016; @kleinschmidt2020]. These studies found the same distributional learning model previously applied to visually-labeled non-word stimulus also explains a substantial share of the changes in listeners' perception in these distributional learning experiments ($R^2 > 81\%$). 

Studies summarized in the preceding paragraph come particularly close to the standard envisioned by @xie2023. Here, we build on these efforts, going beyond predictions 2a,b. By assessing the quantitative support for predictions 1-4 simultaneously, we put distributional learning to an unprecedented, quantitative 'stress test'. For example, while the quantitative fit of distributional learning models reported in previous work for predictions 2a,b is impressive, none of these studies required that the model *also* had to explain listeners' perception before the experiment solely based on an estimate of the distributions they have encountered previously. Simply put, the more we ask the model to explain *at the same time* with the same set of parameters, the more we might detect previously unrecognized limits in the explanatory power of distributional learning. Or, in the words of @newell1973, there is merit in "forcing enough detail and scope to tighten the inferential web that ties our experimental studies together" [p. 24, see also @coretta2023; @yarkoni-westfall2017]. 

To anticipate our results, the effort invested into tightening the 'inferential web' can pay off. We find that the changes in listeners' categorization behavior we observe *largely* follow the predictions of distributional learning models. In particular, we find that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (prediction 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find initial---though not decisive---evidence that changes in rate of adaptation throughout exposure are consistent with power law of learning (prediction 3 - *diminishing returns*). Perhaps most strikingly, we find that a variant of the ideal adaptor model that is constrained to match the' prior expectations of a 'typical listener' predicts participants' changing perception across a large number of exposure-test combinations with just two degrees of freedom ($R^2 = 96\%$). However, our model-guided data interpretation also reveals a previously unrecognized constraint on rapid adaptation. In particular, we do *not* find support for prediction 4 (*learn to convergence*). Rather, changes in listeners' behavior seem to plateau before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions (cf. the *premature convergence* panel of Figure \@ref(fig:predictions)D). We discuss the implications for theories of adaptive speech perception. 

<!-- ^[Such unexpected limitations are more informative for theory development than findings that are 'merely' counter-intuitive. For instance, contrary to intuitions, some failures to find adaptation for some speech stimuli [@floccia2006; @zheng-samuel2020] are compatible with distributional learning [@tan2021], and the same has been argued for some seemingly arbitrary properties of adaptive speech perception [e.g., the 'undoing' of adaptation after prolonged exposure to the exact same stimulus, @vroomen2007; @kleinschmidt-jaeger2012]. The model-guided approach we take here is intended to reduce the reliance on intuition.]  -->

```{r}
rm(
  d.talker_IO.VOT,
  p.cat,
  p.PSE,
  p.PSE_change,
  p.gaussian)
```

