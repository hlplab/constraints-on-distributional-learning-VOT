```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Human speech perception is a remarkable feat. Listeners can typically understand speech across a wide range of acoustic conditions and talkers. This ability is particularly impressive given the variability inherent in acoustic signal. The same word spoken by different talkers can sound quite different. Conversely, the same acoustic signal can imply different words depending on the talker. Despite this variability, listeners tend to recognize speech quickly and accurately. Automatic speech recognition systems trained on more words than people will experience in a lifetime are just beginning to approach the robustness that listeners effortlessly display in everyday speech understanding.

Research on speech perception has identified *adaptivity* as a key component to this ability. For example, first encounters with an unfamiliar accent can cause initial processing difficulty. However, this difficulty tends to diminish with exposure, often quite rapidly [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. Eighteen short sentences from a talker with an unfamiliar accent---even a comparatively strong second language accent---have been found to significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Findings like these suggests that speech perception can be highly adaptive, allowing listeners to flexibly adjust how they map the acoustic signal onto phonetic categories and word meanings. While such adaptivity may seem obvious in hindsight, its discovery was a major breakthrough in the field of speech perception, spurring the development of new paradigms and theories [for reviews, see @bent-baeseberk2021; @cummings-theodore2023; @schertz-clare2020; @zheng-samuel2023]. 

Despite this progress, the mechanisms underlying adaptive speech perception remain largely unknown. We know *that* listeners adapt to unfamiliar talkers, but we do not know *how* they do so. Even some of the most basic properties of adaptive speech perception remain unknown. We do not know how speech perception changes incrementally during the first few moments of listening to an unfamiliar talker. Does adaptation unfold gradually, or is it more or less spontaneous? Is adaptation guided by prior expectations based on listeners' lifelong experiences, and if so, how are these expectations integrated with the unfamiliar speech? Are there limits to listeners' ability to fully adapt to a new talker? While we know that exposure improves perception, we do not know how far this adaptation goes: do listeners really continue to adapt until they have fully learned the new accent?

Contemporary theories of speech perception make precise, quantifiable predictions about these questions, but these predictions remain largely untested [for review, see @xie2023]. Perhaps the most developed of these theories are known as *distributional learning* models [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @assmann-nearey2007; @lancia-winter2013; @sohoglu-davis2016]. While these models differ in important aspects, they share the central assumption that listeners incrementally learn and store information about the talker's speech. Listeners are assumed to learn the statistics of the talker's speech, such as the average values of phonetic cues, their variability, or even the full phonetic distributions of each speech category. These statistical properties are then used to interpret subsequent speech from the talker [for reviews, see @bent-baeseberk2021; @schertz-clare2020; @xie2023]. <!-- For example, some of the most parsimonious distributional learning accounts---normalization accounts---assume that listeners adapt their expectations for the phonetic cues the talker produces. Talkers with faster speech rates, for example, might have overall shorter durational cues. Normalization accounts propose that listeners learn, say, the average value that a talker produces for a cue, and interpret subsequent speech from the talker relative to this expectation [@assmann-nearey2007; @mcmurray-jongman2011]. Less parsimonious distributional learning accounts propose that listeners even learn the cue distributions that the talker produces for each category [@johnson1997; @kleinschmidt-jaeger2015]. --> 

As a consequence of this shared assumption, distributional learning models also share several critical predictions. Listeners' categorizations are predicted to change incrementally with exposure. The direction and magnitude of that change should gradiently depend on (1) listeners' prior expectations based on relevant previously experienced speech input from other talkers, and both (2a) the amount and (2b) distribution of phonetic cues in the exposure input from the unfamiliar talker [for review, see @xie2023]. In particular, listeners' categorization functions---the mapping from phonetic cues to categories---should gradually shift from a starting point that reflects the statistics of previously experienced speech towards a target that reflects the statistics of the new talker's speech. This incremental adaptation is predicted to (3) proceed until the listener has fully learned the statistics of the new talker's speech. Finally, some distributional learning models further commit to specific learning mechanisms that constrain how exactly adaptation is expected to accumulate incrementally: both error-driven theories [@harmon2019; @olejarczuk2018; @sohoglu-davis2016] and theories of ideal information integration [@kleinschmidt-jaeger2015; @kleinschmidt2020] predict that (4) adaptation initially proceeds quickly and then slows down as the listener approaches the correct mapping from the acoustic signal to phonetic categories. Figure \@ref(fig:predictions) illustrates these predictions and contrasts them with other possible scenarios.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally. The top row shows how listeners' categorization functions along a phonetic continuum change with increasing exposure, from the dashed starting point to the solid black target expected from an idealized learner that has fully learned the relevant statistics. The bottom row shows the same scenarios but as changes in the point of subjective equality (PSE), the point along the phonetic continuum at which listeners are equally likely to identify a sound as an instance of category "A" or "B". The highlighted column illustrates the predictions of models that assume error-based learning or ideal information integration. The rightmost column shows a scenario in which learners adapt but do not reach the correct mapping from the acoustic signal to phonetic categories. <!-- TO DO: linear to convergence, immediate convergence, gradiently diminishing returns to convergence, plateau prior to convergence. gray categorization functions, solid black learning target, dashes light gray prior; x-axis "phonetic continuum" from 0 to 85 (top) "Trial" (bottom), y-axis "Probability of responding category A" from 0 to 1 (top) "PSE" (bottom) -->

The specificity of predictions (1)-(4) contrasts with more informal hypotheses, which remain common in the field (including in our own work). This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Yet, even basic predictions (1)-(4) of distributional learning models remain untested.^[This tendency is hardly specific to speech perception research. Seminal works from the early days of the cognitive sciences describe the field as often phenomenon-driven rather than theory-driven, and insightfully discussed the consequences of this tendency [@newell1974; @platt1964]. Recent reviews have found this trend continued [@yarkoni-westfall2017].] This is in part due to the types of paradigms typically used in research on adaptive speech perception. The most common paradigms expose one group of listeners to one speech pattern, and a second group of listeners to another speech pattern. Following exposure, both groups are tested on their ability to recognize one of the two speech patterns. Such designs are consistent with the goal of first-generation studies to establish the *existence* of adaptive speech perception. They do, however, offer only weak tests of existing theories [for demonstration, see @xie2023]. In addition, the observed changes in listeners' perception are rarely compared to the quantitative predictions of existing theories. For example, there is now increasing evidence that changes in listeners' categorization functions depend *in some way* on the specific acoustic and phonetic properties of the speech input [e.g., @bertelson2003; @chladkova2017; @eisner-mcqueen2005; @kraljic-samuel2005; @kurumada2013; @norris2003; @reinisch-holt2014]. However, the extent to which these effects are consistent with the predictions of distributional learning models remains largely unexplored (we discuss notable exceptions below). Put simply, it is one thing to find that differences in inputs lead to differences in behavior; it is another thing to test whether the direction and magnitude of changes in behavior can be consistently explained by existing theories. Recent reviews have thus called for the development of paradigms that can more strongly constrain theories of adaptive speech perception [@bent-baeseberk2021; @schertz-clare2020; @xie2023]. 

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

The present study is intended as a response to this call. We present a novel incremental exposure-test paradigm, and use it to test predictions (1)-(4). As we discuss below in more detail, the paradigm integrates, and builds on, findings from separate lines of research on unsupervised distributional learning during speech perception [DL, @clayards2008; @colby2018; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [LGPL, @cummings-theodore2023; VGPL, @kleinschmidt-jaeger2012; @vroomen2007], and accent adaptation [AA, @hitczenko-feldman2016; @tan2021]. Figure \@ref(fig:block-design-figure) illustrates our approach. Between groups of participants, we manipulate the amount and distribution of phonetic cues in the exposure input. <!-- We focus on a phonetic contrast that is known to be subject to adaptive changes in perception [syllable-initial /d/-/t/ in US English, @kleinschmidt2015; @kraljic-samuel2006]. --> The three exposure distributions we use are shifted to different degrees both relative to each other, and relative to listeners' prior expectations, allowing us to test predictions (1) and (2a,b). We measure listeners' categorization functions at multiple points during exposure, and test whether the direction and magnitude of the observed changes in behavior are consistent with the predictions of distributional learning models, including prediction (4) about the rate of changes in listeners' behavior. To further guide the interpretation of results, we use normative models of adaptive speech perception [ideal observers and adaptors, @massaro1989; @feldman2009; @kleinschmidt-jaeger2015; @xie2023]. This enables predictions about---intentionally idealized---listeners and distributional learners, prior to considerations about memory or other cognitive limitations. Comparisons of participants' categorization functions against these normative models provides a principled and informative approach to identifying constraints on adaptive speech perception, addressing prediction (3) [see also @norris-cutler2021].

To anticipate our results, we find that the changes in listeners' categorization behaviors largely follow the predictions of distributional learning models. In particular, we present the first evidence that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (predictions 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find that changes in rate of adaptation across exposure are consistent with the predictions of error-driven learning theories and theories of ideal information integration (prediction 4). We show that a Bayesian model of adaptation that is based on principles of ideal information integration [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses with very high accuracy ($R^2 = XXX$). However, not all observations we make fit the predictions of distributional learning models. In particular, we find little support for prediction 3. Rather, we find evidence for a previously unrecognized constraint on adaptive speech perception: changes in listeners' behavior seem to plateau long before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions. We further find that this constraint on adaptation seems to be asymmetric, depending on the direction of the shift in the exposure input relative to listeners' prior expectations. We discuss the implications of our findings for theories of adaptive speech perception, and suggest how future variants of our paradigm can be used to further contrast different models of adaptive speech perception. Before we describe our study, we summarize the relevant literature in more detail.

## What is known?
Previous work provides initial support for predictions (2a) and (2b). For example, recent LGPL/VGPL studies provide evidence in support of prediction (2a)---that the *amount* of phonetic evidence during exposure gradiently affects the magnitude of shifts in listeners' categorization boundary [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @vroomen2007]. In such paradigms, listeners are exposed to natural recordings of one phonetic category (e.g., /s/) and shifted instances of a second category that are manipulated to be perceptually more similar to the first category (e.g., /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/). Both the typical and the shifted sound instances are lexically or visually labeled by their context. For example, in an LGPL study, the lexical context will disambiguate the intended category of both the typical sounds (e.g., "dino*s*aur") and the shifted sounds (e.g., "medi*sh*ine"). Studies like this typically compare two groups of listeners that differ only in which of the two sounds was shifted. For example, one group of listeners might be exposed to 20 typical /s/ and 20 shifted /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/, mixed with 160 filler words that do not contain either sound. The other group of listeners might be exposed to 20 typical /`r linguisticsdown::cond_cmpl("ʃ")`/ and 20 shifted /`r linguisticsdown::cond_cmpl("ʃ")`/-like /s/, mixed with the same 160 filler words. Following exposure, the two groups of listeners will categorize sounds along an unlabeled test continuum (e.g., "asi" to "ashi") differently. Specifically, listeners in each group will categorize more sounds along the continuum as belonging to the category that was shifted during exposure [e.g., @norris2003; @eisner-mcqueen2005; @kraljic-samuel2005]. 

In a particularly informative study, Cummings and Theodore compared shifts in categorization function between groups of listeners after exposure to 1, 4, 10, or 20 lexically labeled shifted /s/ or /`r linguisticsdown::cond_cmpl("ʃ")`/ tokens (each matched by an equal number of unshifted tokens from the opposite category). Shifts in listeners' categorization functions increased with the number of exposure to tokens, in line with prediction (2a) of distributional learning models. @vroomen2007 found similarly increasing shifts in categorization functions *within* participants, comparing the effects of 1, 2, 4, ..., 32 exposures to visually labeled shifted tokens [see also @kleinschmidt-jaeger2012]. However, LGPL/VGPL paradigms---at least as used traditionally---limit experimenters' control over the phonetic properties of the exposure stimuli: consistent with the goals of those studies, shifted sound instances are selected to be perceptually ambiguous between two categories, rather than to exhibit specific phonetic distributions. This limits the extent to which such paradigms can inform predictions (1) and (2b) about the effects of phonetic distributions in prior and recent experience. To the extent that LGPL/VGPL research has assessed the effects of phonetic properties, this has thus largely been limited to qualitative post-hoc analyses [@drouin2016; @kraljic-samuel2007; @tzeng2021; for quantitative tests, see @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. 

Support for prediction (2b) has thus primarily come from DL studies. In an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the *bees*-*peas* continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning how /b/ and /p/ are distributed along VOT, listeners in the wide variance group were predicted to exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found, providing support for prediction (2b) that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior in ways consistent with distributional learning theories [see also @nixon2016; @theodore-monto2019].

Other studies have exposed different groups of listeners to distributions that were shifted in one or the other direction along a phonetic continuum---very much like the three exposure conditions in Figure \@ref(fig:block-design-figure) [@chladkova2017; @colby2018; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt2020]. For example, @kleinschmidt-jaeger2016 used the stimuli developed by Clayards to expose five different groups of listeners to VOT distributions for /b/ and /p/ that were shifted to different degrees. The five different exposure conditions were each shifted by 10msecs in VOT relative to the other, but held constant the distance between the /b/ and /p/ mean <!-- (always 40ms) --> and the variance of /b/ and /p/. <!-- (both always 8.3ms$^2$).--> All groups of listeners were exposed to 222 trials of exposure input. Unlike the present study, Kleinschmidt and Jaeger did not include a pre-test or incremental intermittent testing. Instead, the effect of exposure was estimated by estimating listeners' categorization functions over the last third of the 222 trials. This revealed that listeners' categorization functions were shifted in the direction of the exposure input, and that the magnitude of this shift increased with the degree of exposure. The fact that all five exposure conditions elicited the predicted effect relative to each other constitutes particularly strong support consistent with prediction (2b) that the distribution of phonetic cues in the exposure input affects the magnitude of adaptive changes in listeners' categorization functions. 

Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], these findings suggests that the *outcome* of adaptation is qualitatively compatible with predictions (2a) and (2b) of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015].^[A related line of work has used distributional learning and training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mcclelland1999; @pajak-levy2012; @pisoni1982]. These studies, too, have observed learning behavior qualitatively compatible with distributional learning models [for review, see @pajak2016].] Previous studies have, however, relied on tests that averaged over, and/or followed, hundreds of exposure trials. This leaves open *how adaptation incrementally unfolds* throughout the earliest moments of exposure---i.e., whether listeners' categorization behavior indeed changes in the way predicted by models of adaptive speech perception, developing from expectations based on previously experienced phonetic distributions (prediction 1) to increasing integration of the phonetic distributions observed during exposure to the unfamiliar talker (predictions 2a,b). Previous studies also focused on one prediction at a time, leaving open how the effect of prior expectations and the statistics of the unfamiliar input *jointly* explain adaptation, or even how the amount and distribution of exposure inputs *jointly* explain adaptation. Finally, we are not aware of any previous attempts to address predictions (3) and (4): without incremental testing, it is difficult to assess whether there are hard limits on adaptivity or simply 'how far the learner has gotten' with the exposure input they have received so far [for discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020]. For the same reasons, it is difficult to assess whether the build-up of adaptation follows the predictions of error-based learning or ideal information integration (prediction 4). 

The incremental exposure-test paradigm in Figure \@ref(fig:block-design-figure) begins to address these knowledge gaps. <!-- The experiment starts with a test block that assesses listeners' state prior to informative exposure---often assumed, but not tested, to be identical across exposure conditions [see also @colby2018; @xie2021cognition]. Additional intermittent tests---opaque to participants---then assess incremental changes up to the first 144 informative exposure trials. This lets us assess how the joint effects of exposure amount and exposure distribution---corresponding to predictions (2a) and (2b)---unfold *incrementally*, and whether this unfolding follows the predictions of error-based learning and ideal information integration (prediction 4). By comparing the direction of adaptation not only across conditions, but also relative to the distribution of phonetic cues in listeners' prior experience, we begin to assess prediction (1). Finally, by comparing adaptation against the behavior expected under idealized learners, we address prediction (3). We test predictions (1)-(4) in a Bayesian mixed-effects psychometric model (a mixture model extension of generalized linear mixed-effect models). Such models are commonly used in research on psychophysics to correct for attentional lapses and responses biases [see @prins2019bayesian], but remain underutilized within research on speech perception. --> 
In addition to our primary goals, we took several modest steps towards addressing concerns about ecological validity that might limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distribution in the experiment [see discussion in @baese-berk2018]. For example, previous distributional learning studies have often used highly unnatural, 'robotic'-sounding, speech. Beyond raising questions about what types of expectations listeners apply to such speech, these stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. Similarly, LGPL/VGPL studies have often used perceptually ambiguous stimuli obtained by 'acoustic blending'---mixing recordings of two words (e.g., "sin" and "shin") at different relative intensity. This, too, can create acoustic properties that are rarely, if ever, observed in human speech (Rachel Theodore, p.c.).  We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.

## Open science
All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). Following @xie2023, both this article and its supplementary information (SI) are written in R Markdown. This allows other researchers to replicate and validate our analyses with the press of a button using freely available software [R, @R; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an annual undergraduate class at the University of Rochester (BCS206/207), in which students replicate and extend previous work in the cognitive sciences. The ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work [@kleinschmidt-jaeger2015; @tan2021; @xie2023]. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.


