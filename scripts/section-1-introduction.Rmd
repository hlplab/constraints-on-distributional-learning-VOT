```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Successful speech recognition requires that listeners map the acoustic signal onto words and meanings. But this signal-to-meaning mapping varies across contexts and talkers. The same word spoken by different talkers can sound quite different; and conversely, the same acoustic signal can imply different words depending on the talker. Yet, healthy young adult listeners typically recognize speech quickly and accurately across a wide range of talkers and acoustic conditions (after decades of advances, automatic speech recognition is now approaching the accuracy that most listeners display effortlessly during everyday speech perception).

Research has identified *adaptivity* as a key component to the robustness of human speech perception. Although first encounters with an unfamiliar accent can cause initial processing difficulty, this difficulty can diminish rapidly with exposure [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. Twenty short sentences from an unfamiliar second language accent can significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Under ideal conditions, deviations from expected pronunciations along familiar phonetic dimensions can be detected after even shorter exposure [e.g., @cummings-theodore2023; @liu-jaeger2018], sometimes after a single trial [@vroomen2007]. Findings like these suggest that speech perception is highly adaptive, allowing listeners to quickly adjust the mapping from acoustics to phonetic categories and word meanings. While such rapid adaptation is now well-documented, its discovery challenged long-held assumptions, and spurred the development of new paradigms and theories [reviewed in @bent-baeseberk2021; @schertz-clare2020]. Of course, exposure had long been known to affect speech perception---after all, we can learn new languages with enough exposure. However, rapid adaptation to unfamiliar speech patterns unfolds over much shorter times scales (seconds and minutes).

What remains unclear is *how* rapid adaptation is achieved: how do listeners integrate information from a new talker, and how does this come to incrementally change listeners' interpretation of that talker's speech? This is the question we seek to contribute to here. To appreciate our approach to this question, it is helpful to reflect on the state of the field. Similar to many other fields in the cognitive sciences, research on adaptive speech perception tends to discuss informal---often descriptive, rather than explanatory---hypotheses [see discussion in @norris-cutler2021; @xie2023]. This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Such descriptions do not specify what mechanisms support adaptive speech perception, nor do they make predictions about how adaptation unfolds incrementally with each new observation from an unfamiliar talker. 

Viewed from the perspective of such informal hypotheses, research on adaptive speech perception can be an open-ended list of questions. Is adaptation more or less immediate, or does it unfold gradually? If the latter, do changes in listeners' behavior accumulate additively, leading to more or less linear changes in behavior? And, how does listeners' prior experience affect how listeners adapt? Are there limits to listeners' ability to fully adapt to a new talker? Or can we adapt to more or less any accent provided sufficient input? Under a question- rather than theory-driven approach, each such question can be---and often is---viewed in isolation. <!-- Integration of findings across studies is limited to intuitions of the type described in the preceding paragraph---intuitions that can be misleading [see discussion of "category expansion" in @hitczenko-feldman2016; "boundary shift" in @kleinschmidt-jaeger2015]. Integration of findings across studies is left to reviews, which then are limited to qualitative integration. --> This makes it difficult develop and test theoretical models that are sufficiently constrained to detect *informative incompatibility* with the data. As Allen Newell famously put it more than 50 years ago "you can't play 20 questions with nature and win" [@newell1973, p. 1].

Critically, there *are* integrative theories that make clear, quantifiable predictions about all of the questions in the preceding paragraph, including basic predictions that remain untested. One of the most developed of these theories is the hypothesis that adaptive speech perception draws on *distributional learning* [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assmann2007; @sohoglu-davis2016]. While distributional learning models differ from each other in important aspects, they share the central assumption that listeners incrementally learn and store information about the phonetic distributions that characterize the talker's speech. This includes information like the average values of phonetic cues, their variability, or even the full phonetic distributions of all speech categories. These statistical properties are then used to interpret subsequent speech from the talker, supporting robust speech recognition across talkers [reviewed in @schertz-clare2020; @xie2023]. 

With these assumptions of distributional learning theories come shared predictions, which we introduce next. Following this, we discuss why recent reviews have called for changes in the field's approach to adaptive speech perception [e.g., @bent-baeseberk2021; @xie2023]. This includes the need for paradigms that elicit richer, more constraining, quantitative data, as well as model-guided analyses that take advantage of such data [see also discussion in @coretta2023; @yarkoni-westfall2017]. This motivates the present study. We take a step towards the integrative, theory-driven vision outlined in @newell1973, "forcing enough detail and scope to tighten the inferential web that ties our experimental studies together" [p. 24]. We use this approach to test whether distributional learning can actually explain (1) a non-trivial share of the rapid changes in listeners' perception that come with exposure, and (2) any potential limits of such adaptation.

## Predictions of distributional learning
Consider a listener's initial encounter with an unfamiliar talker who produces some familiar sound categories, e.g. /d/ and /t/ in US English, in an unfamiliar way (dashed phonetic distributions in Figure \@ref(fig:predictions)A). Panels B & C show how the ideal categorization functions for the unfamiliar talker differ from those for a 'typical' talker of US English. Distributional learning theories predict that listeners' perception should change incrementally with exposure to the talker's speech. Specifically, the direction and magnitude of that change should gradiently depend on listeners' prior expectations based on relevant previously experienced speech input (**prediction 1 - *prior expectations***), and both the amount (**prediction 2a - *exposure amount***) and distribution of phonetic cues in the exposure input from the unfamiliar talker [**prediction 2b - *exposure distribution* **, for review, see @xie2023]. Listeners' categorization functions---the mapping from acoustics to phonetic categories and words---should gradually shift from a starting point that reflects the phonetic distributions of previously experienced speech towards a target that reflects the phonetic distributions of the new talker's speech. Some distributional learning models further predict that adaptation initially proceeds quickly and then slows down as the listener's expectations come to more closely approximate the phonetic distributions of the unfamiliar talker [**prediction 3 - *diminishing returns***, e.g., @harmon2019; @kleinschmidt-jaeger2015; @olejarczuk2018; @sohoglu-davis2016]. Finally, standard distributional learning models further make the critical prediction that this shift proceeds until the listener has fully learned the statistics of the new talker's speech (**prediction 4 - *learn to convergence***).^[Predictions 1-4 assume that listeners *know* that they are listening to the same new talker throughout exposure. Talker recognition is itself an inference process that we do not further discuss here [but see @kleinschmidt-jaeger2015; @magnuson-nusbaum2007].] Here, we aim to test all four predictions.

Figure \@ref(fig:predictions)D illustrates predictions 1-4. All panels show predictions 1 and 2a,b: the point of subjective equality (PSE) of listeners' categorization function initially is based on listeners' expectation for a 'typical' talker of US English (bottom gray line, prediction 1). With exposure, listeners acquire information about the unfamiliar talker's speech, and thus shift their categorization function towards the PSE that would be ideal for that talker's phonetic distributions (prediction 2b). With more exposure, this shift increases (prediction 2a). The four panels of Figure \@ref(fig:predictions)D  differ, however, in whether show predictions 3 and 4 (yellow shaded background) or alternative scenarios, such as immediate talker switching; linear, rather than sublinear, changes; or convergence against partial adaptation. These scenarios do not constitute an exhaustive list, but rather are meant to illustrate that there are many possible ways that adaptive speech perception might unfold---some more plausible than others. 'Premature convergence' against partial adaptation, for example, could result from strong constraints on distributional learning, as expected if the early moments of adaptation are limited to the reweighting of previously learned dialect or sociolect representations, rather than the acquisition of new representations for the unfamiliar talkers' speech [see discussion in @kleinschmidt-jaeger2015; @wade2022; @xie2018]. We return to this point in the general discussion, as it turns out to be particularly relevant to the present study.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally, using the pronunciation of US English word-initial /d/ and /t/ as an example (as in "dip" vs. "tip"). **A)** Thin gray lines indicate cross-talker variability in the realization of /d/ and /t/ along the primary phonetic cue that distinguishes them in US English (voice onset timing or VOT). Shown are 20 random talkers from a database of connected speech [@chodroff-wilson2018]. Thicker gray lines indicate the VOT distributions expected for a 'typical' talker (averaging over all talkers in the database). Dashed lines indicate a hypothetical unfamiliar talker with a noticeably different VOT distribution. **B)** Ideal categorization functions (described under *Methods*) along the VOT continuum for speech from a typical talker (solid gray) and speech from the unfamiliar talker (dashed blue). Arrows point to the point of subjective equality (PSE), the VOT that listeners are equally likely to identify as /d/ or /t/ (aka "category boundary"). **C)** Same as B) but just showing the PSE, now on y-axis, which is how we plot changes in PSE with increasing exposure in the next panel. **D)** Different ways in which listeners' PSEs might incrementally change with increasing exposure to the unfamiliar talker (from more transparent to less transparent). Horizontal lines indicates the ideal PSEs from C). 

```{r predictions, fig.height=base.height*3+2/3, fig.width=base.width*5, fig.cap="(ref:predictions)", fig.pos="H"}
# Create all talker-specific IOs for connected speech data, using only the VOT cue
d.talker_IO.VOT <- 
  make_IOs_from_data (
    data = d.chodroff_wilson.connected,
    cues = c("VOT"),
    groups = "Talker") 

# Add x, PSE, categorization, and get gaussian geoms
d.talker_IO.VOT %<>%
  nest(io = -Talker) %>%
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  add_gaussians_as_geoms_to_io(alpha = .2, linetype = 1, linewidth = .5) %>%
  bind_rows(
    # Do the same after aggregating all talker-specific IOs into a single 'typical' talker IO
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(Talker = "typical") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = 1, linewidth = 1.2),
    # Do the same after aggregating all talker-specific IOs into a single talker that is shifted by 35ms relative to the 'typical' talker
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(
        mu = map(mu, ~ .x + 35),
        Talker = "unfamiliar") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = "twodash", linewidth = 1.2))

p.gaussian <- 
  ggplot() +
  (d.talker_IO.VOT %>% 
     filter(
       !(Talker %in% c("typical", "unfamiliar")),
       Talker %in% sample(unique(Talker), 20)) %>% 
     pull(gaussian)) + 
  (d.talker_IO.VOT %>% 
     filter(Talker == "typical") %>% .$gaussian) +
  scale_colour_manual(values = c(colours.category_greyscale)) +
  guides(colour = "none") +
  new_scale_colour() +
  (d.talker_IO.VOT %>% 
     filter(Talker == "unfamiliar") %>% .$gaussian) +
  scale_colour_manual("Category", values = c("#02427e", "#b4dafe")) +
  guides(color = guide_legend(override.aes = list(size = 0.5, colour = c(colours.category_greyscale), values = c("/d/", "/t/")))) +
  labs(x = "VOT (ms)", y = "Density") +
  theme(
    legend.background = element_rect(fill='white'),
    legend.box.background = element_rect(fill='transparent'),
    legend.key = element_rect(fill = "transparent"),
    legend.key.size = unit(0.5, "cm"),
    legend.key.spacing = unit(0.05, "cm"),
    legend.key.height = unit(0.01, "cm"),
    legend.title = element_text(size = 7), 
    legend.text = element_text(size = 6),
    legend.position = "inside",
    legend.position.inside = c(0.79,0.8))

p.cat <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>% 
  select(Talker, category, categorization, PSE) %>% 
  filter(category == "/t/") %>% 
  unnest(categorization, names_repair = "unique") %>% 
  ggplot(aes(x = VOT, y = response, group = Talker, colour = Talker, linetype = Talker)) +
  geom_line(linewidth = 1.2) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  scale_linetype_manual(values = c("solid", "twodash")) +
  #scale_x_continuous("VOT (ms)", limits = c(-25, 130), breaks = c(0, 37, 72, 100)) +
  scale_y_continuous('Proportion "t"-responses', breaks = c(0, .5, 1)) +
  guides(linetype = "none", colour = "none") +
   geom_segment(
     data = 
       d.talker_IO.VOT %>% 
       filter(Talker %in% c("typical", "unfamiliar")) %>%
       mutate(x = PSE, xend = PSE, y = .5, yend = .01),
    mapping = aes(x = x, xend = xend, y = y , yend = yend, color = Talker),
    alpha = 0.5,
    arrow = arrow(type = "open" , length = unit(0.04, "npc")),
    inherit.aes = F) +
  scale_x_continuous(limits = c(15, 90), breaks = c(37, 72)) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

p.PSE <- 
d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar"), category == "/t/") %>%
  select(Talker, category, PSE) %>% 
  mutate(PSE = round(PSE)) %>% 
  ggplot() +
  geom_hline(data = . %>% filter(Talker == "typical"), aes(yintercept = PSE), 
             color = "grey", linewidth = 1.5, linetype = 1) +
  geom_hline(data = . %>% filter(Talker == "unfamiliar"), aes(yintercept = PSE), 
             color = "#02427e", linewidth = 1.5, linetype = "twodash") +
  scale_y_continuous("Ideal PSE (ms VOT)", limits = c(30, 75), breaks = c(37, 72)) +
  scale_x_continuous("", limits = c(0, .5), breaks = NULL) +
   annotate(
       geom = "text",
       x = 0.1, 
       y = c(41, 69),
       justify = 1,
       label = c("typical", "unfamiliar"),
       color = c("grey", "#02427e"),
       fontface = "bold",
       size = 2.5) +
  theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
 
PSE_change <- 
  expand_grid(
    PSE = c(),
    learning_pattern = factor(c("immediate", "linear", "diminished", "premature")),
    exposure_amount = factor(c("none", "1/3", "2/3", "complete"))) %>%
  mutate(
    learning_pattern = fct_relevel(learning_pattern, "immediate", "linear", "diminished", "premature"),
    exposure_amount = fct_relevel(exposure_amount, "none", "1/3", "2/3", "complete"),
    PSE = case_when(
      exposure_amount == "none" ~ 37,
      exposure_amount == "complete" & learning_pattern %in% c("immediate", "linear") ~ 72,
      exposure_amount %in% c("1/3", "2/3") & learning_pattern == "immediate" ~ 72,
      exposure_amount == "1/3" & learning_pattern == "linear" ~ 37 + ((72 - 37 + 1)/3),
      exposure_amount == "2/3" & learning_pattern == "linear" ~ 37 + 2 * ((72 - 37 + 1)/3),
      exposure_amount == "1/3" & learning_pattern == "diminished" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount == "2/3" & learning_pattern == "diminished" ~ 37 + 1.6 * ((72 - 37 + 1)/2),
      exposure_amount == "complete" & learning_pattern == "diminished" ~ 37 + 1.85 * ((72 - 37 + 1)/2) ,
      exposure_amount == "1/3" & learning_pattern == "premature" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount %in% c("2/3", "complete") & learning_pattern == "premature" ~ 37 + 25))

p.PSE_change <- 
  PSE_change %>% 
  ggplot(aes(exposure_amount, PSE, alpha = exposure_amount, group = 1), colour = "#02427e") +
  geom_rect(
    data = PSE_change  %>% 
      nest(data = c(exposure_amount, PSE)) %>% 
      group_by(learning_pattern) %>% 
      slice_sample(n = 1) %>% 
      select(learning_pattern) %>% 
      mutate(ymin = ifelse(learning_pattern == "diminished", -Inf, NA),
             ymax = ifelse(learning_pattern == "diminished", Inf, NA)),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "yellow",
    alpha = .15,
    inherit.aes = F) +
  geom_point(size = 1.8, colour = "#02427e") +
  geom_line(alpha = .3) +
  geom_hline(aes(yintercept = 37), linetype = 1, alpha = .8, colour = "grey") +
  geom_hline(aes(yintercept = 72), linetype = 2, alpha = .6, colour = "#02427e") +
  scale_x_discrete("Exposure amount", breaks = c("none", "complete"), labels = c("none", "full")) +
  scale_y_continuous("PSE (ms VOT)", limits = c(25, 75), breaks = c(30, 40, 50, 60, 70)) +
  guides(alpha = "none") +
  facet_wrap(~learning_pattern, nrow = 1, labeller = labeller(learning_pattern = c("immediate" = "immediate switch", "linear" = "linear", "diminished" = "convergence w/\ndiminishing returns", "premature" = "premature convergence\nw/ diminishing returns"))) +
  theme(strip.text = element_text(size = 9))


((p.gaussian | p.cat | p.PSE) / p.PSE_change) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

## What is (not) known about these predictions?
We discuss the available evidence for prediction 1-4 in more depth after presenting our own results. As Table \@ref(tab:predictions) summarizes, previous work has to some extent tested predictions 1 and 2a,b, whereas predictions 3 and 4 remain untested for adaptive speech perception. Diminishing returns (prediction 3)---a.k.a. the power law of learning [@newell-rosenbloom1981]---are predicted by many theories of learning [e.g., associative learning, @rescorla-wagner1972] and have been demonstrated across a wide range of learning phenomena [e.g., @anderson1990; @logan1988; @palmeri1997]. It is, however, unknown whether rapid changes in speech perception reflect the same type of learning, or any learning in the more narrow sense at all [see discussion in @xie2018]. For instance, in neuroimaging studies on adaptive speech perception, it is common to attribute rapid adaptation to changes in decision-making, rather than changes in the distributional mapping from phonetics to speech categories [e.g., @blanco-elorrieta2021; @myers-mesite2014]. Even less is known about prediction 4: despite it being a central prediction of distributional learning theories, it is not yet known whether rapid adaptation actually converges against the phonetic distributions in the input.

Some readers might thus consider predictions 3 and 4 to be of most interest, or---if one takes for granted that rapid adaptation during speech perception is driven by distributional learning---perhaps only prediction 4. However, even for the first two predictions, recent reviews have concluded that the evidence is perhaps not as strong as is often assumed [@bent-baeseberk2021; @xie2023]. We briefly discuss two reasons for this.

\begin{table}[!ht]
\begin{small}
\begin{tabular}{p{0.25\textwidth}p{0.75\textwidth}}
\hline
Prediction & Supported by evidence from \\
\hline
(1) - {\em prior expectations} &  (e.g., Kang \& Schertz, 2021; Schertz et al., 2016; Tan et al., 2021; Xie et al., 2021) \\

(2a) - {\em exposure amount}  &  (e.g, Vroomen et al., 2007; Cummings \& Theodore, 2023; Kleinschmidt \& Jaeger, 2011; Liu \& Jaeger, 2018) \\

(2b) - {\em exposure distribution} &  (e.g., Chl\'adkov\'a et al., 2017; Clayards et al., 2008; Colby et al., 2018; Hitczenko \& Feldman, 2016; Idemaru \& Holt, 2011; Theodore \& Monto, 2019) \\

(3) - {\em diminishing returns} & \textsc{Not previously tested for rapid changes in speech perception}. \\

(4) - {\em learn to convergence}  & \textsc{Not previously tested for rapid changes in speech perception} \\

\hline
\end{tabular}
\caption{Predictions of distributional learning models about incremental adaptation to an unfamiliar talker, and what is known about these predictions. Existing qualitative evidence suggests that the *outcome* of learning is compatible with predictions 1, 2a,b (only prediction 2a has been tested against {\em incremental} changes in listeners' behavior). Prediction 3 and 4 remain untested for adaptive speech perception.}
\label{tab:predictions}
\end{small}
\end{table}

<!-- Prediction 1 ({\bf prior expectations}) & Listeners' categorization function prior to informative exposure to an unfamiliar talker's speech is determined by the statistics of the speech input they have previously experienced from other talkers. & AA, \cite{kang-schertz2021, schertz2016, tan2021, xie2021cognition} \\ -->

<!-- Prediction 2a ({\bf exposure amount}) & \multirow{2}{.6\textwidth}{With increasing exposure to the new talker, listeners' adapt their prior expectations by integrating information about the talker's phonetic distributions. The direction and magnitude of changes in listeners' categorization function relative to their pre-exposure behavior are determined by the amount and distribution of phonetic cues relative the statistics of previously experienced speech input}. & LGPL/VGPL, \cite{cummings-theodore2023, kleinschmidt-jaeger2012, liu-jaeger2018, vroomen2007} \\ -->

<!-- Prediction 2b ({\bf exposure distribution}) & & AA, \cite{xie2021cognition, tan2021}; DL, \cite{clayards2008, chladkova2017, colby2018, idemaru-holt2011, kleinschmidt-jaeger2016, theodore-monto2019} \\ -->

<!-- Prediction 3 ({\bf diminishing returns}) & With each new observation, changes in listeners' behavior depend on the prediction error (or equivalently, the amount of new information) associated with that observation. As a consequence, listeners' behavior should initially change quickly and then less and less as listeners converge against the exposure distribution. & LGPL \cite{liu-jaeger2018} \\ 

<!-- Prediction 4 ({\bf learn to convergence}) &  With additional exposure, listeners will continue to adapt until they have fully learned the exposure distribution. & \\ -->

First, the different predictions have typically been pursued in separate lines of research, using very different paradigms. For instance, some paradigms investigate exposure to natural accents, typically using sentence recordings or even longer narrative recordings, with all their phonetic and other linguistic complexities [e.g. @bradlow-bent2008; @sidaras2009]. On the other end of the spectrum, some paradigms investigate exposure to many repetitions of the exact same non-word stimulus [e.g., @kleinschmidt-jaeger2012; @vroomen2007]. This has raised questions as to whether the different paradigms tap into the same mechanisms [e.g., @zheng-samuel2020; @xie2023], and the extent to which results of different paradigms are likely to generalize to everyday speech perception [e.g., @baeseberk2018]. 

For instance, prediction 1 has typically been tested by contrasting different groups of listeners in their ability to correctly understand natural accents, depending on the amount of long-term exposure listeners had with the accent [e.g., @porretta2017; @witteman2013]. In contrast, the bulk of existing research on prediction 2a (*exposure amount*) comes from paradigms that use manipulated speech, typically isolated non-word or word recordings [*lexically/visually-guided perceptual learning*, @cummings-theodore2023; @liu-jaeger2018; @poellmann2011; @vroomen2007]. These studies have found that repeated exposure in such paradigms results in a gradient, cumulative build-up of changes in listeners perception---in line with prediction 2a. Finally, prediction 2b has almost exclusive been tested in so-called *distributional learning* paradigms [e.g. @clayards2008; @escudero2011; @goudbeek2008; @idemaru-holt2011; @logan1991; @maye2002; @pisoni1982; @zhang-holt2018]. These paradigms, too, tend to repeat a small set of stimuli many times. Unlike perceptual learning paradigms, however, distributional learning experiments explicitly manipulate the distribution of phonetic cues. The paradigms also differ in whether the input is labeled or not---i.e., whether listeners can easily infer which sound categories the talker intended to produce. Whereas labeling information is typically absent in distributional learning experiments, thus requiring unsupervised learning [but see e.g., @chladkova2017; @goudbeek2008; @kleinschmidt2015], <!-- TO DO: Maryann, are these three papers all using some form of partially supervised exposure? no only kleinschmidt, raizada, jaeger. the other 2 compared labeled and unlabeled --> exposure stimuli in visually- or lexically-guided perceptual learning experiments are *always* labeled. Everyday speech perception, however, most likely would benefit from both supervised and supervised learning [@gibson2013]: in naturally-accented, connected speech, lexical and other linguistic context will provide effectively label some but not all sound segments. In short, there is value in investigating multiple predictions in the same paradigm---ideally, in a paradigm that mimics some of the complexities and affordances of everyday speech.

Some recent reviews [e.g., @xie2023] highlight a second, perhaps more concerning, limitation of existing evidence: most of it comes from weak tests of distributional learning theories (including some of our own work). Most notably, most previous work has typically *not* tested whether the phonetic distributions---i.e., the theoretical quantity that distributional learning theories predict to be the primary driver of adaptation---can actually quantitatively predict listeners' behavior. For instance, as summarized above, there is evidence that prior experience with an accent facilitates perception in line with prediction 1 [e.g., @porretta2017; @witteman2013], or that additional exposure further improves perception in line with prediction 2a,b [e.g., @escudero-williams2014; @logan1991]. However, none of these studies tested whether the specific observed effects of exposure are quantitatively predicted by the phonetic distributions in listeners' input. Such findings offer important initial tests of some of the core predictions of distributional learning theories. They do not, however, entail that distributional learning is the critical mechanism underlying adaptive speech perception. If it is, distributional learning should provide a *strong quantitative model of adaptive speech perception*, predicting a substantial share of the changes in listeners' perception while also correctly predicting any qualitative limits in listeners' adaptivity. This is what we aim to test here, essentially putting distributional learning to a 'stress test'. 

Based on computational simulations, @xie2023 argue that strong tests of distributional learning theories require (a) information about the distribution of phonetic cues in both listeners' prior experience and during exposure, (b) paradigms that measure incremental changes in listeners' behavior both within and across exposure conditions, and, critically, (c) analyses that quantitatively link the latter to the former---ideally, while comparing those changes in listeners' behavior to quantitative predictions of distributional learning models. Few existing studies meet these criteria. For example, only a handful of studies have begun to ask whether the effects of prior experience (prediction 1) can at least *qualitatively* be predicted from the phonetic distributions listeners are likely to have experienced previously [@kang-schertz2021; @schertz2016; @tan2021; @xie2021cognition]. The same holds for prediction 2a about the amount of exposure to the unfamiliar talker [@cummings-theodore2023; @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. Such qualitative tests leave important questions unaddressed. For instance, some recent experiments on lexically-guided perceptual learning suggest that adaptation only accumulates up to a certain number of exposure stimuli, after which no further changes were observed [@cummings-theodore2023; @liu-jaeger2018]. Does this 'ceiling' effect reflect the specific phonetic properties of the stimuli used in the experiment, or does it point to constraints on adaptive speech perception that are *not* predicted by distributional learning [see discussion in @kleinschmidt-jaeger2015]? To answer questions like these, it is necessary to compare incremental changes in listeners' perception against quantitative predictions of distributional learning models that are given the input as listeners. 

Even research that has explicitly manipulated the phonetic distributions listeners are exposed to (prediction 2b), has mostly been limited to qualitative test---e.g., whether the *direction* of the difference in listeners' categorization function after exposure can be predicted from the phonetic distributions during exposure [e.g., @clayards2008; @harmon2019; @nixon2016; @theodore-monto2019; for notable exceptions, see @bejjanki2011; @hitczenko-feldman2016; @kleinschmidt-jaeger2016].<!-- Could be omitted up to next para: --> For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Using a distributional learning model similar to the idealized learners we presented below, Clayards and colleagues predicted that listeners in the wide variance group would exhibit a more shallow categorization function than the narrow variance group. This is precisely what they found, providing qualitative support for prediction 2b [see also @nixon2016; @theodore-monto2019]. Studies like those by Clayards and colleagues come particularly close to the standard called for by @xie2023, and they form the inspiration for the present work. Next, we describe how we build on these works, including stronger joint tests of predictions 1 and 2a,b, as well as the first test of predictions 3 and 4 for adaptive speech perception.

## The present study
We combine (i) an incremental exposure-test distributional learning paradigm with (ii) incremental Bayesian mixed-effects psychometric models, and (iii) model-guided interpretation. Our paradigm is shown in Figure \@ref(fig:block-design-figure). Between groups of participants, we manipulate the distributions of phonetic cues during exposure (including both labeled and unlabeled input). Specifically, the exposure distributions are shifted to different degrees both relative to each other, and relative to listeners' prior expectations. Within each participant, we measure changes in the categorization functions at multiple points during exposure. This combination of exposure conditions and incremental testing allows us to obtain substantially more fine-grained, quantitative data about the early effects of distributional exposure than in previous work: in total, we measure participants' perception of /d/ and /t/ at over 400 combinations of preceding exposure and current VOT input (at least an order of magnitude more than in previous work). As we describe later, this is an important feature for work that seeks to assess how well a theory can actually explain listeners' behavior.

Unlike previous distributional learning, which have focused exclusively on the *outcome* of learning, we assess the effects of exposure incrementally, including tests after much shorter exposure than in previous experiments of this type [an order of magnitude fewer trials, @chladkova2017; @clayards2008; @colby2018; @escudero2011; @goudbeek2008; @goudbeek2009; @logan1991]. As detailed under *Methods*, we also take several modest steps to improve the ecological validity of the distributional learning paradigm, without loss of control over the relevant phonetic distributions [see discussion in @baeseberk2018]. 

The mixed-effects psychometric analyses we employ provide a theory-agnostic way to quantify these incremental changes in listeners' categorization function---the mapping from phonetic cues to speech categories. Finally, we use model-guided interpretation through comparison to ideal observer and ideal adaptor models to determine how closely listeners' behavior before, during, and following exposure matches the predictions of distributional learning [@feldman2009; @kleinschmidt-jaeger2015; @massaro1989; @xie2023]. This sheds light on whether distributional learning can explain a non-trivial share of the rapid changes in speech perception that come with exposure to an unfamiliar talker. Together, (i)-(iii) thus allow us to put the predictions in Table \@ref(tab:predictions) to a much stronger quantitative test than in previous work. 

<!-- TO DO: name exposure conditions in figure (vertical name in color matching the row, placed at left side of figure?) -->
<!-- perhaps you could add a little participant figure to each of the rows? with a right arrow to indicate that these participants are going through the blocks in this order??? -->
```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three {\\em between}-participant exposure conditions (rows) differed only in the distribution of voice onset time (VOT) during exposure (colored blocks). VOT is the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

<!-- Could be omitted up to next para: -->
To anticipate our results, we find that the changes in listeners' categorization behavior we observe *largely* follow the predictions of distributional learning models. In particular, we find that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (prediction 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find initial---though not decisive---evidence that changes in rate of adaptation throughout exposure are consistent with power law of learning (prediction 3). Perhaps most strikingly, we find that a comparatively simple model of distributional learning [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses across the 400+ exposure-test combinations with high accuracy ($R^2 = 96\%$). However, our model-guided data interpretation also reveals a previously unrecognized constraint on rapid adaptation that are unexpected under any existing model. In particular, we do *not* find support for prediction 4 (*learn to convergence*). Rather, changes in listeners' behavior seem to plateau before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions (cf. the *premature convergence* panel of Figure \@ref(fig:predictions)D). We discuss the implications of our findings for theories of adaptive speech perception, and suggest how future variants of our approach can be used to further contrast different models of adaptive speech perception. 

<!-- ^[Such unexpected limitations are more informative for theory development than findings that are 'merely' counter-intuitive. For instance, contrary to intuitions, some failures to find adaptation for some speech stimuli [@floccia2006; @zheng-samuel2020] are compatible with distributional learning [@tan2021], and the same has been argued for some seemingly arbitrary properties of adaptive speech perception [e.g., the 'undoing' of adaptation after prolonged exposure to the exact same stimulus, @vroomen2007; @kleinschmidt-jaeger2012]. The model-guided approach we take here is intended to reduce the reliance on intuition.]  -->

## Open science
All data and code for this article are available on OSF at [https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30](https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30). The OSF repo also contains detailed supplementary information (SI) that we refer to throughout this article. Following @xie2023, both this article and its SI are written in R Markdown. This allows other researchers to replicate and revise our analyses with the press of a button using freely available software [@R-base; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an undergraduate class at the University of Rochester (BCS206/207). The experiment was originally designed to address predictions 1, 2a,b, and 4. Our analyses of prediction (3 - *diminishing returns*) are post-hoc, as are some of the analyses we present to understand the evidence against prediction 4 (*learn to convergence*). All post-hoc analyses are indicated as such. Finally, the ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work ### **ommitted for review** ###<!-- [@kleinschmidt-jaeger2015; @tan2021; @xie2023]-->. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.

```{r}
rm(
  d.talker_IO.VOT,
  p.cat,
  p.PSE,
  p.PSE_change,
  p.gaussian)
```

