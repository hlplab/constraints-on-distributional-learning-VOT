```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Successful speech recognition requires that listeners map the acoustic signal onto words and meanings. But this signal-to-meaning mapping varies across talkers and context. The same word spoken by different talkers can sound quite different; and conversely, the same acoustic signal can imply different words depending on the talker. Yet, healthy young adult listeners typically recognize speech quickly and accurately across a wide range of talkers and acoustic conditions (after decades of advances, automatic speech recognition is now approaching the accuracy that most listeners display effortlessly during everyday speech perception).

Research has identified *adaptivity* as a key component to the robustness of human speech perception. Although first encounters with an unfamiliar accent can cause initial processing difficulty, this difficulty can diminish rapidly with exposure [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. For instance, twenty or so short sentences from an unfamiliar second language accent can significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Under ideal conditions, deviations from expected pronunciations along familiar phonetic dimensions can be detected after even shorter exposure [@cummings-theodore2023; @kleinschmidt-jaeger2012; @liu-jaeger2018], sometimes after a single trial [@vroomen2007]. Findings like these suggest that speech perception is highly adaptive, allowing listeners to quickly adjust the mapping from acoustics to phonetic categories and word meanings. While we now may take such rapid adaptation for granted, its discovery challenged long-held assumptions, and spurred the development of new paradigms and theories [reviewed in @bent-baeseberk2021; @cummings-theodore2023; @schertz-clare2020; @zheng-samuel2023]. Of course, researchers had long known that exposure affect speech perception even in adults---after all, we can learn new languages, provided we get enough exposure. However, the rapid changes described above unfold over much shorter times scales (seconds and minutes).

To this day, it remains unclear *how* rapid adaptation is achieved. How do listeners integrate information from a new talker, and how does this come to incrementally change their interpretation of that talker's speech? This is the question we seek to contribute to here. To appreciate our approach to this question, it is helpful to briefly reflect on the state of the field. Research on adaptive speech perception tends to discuss informal---often descriptive, rather than explanatory---hypotheses [see discussion in @norris-cutler2021; @xie2023]. This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Such descriptions do not specify what mechanisms support adaptive speech perception, nor do they make predictions about how adaptation unfolds incrementally with each new observation from an unfamiliar talker (the same criticism applies to some of our own work). 

Viewed from the perspective of such informal hypotheses, research on adaptive speech perception can be an open-ended list of questions. Is adaptation more or less immediate, or does it unfold gradually? If the latter, do changes in listeners' behavior accumulate additively, leading to more or less linear changes in behavior? If changes are non-linear, are they first slow and then fast, or first fast and then slow? And, how does listeners' prior experience affect how listeners adapt? Are there limits to listeners' ability to fully adapt to a new talker? Or can we adapt to more or less any accent provided sufficient input? Under a question- rather than theory-driven approach, each such question can be---and often is---viewed in isolation. <!-- Integration of findings across studies is limited to intuitions of the type described in the preceding paragraph---intuitions that can be misleading [see discussion of "category expansion" in @hitczenko-feldman2016; "boundary shift" in @kleinschmidt-jaeger2015]. --> Integration of findings across studies is left to reviews, which then are limited to qualitative integration. This makes it difficult develop and test theoretical models that are sufficiently predictive to detect *informative incompatibility* with the data. As Allen Newell put it ever so eloquently many decades ago "you can't play 20 questions with nature and win" [@newell1973, p. 1].

Critically, there *are* theories that make clear, quantifiable predictions about all of the questions in the preceding paragraph, including basic predictions that remain untested. One of the most developed of these theories is the hypothesis that adaptive speech perception draws on *distributional learning* [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assmann2007; @sohoglu-davis2016]. While distributional learning models differ from each other in important aspects, they share the central assumption that listeners incrementally learn and store information about talkers' speech. This includes information about the phonetic distributions that characterize the talker's speech, such as the average values of phonetic cues, their variability, or even the full phonetic distributions of all speech categories. These statistical properties are then used to interpret subsequent speech from the talker, supporting robust speech recognition across talkers [reviewed in @schertz-clare2020; @xie2023]. 

With these assumptions come shared qualitative predictions, which we introduce next. Following this, we discuss why recent reviews have called for novel paradigms and analysis approaches to test these predictions [@bent-baeseberk2021; @xie2023]. Simply put, previous work has identified several properties of adaptive speech perception that are *qualitatively* compatible with models of distributional learning---e.g., that prior experience with an accent facilitates perception [e.g., @porretta2017; @witteman2013], or that additional exposure further improves perception [e.g., @escudero-williams2014; @logan1991]. This does not, however, entail that distributional learning is the critical mechanism underlying adaptive speech perception. If it is, distributional learning should at least provide a *strong quantitative model of adaptive speech perception*, predicting a substantial share of the changes in listeners' perception while also correctly predicting any qualitative limits in listeners' adaptivity. This type of strong test is what we aim to develop here. We take a step towards the integrative, theory-driven vision outlined in @newell1973, "forcing enough detail and scope to tighten the inferential web that ties our experimental studies together" [p. 24]. As we discuss below, this requires a new combination of paradigm, analyses and model-guided interpretation [see discussion in @coretta2023; @yarkoni-westfall2017]. But this effort can pay off: our 'stress test' of distributional learning models suggests an important, previously unrecognized, constraint on adaptive speech perception.

## Predictions of distributional learning
Consider a listener's initial encounter with an unfamiliar talker who produces some sounds, e.g. /d/ and /t/, in an unexpected way (Figure \@ref(fig:predictions)A). Listeners' perception is predicted to change incrementally with exposure to the talker's speech (Figure \@ref(fig:predictions)B-C). Distributional learning models make four predictions about how these changes unfold incrementally. First, the direction and magnitude of that change should gradiently depend on listeners' prior expectations based on relevant previously experienced speech input from other talkers (**prediction 1 - *prior expectations***), and both the amount (**prediction 2a - *exposure amount***) and distribution of phonetic cues in the exposure input from the unfamiliar talker [**prediction 2b - *exposure distribution* **, for review, see @xie2023]. Specifically, listeners' categorization functions---the mapping from acoustics to phonetic categories and words---should gradually shift from a starting point that reflects the phonetic distributions of previously experienced speech towards a target that reflects the phonetic distributions of the new talker's speech. Some distributional learning models further predict that adaptation initially proceeds quickly and then slows down as the listener approaches the correct mapping from the acoustic signal to phonetic categories [**prediction 3 - *diminishing returns***, e.g., @harmon2019; @kleinschmidt-jaeger2015; @olejarczuk2018; @sohoglu-davis2016]. Such diminishing returns---a.k.a. the power law of learning [@newell-rosenbloom1981]---are predicted by many theories of learning [e.g., associative learning, @rescorla-wagner1972] and have been demonstrated across a wide range of learning phenomena [e.g., @anderson1990; @logan1988; @palmeri1997]. It is, however, unknown whether rapid changes in speech perception reflect the same type of learning, or any learning in the more narrow sense at all [see discussion in @xie2018]. For instance, in neuroimaging studies on adaptive speech perception, it is common to attribute rapid adaptation to changes in decision-making, rather than changes in the distributional mapping from phonetics to speech categories [e.g., @blanco-elorrieta2021; @myers-mesite2014]. Finally, standard distributional learning models further make the critical prediction that this shift proceeds until the listener has fully learned the statistics of the new talker's speech (**prediction 4 - *learn to convergence***)---a prediction that has, to the best of our knowledge, remained untested.^[Predictions 1-4 assume that listeners *know* that they are listening to the same new talker. Talker recognition is itself an active inference process that we do not further discuss here [but see @kleinschmidt-jaeger2015; @magnuson-nusbaum2007].]

## What is (not) known about these predictions?

\begin{table}[!ht]
\begin{small}
\begin{tabular}{p{0.28\textwidth}p{0.7\textwidth}}
\hline
Prediction & Supported by evidence from \\
\hline
(1) - {\em prior expectations} & (Kang \& Schertz, 2021; Schertz et al., 2016; Tan et al., 2021; Xie et al., 2021) \\

(2a) - {\em exposure amount}  & (Vroomen et al., 2007; Cummings \& Theodore, 2023; Kleinschmidt \& Jaeger, 2011; Liu \& Jaeger, 2018) \\

(2b) - {\em exposure distribution} & (Chl\'adkov\'a et al., 2017; Clayards et al., 2008; Colby et al., 2018; Hitczenko \& Feldman, 2016; Idemaru \& Holt, 2011; Kleinschmidt, 2020; Theodore \& Monto, 2019) \\

(3) - {\em diminishing returns} & \textsc{Not previously tested for rapid changes in speech perception}. \\

(4) - {\em learn to convergence}  & \textsc{Not previously tested for rapid changes in speech perception} \\

\hline
\end{tabular}
\caption{Predictions of distributional learning models about incremental adaptation to an unfamiliar talker. Only prediction 2a has been tested against {\em incremental} changes in listeners' behavior.}
\label{tab:predictions}
\end{small}
\end{table}

<!-- Prediction 1 ({\bf prior expectations}) & Listeners' categorization function prior to informative exposure to an unfamiliar talker's speech is determined by the statistics of the speech input they have previously experienced from other talkers. & AA, \cite{kang-schertz2021, schertz2016, tan2021, xie2021cognition} \\ -->

<!-- Prediction 2a ({\bf exposure amount}) & \multirow{2}{.6\textwidth}{With increasing exposure to the new talker, listeners' adapt their prior expectations by integrating information about the talker's phonetic distributions. The direction and magnitude of changes in listeners' categorization function relative to their pre-exposure behavior are determined by the amount and distribution of phonetic cues relative the statistics of previously experienced speech input}. & LGPL/VGPL, \cite{cummings-theodore2023, kleinschmidt-jaeger2012, liu-jaeger2018, vroomen2007} \\ -->

<!-- Prediction 2b ({\bf exposure distribution}) & & AA, \cite{xie2021cognition, tan2021}; DL, \cite{clayards2008, chladkova2017, colby2018, idemaru-holt2011, kleinschmidt-jaeger2016, theodore-monto2019} \\ -->

<!-- Prediction 3 ({\bf diminishing returns}) & With each new observation, changes in listeners' behavior depend on the prediction error (or equivalently, the amount of new information) associated with that observation. As a consequence, listeners' behavior should initially change quickly and then less and less as listeners converge against the exposure distribution. & LGPL \cite{liu-jaeger2018} \\ 

<!-- Prediction 4 ({\bf learn to convergence}) &  With additional exposure, listeners will continue to adapt until they have fully learned the exposure distribution. & \\ -->
-->

We discuss the available evidence for prediction 1-4 (summarized in Table \@ref(tab:predictions) below) in more depth after presenting our own results. Although the present study test all predictions, we were most interested in prediction 4, and how it might interact with prediction 3. Despite prediction 3 being a central part of distributional learning theories, it is not yet known whether listeners actually converge against the phonetic distributions in the input. Figure \@ref(fig:predictions)D illustrates predictions 3 and 4 and contrasts them with other possible scenarios, such as immediate talker switching; linear, rather than sublinear, changes; or convergence against partial adaptation. These alternative scenarios do not constitute an exhaustive list, but rather are meant to illustrate that there are many possible ways that adaptive speech perception might unfold---some more plausible than others. 'Premature convergence' against partial adaptation, for example, could result from strong constraints on distributional learning, as expected if the early moments of adaptation are limited to the reweighting of previously learned dialect or sociolect representations, rather than the acquisition of new representations [see discussion in @kleinschmidt-jaeger2015; @wade2022; @xie2018]. We return to this point in the general discussion, as it turns out to be highly relevant to the present study.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally, using the pronunciation of US English word-initial /d/ and /t/ as an example (as in "dip" vs. "tip"). **A)** Transparent lines indicate cross-talker variability in the realization of /d/ and /t/ along the primary cue used to distinguish them (voice onset timing or VOT). Shown are 20 random talkers from a database of connected speech [@chodroff-wilson2018]. Thicker solid lines indicate a 'typical' talker (averaging over all talkers in the database). Dashed lines indicate a hypothetical unfamiliar talker with a noticeably different distribution of VOT values. **B)** Ideal categorization functions (described under *Methods*) along the VOT continuum for speech from a typical talker (solid gray) and speech from the unfamiliar talker (dashed blue). Arrows point to the point of subjective equality (PSE), the VOT that listeners are equally likely to identify as /d/ or /t/ (aka "category boundary"). **C)** Same as B) but just showing the PSE, now on y-axis, which is how we plot changes in PSE with increasing exposure in the next panel. **D)** Different ways in which listeners' PSEs might incrementally change with increasing exposure to the unfamiliar talker (from more transparent to less transparent). Horizontal lines indicates the ideal PSEs from C). 

```{r predictions, fig.height=base.height*3+2/3, fig.width=base.width*5, fig.cap="(ref:predictions)", fig.pos="H"}
# Create all talker-specific IOs for connected speech data, using only the VOT cue
d.talker_IO.VOT <- 
  make_IOs_from_data (
    data = d.chodroff_wilson.connected,
    cues = c("VOT"),
    groups = "Talker") 

# Add x, PSE, categorization, and get gaussian geoms
d.talker_IO.VOT %<>%
  nest(io = -Talker) %>%
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  add_gaussians_as_geoms_to_io(alpha = .2, linetype = 1, linewidth = .5) %>%
  bind_rows(
    # Do the same after aggregating all talker-specific IOs into a single 'typical' talker IO
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(Talker = "typical") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = 1, linewidth = 1.2),
    # Do the same after aggregating all talker-specific IOs into a single talker that is shifted by 35ms relative to the 'typical' talker
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(
        mu = map(mu, ~ .x + 35),
        Talker = "unfamiliar") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = "twodash", linewidth = 1.2))

p.gaussian <- 
  ggplot() +
  (d.talker_IO.VOT %>% 
     filter(
       !(Talker %in% c("typical", "unfamiliar")),
       Talker %in% sample(unique(Talker), 20)) %>% 
     pull(gaussian)) + 
  (d.talker_IO.VOT %>% 
     filter(Talker == "typical") %>% .$gaussian) +
  scale_colour_manual(values = c(colours.category_greyscale)) +
  guides(colour = "none") +
  new_scale_colour() +
  (d.talker_IO.VOT %>% 
     filter(Talker == "unfamiliar") %>% .$gaussian) +
  scale_colour_manual("Category", values = c("#02427e", "#b4dafe")) +
  guides(color = guide_legend(override.aes = list(size = 0.5, colour = c(colours.category_greyscale), values = c("/d/", "/t/")))) +
  labs(x = "VOT (ms)", y = "Density") +
  theme(
    legend.background = element_rect(fill='white'),
    legend.box.background = element_rect(fill='transparent'),
    legend.key = element_rect(fill = "transparent"),
    legend.key.size = unit(0.5, "cm"),
    legend.key.spacing = unit(0.05, "cm"),
    legend.key.height = unit(0.01, "cm"),
    legend.title = element_text(size = 7), 
    legend.text = element_text(size = 6),
    legend.position = "inside",
    legend.position.inside = c(0.79,0.8))

p.cat <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>% 
  select(Talker, category, categorization, PSE) %>% 
  filter(category == "/t/") %>% 
  unnest(categorization, names_repair = "unique") %>% 
  ggplot(aes(x = VOT, y = response, group = Talker, colour = Talker, linetype = Talker)) +
  geom_line(linewidth = 1.2) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  scale_linetype_manual(values = c("solid", "twodash")) +
  #scale_x_continuous("VOT (ms)", limits = c(-25, 130), breaks = c(0, 37, 72, 100)) +
  scale_y_continuous('Proportion "t"-responses', breaks = c(0, .5, 1)) +
  guides(linetype = "none", colour = "none") +
   geom_segment(
     data = 
       d.talker_IO.VOT %>% 
       filter(Talker %in% c("typical", "unfamiliar")) %>%
       mutate(x = PSE, xend = PSE, y = .5, yend = .01),
    mapping = aes(x = x, xend = xend, y = y , yend = yend, color = Talker),
    alpha = 0.5,
    arrow = arrow(type = "open" , length = unit(0.04, "npc")),
    inherit.aes = F) +
  scale_x_continuous(limits = c(15, 90), breaks = c(37, 72)) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

p.PSE <- 
d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar"), category == "/t/") %>%
  select(Talker, category, PSE) %>% 
  mutate(PSE = round(PSE)) %>% 
  ggplot() +
  geom_hline(data = . %>% filter(Talker == "typical"), aes(yintercept = PSE), 
             color = "grey", linewidth = 1.5, linetype = 1) +
  geom_hline(data = . %>% filter(Talker == "unfamiliar"), aes(yintercept = PSE), 
             color = "#02427e", linewidth = 1.5, linetype = "twodash") +
  scale_y_continuous("Ideal PSE (ms VOT)", limits = c(30, 75), breaks = c(37, 72)) +
  scale_x_continuous("", limits = c(0, .5), breaks = NULL) +
   annotate(
       geom = "text",
       x = c(0.05, 0.09), 
       y = c(39, 70),
       label = c("typical", "unfamiliar"),
       fontface = "bold",
       size = 2.5) +
  theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
 
PSE_change <- 
  expand_grid(
    PSE = c(),
    learning_pattern = factor(c("immediate", "linear", "diminished", "premature")),
    exposure_amount = factor(c("none", "1/3", "2/3", "complete"))) %>%
  mutate(
    learning_pattern = fct_relevel(learning_pattern, "immediate", "linear", "diminished", "premature"),
    exposure_amount = fct_relevel(exposure_amount, "none", "1/3", "2/3", "complete"),
    PSE = case_when(
      exposure_amount == "none" ~ 37,
      exposure_amount == "complete" & learning_pattern %in% c("immediate", "linear") ~ 72,
      exposure_amount %in% c("1/3", "2/3") & learning_pattern == "immediate" ~ 72,
      exposure_amount == "1/3" & learning_pattern == "linear" ~ 37 + ((72 - 37 + 1)/3),
      exposure_amount == "2/3" & learning_pattern == "linear" ~ 37 + 2 * ((72 - 37 + 1)/3),
      exposure_amount == "1/3" & learning_pattern == "diminished" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount == "2/3" & learning_pattern == "diminished" ~ 37 + 1.6 * ((72 - 37 + 1)/2),
      exposure_amount == "complete" & learning_pattern == "diminished" ~ 37 + 1.85 * ((72 - 37 + 1)/2) ,
      exposure_amount == "1/3" & learning_pattern == "premature" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount %in% c("2/3", "complete") & learning_pattern == "premature" ~ 37 + 25))

p.PSE_change <- 
  PSE_change %>% 
  ggplot(aes(exposure_amount, PSE, alpha = exposure_amount, group = 1), colour = "#02427e") +
  geom_rect(
    data = PSE_change  %>% 
      nest(data = c(exposure_amount, PSE)) %>% 
      group_by(learning_pattern) %>% 
      slice_sample(n = 1) %>% 
      select(learning_pattern) %>% 
      mutate(ymin = ifelse(learning_pattern == "diminished", -Inf, NA),
             ymax = ifelse(learning_pattern == "diminished", Inf, NA)),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "yellow",
    alpha = .15,
    inherit.aes = F) +
  geom_point(size = 1.8, colour = "#02427e") +
  geom_line(alpha = .3) +
  geom_hline(aes(yintercept = 37), linetype = 1, alpha = .8, colour = "grey") +
  geom_hline(aes(yintercept = 72), linetype = 2, alpha = .6, colour = "#02427e") +
  scale_x_discrete("Exposure amount", breaks = c("none", "complete"), labels = c("none", "full")) +
  scale_y_continuous("PSE (ms VOT)", limits = c(25, 75), breaks = c(30, 40, 50, 60, 70)) +
  guides(alpha = "none") +
  facet_wrap(~learning_pattern, nrow = 1, labeller = labeller(learning_pattern = c("immediate" = "immediate switch", "linear" = "linear", "diminished" = "convergence w/\ndiminishing returns", "premature" = "premature convergence\nw/ diminishing returns"))) +
  theme(strip.text = element_text(size = 9))


((p.gaussian | p.cat | p.PSE) / p.PSE_change) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

For now, we briefly illustrate why recent reviews have called for new experimental paradigms and analysis approaches that can more strongly constrain theories of adaptive speech perception [@bent-baeseberk2021; @schertz-clare2020; @xie2023]. This call is of relevance even for predictions 1-2a,b, which have received qualitative support from previous work. Based on computational simulations, Xie and colleagues argue that strong tests of distributional learning theories require (a) information about the distribution of phonetic cues in both listeners' prior experience and during exposure, (b) paradigms that measure incremental changes in listeners' behavior both within and across exposure conditions, and, critically, (c) analyses that quantitatively link the latter to the former---ideally, while comparing those changes in listeners' behavior to quantitative predictions of distributional learning models. 

Few existing studies meet these criteria. The most common paradigms expose two groups of listeners to different speech patterns. Following exposure, both groups are tested on identical stimuli to see how exposure affected perception. Such designs were effective in establishing the *existence* of adaptive speech perception. They do, however, offer only weak tests of distributional learning models [see discussion in @xie2023; @cummings-theodore2023]: it is one thing to show that differences between two exposure conditions lead to a change in behavior that is in the direction predicted by distributional learning; it is rather different thing to test whether the changes in listeners' behavior over a large number of exposure-test combinations can be consistently explained by distributional learning.

For example, there is now evidence that long-term exposure to a particular accent over months and years can facilitate more accurate understanding of that accent, and faster adaptation to unfamiliar talkers of that accent [e.g., @porretta2017; @witteman2013]. Findings like these demonstrate that prior experience affects speech perception, in line with prediction 1 of distributional learning theories. These findings leave open, however, whether the specific benefits of prior experiences are predicted by the *distributions of phonetic cues* that listeners have previously experienced, rather than some other non-distributional learning algorithm. Only a handful of studies have begun to address this question, testing whether differences in the adaptation *outcome* between participant groups can at least qualitatively be accounted for by an estimate of the distributions of phonetic cues in participants' prior experience [@kang-schertz2021; @schertz2016; @tan2021; @xie2021cognition]. Here we build on those studies, assessing how well distributional learning models can explain the quantitative effects of a 'typical' listener's prior experience on both the perception of, and adaptation to, an unfamiliar talker. 

Similar considerations apply to findings that repeated exposure can result in a gradient, cumulative build-up of changes in listeners perception---in line with prediction 2a [e.g., @cummings-theodore2023; @liu-jaeger2018; @poellmann2011; @vroomen2007]. None of these studies analyzed the phonetic distributions listeners were exposed to, or linked those distributions to the observed incremental changes in listener's behavior. This leaves open whether distributional learning can explain the observed gradient changes in listeners' perception---as predicted if distributional learning is the driving force behind rapid adaptive speech perception. For instance, some recent findings suggest that adaptation only accumulates up to a certain number of exposure stimuli, after which no further changes were observed [@cummings-theodore2023; @liu-jaeger2018]. Does this 'ceiling' effect reflect convergence against the phonetic distributions of the exposure stimuli, or does it point to constraints on adaptive speech perception that are *not* predicted by distributional learning [see discussion in @kleinschmidt-jaeger2015]? To answer questions like these, it is necessary to compare incremental changes in listeners' perception against quantitative predictions of distributional learning models. This in turn requires paradigms that deliver more fine-grained data, providing sufficiently constraining evidence to evaluate such models.

"Distributional learning" experiments take a step in this direction by explicitly manipulate the phonetic distributions during exposure [e.g. @clayards2008; @escudero2011; @goudbeek2008; @goudbeek2009; @idemaru-holt2011; @maye2002; @pisoni1982; @zhang-holt2018]. These experiments have typically focused on prediction 2b, manipulating the phonetic distributions in the exposure input. Of particular note are studies that further applied distributional learning models to the exposure inputs of those experiments, and qualitatively or quantitatively compared the predictions of these models to listeners' behavior [@bejjanki2011; @clayards2008; @hitczenko-feldman2016; @harmon2019; @kleinschmidt2015]. <!-- Could be omitted up to next para: --> For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Using a distributional learning model similar to the idealized learners we presented below, Clayards and colleagues predicted that listeners in the wide variance group would exhibit a more shallow categorization function than the narrow variance group. This is precisely what they found, providing qualitative support for prediction 2b [see also @nixon2016; @theodore-monto2019]. Studies like those by Clayards and colleagues come particularly close to the standard called for by @xie2023, and they form the inspiration for the present work. Existing studies have, however, used much longer exposure than what we aim for here [an order of magnitude more than the earliest test in our study, @chladkova2017; @clayards2008; @colby2018; @escudero2011; @goudbeek2008; @goudbeek2009; @logan1991]. Next, we describe how we build on these works, including stronger joint tests of predictions 1 and 2a,b, as well as the first test of predictions 3 and 4 for adaptive speech perception.

## The present study
<!-- TO DO: potentially link to (a)-(c) above -->
We combine (i) the incremental exposure-test distributional learning paradigm shown in Figure \@ref(fig:block-design-figure) with (ii) incremental Bayesian mixed-effects psychometric models, and (iii) model-guided interpretation. Between groups of participants, we manipulate the distributions of phonetic cues during exposure (including both labeled and unlabeled input). Specifically, the exposure distributions are shifted to different degrees both relative to each other, and relative to listeners' prior expectations. Within each participant, we measure changes in the categorization functions at multiple points during exposure. This combination of exposure conditions and incremental testing allows us to obtain substantially more fine-grained, quantitative data about the early effects of distributional exposure than in previous work: in total, we measure participants' perception of /d/ and /t/ at over 400 combinations of preceding exposure and current VOT input. The mixed-effects psychometric analysis we present provides a theory-agnostic way to quantify these incremental changes in listeners' categorization function---the mapping from phonetic cues to speech categories. Finally, we use model-guided interpretation through comparison to ideal observer and ideal adaptor models to determine how closely listeners' behavior before, during, and following exposure matches the predictions of distributional learning [@feldman2009; @kleinschmidt-jaeger2015; @massaro1989; @xie2023]. This sheds light on whether distributional learning can explain a non-trivial share of the rapid changes in speech perception that come with exposure to an unfamiliar talker. Together, (i)-(iii) thus allow us to put the predictions in Table \@ref(tab:predictions) to a much stronger quantitative test than in previous work. 

<!-- TO DO: name exposure conditions in figure (vertical name in color matching the row, placed at left side of figure?) -->
<!-- perhaps you could add a little participant figure to each of the rows? with a right arrow to indicate that these participants are going through the blocks in this order??? -->
```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three {\\em between}-participant exposure conditions (rows) differed only in the distribution of voice onset time (VOT) during exposure (colored blocks). VOT is the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

Beyond our primary goals, the present work is motivated by one additional consideration. The need for quantifiable predictions---and the control this requires over the relevant phonetic distributions---can conflict with ecological validity. <!---  (though advances in automatic speech recognition and large language models might ultimately help resolve this tension)--> This has made some researchers question whether the mechanisms that explain adaptation during that are characteristic of everyday speech perception [see discussion in @baeseberk2018]. As detailed under *Methods*, we take several modest steps towards addressing these concerns. This includes concerns about both the stimuli and their distributions in the experiment. 

<!-- Could be omitted up to next para: -->
To anticipate our results, we find that the changes in listeners' categorization behavior we observe *largely* follow the predictions of distributional learning models. In particular, we find that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (prediction 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find initial---though not decisive---evidence that changes in rate of adaptation throughout exposure are consistent with power law of learning (prediction 3). Perhaps most strikingly, we find that a comparatively simple model of distributional learning [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses across the 400+ exposure-test combinations with high accuracy ($R^2 = 96\%$). However, our model-guided data interpretation also reveals a previously unrecognized constraint on rapid adaptation that are unexpected under any existing model. In particular, we do *not* find support for prediction 4 (*learn to convergence*). Rather, changes in listeners' behavior seem to plateau before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions (cf. the *premature convergence* panel of Figure \@ref(fig:predictions)D). We discuss the implications of our findings for theories of adaptive speech perception, and suggest how future variants of our approach can be used to further contrast different models of adaptive speech perception. 

<!-- ^[Such unexpected limitations are more informative for theory development than findings that are 'merely' counter-intuitive. For instance, contrary to intuitions, some failures to find adaptation for some speech stimuli [@floccia2006; @zheng-samuel2020] are compatible with distributional learning [@tan2021], and the same has been argued for some seemingly arbitrary properties of adaptive speech perception [e.g., the 'undoing' of adaptation after prolonged exposure to the exact same stimulus, @vroomen2007; @kleinschmidt-jaeger2012]. The model-guided approach we take here is intended to reduce the reliance on intuition.]  -->

## Open science
All data and code for this article are available on OSF at [https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30](https://osf.io/hxcy4/?view_only=270fc732415a49f5ab8f1fcaebf46b30). The OSF repo also contains detailed supplementary information (SI) that we refer to throughout this article. Following @xie2023, both this article and its SI are written in R Markdown. This allows other researchers to replicate and revise our analyses with the press of a button using freely available software [@R-base; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an undergraduate class at the University of Rochester (BCS206/207). The experiment was originally designed to address predictions 1, 2a,b, and 4. Our analyses of prediction (3 - *diminishing returns*) are post-hoc, as are some of the analyses we present to understand the evidence against prediction 4 (*learn to convergence*). All post-hoc analyses are indicated as such. Finally, the ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work ### **ommitted for review** ###<!-- [@kleinschmidt-jaeger2015; @tan2021; @xie2023]-->. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.

```{r}
rm(
  d.talker_IO.VOT,
  p.cat,
  p.PSE,
  p.PSE_change,
  p.gaussian)
```

