```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Adaptivity is a hallmark of human speech perception, supporting faster and more accurate speech recognition. On first encounters with an unfamiliar accent, the processing difficulty listeners might initially experience tends to alleviate with exposure [e.g., @bradlow-bent2008; @bradlow2023; @clarke-garrett2004; @sidaras2009; @xie2018; @xie2021jep]. Research over the last few decades has made substantial advances in identifying the conditions required for successful adaptation, its generalizability across talkers, and its longevity [for reviews, see @bent-baeseberk2021; @cummings-theodore2023; @zheng-samuel2023]. It is now clear that listeners' perceptual decision-making---mapping speech inputs to linguistic categories and, ultimately, word meanings---changes based on the acoustic and phonetic properties of recent input [e.g., @bertelson2003; @clayards2008; @mcmurray2011; @eisner-mcqueen2005; @idemaru-holt2011; @kraljic-samuel2005; @kurumada2013; @mcmurray-jongman2011; @norris2003; @reinisch-holt2014; @xie2017; for review, @schertz-clare2020; @xie2023]. This has supported the development of stronger theories of adaptive speech perception that explicitly link the distribution of phonetic properties in recent speech input to changes in subsequent speech recognition [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @assmann-nearey2007; @lancia-winter2013; @sohoglu-davis2016; @xie2023]. 

Previous work has typically framed questions as an 'either-or'---adaptation is either observed or not---consistent with the focus on identifying the necessary conditions for adaptation and generalization [see discussion in @cummings-theodore2023]. Recent reviews of the field instead emphasize the need to move towards stronger tests of existing theories, requiring the development of paradigms that support quantitative comparison to more strongly constrain the space of theoretical possibilities [@baese-berk2018; @schertz-clare2020; @xie2023]. This includes the need for data that characterize how adaptation develops *incrementally* as a function of exposure. While existing theories differ in important aspects, they share critical predictions about incremental adaptation that have remained largely untested: listeners' categorizations are predicted to change incrementally with exposure, and the direction and magnitude of that change should gradiently depend on (1) listeners' prior expectations based on relevant previously experienced speech input from other talkers, and both (2a) the amount and (2b) distribution of phonetic cies in the exposure input from the unfamiliar talker [for review, see @xie2023]. We report initial results from a novel incremental exposure-test paradigm designed to test these predictions during the early moments of adaptation. The development of this paradigm was guided by two considerations that go beyond our immediate goals. First, we aimed for a paradigm that lets us assess the *joint* effects of prior and recent exposure. Second, the paradigm should eventually be scaleable to stronger quantitative tests that allow the comparisons of *predictive models* of adaptive speech perception [for discussion, see @platt1964; @norris-cutler2021; @yarkoni-westfall2017; @xie2023]. In the general discussion, we provide an initial demonstration of how predictive models can support theory development.

Figure \@ref(fig:block-design-figure) illustrates our approach. The experiment integrates, and builds on, computational and behavioral findings from separate lines of research on unsupervised distributional learning during speech perception [DL, @clayards2008; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [LGPL, @cummings-theodore2023; VGPL, @kleinschmidt-jaeger2012; @vroomen2007], and accent adaptation [AA, @hitczenko-feldman2016; @tan2021]. These paradigms have complementing strengths that we seek to combine and extend. <!-- TO DO: consider briefly elaborating here? -->  Following previous work on distributional learning in speech perception, we expose different groups of listeners to phonetic distributions that are shifted to different degrees [@bejjanki2011; @clayards2008; @kleinschmidt2015; @munson2011; @nixon2016; @theodore-monto2019]. Unlike these works, we incrementally assess changes in listeners' categorization from pre-exposure onward.

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. Exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

Following previous DL studies, we use phonetically manipulated stimuli. This gives researchers control over the distribution of acoustic-phonetic properties that listeners experience during exposure and test (unlike traditional AA, LGPL, and VGPL paradigms). Such control is an important prerequisite for stronger tests of predictions (1) and (2a,b). For example, recent findings from LGPL and VGPL provide evidence in support of prediction (2a)---that the amount of phonetic evidence during exposure gradiently affects the magnitude of shifts in listeners' categorization boundary [@cummings-theodore2023; see also @liu-jaeger2018; @liu-jaeger2019]. This includes some initial evidence that these changes accumulate incrementally [@kleinschmidt-jaeger2012; @vroomen2007], in ways consistent with models of adaptive speech perception. LGPL/VGPL paradigms---at least as used traditionally---do, however, limit experimenters' control over the phonetic properties of the exposure stimuli: consistent with the goals of those studies, shifted sound instances are selected to be perceptually ambiguous between two categories, rather than to exhibit specific phonetic distributions. This limits the extent to which such paradigms can inform predictions (1) and (2b) about the effects of phonetic distributions in prior and recent experience. To the extent that LGPL/VGPL research has assessed the effects of phonetic properties, this has thus largely been limited to qualitative post-hoc analyses [@drouin2016; @kraljic-samuel2007; @tzeng2021; for quantitative tests, see @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. 

Support for prediction (2b) has thus primarily come from DL studies. In an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the *bees*-*peas* continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning how /b/ and /p/ are distributed along VOT, listeners in the wide variance group were predicted to exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found [see also @nixon2016; @theodore-monto2019]. 

Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], this important finding suggests that the *outcome* of adaptation is qualitatively compatible with the predictions of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015].^[A related line of work has used distributional learning and training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mcclelland1999; @pajak-levy2012; @pisoni1982]. These studies, too, have observed learning behavior qualitatively compatible with distributional learning models [for review, see @pajak2016].] Previous DL studies have, however, relied on tests that averaged over, and/or followed, hundreds of exposure trials. This leaves open how adaptation incrementally unfolds throughout the earliest moments of exposure---i.e., whether listeners' categorization behavior indeed changes in the way predicted by models of adaptive speech perception, developing from expectations based on previously experienced phonetic distributions to increasing integration of the phonetic distributions observed during exposure to the unfamiliar talker. It also leaves open whether potential constraints on changes in listeners' behavior reflect hard limits on adaptivity or simply the incremental learning outcome---'how far the learner has gotten'---at the only point at which adaptation is assessed [for discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020].

The incremental exposure-test paradigm in Figure \@ref(fig:block-design-figure) begins to address these knowledge gaps. The experiment starts with a test block that assesses listeners' state prior to informative exposure---often assumed, but not tested, to be identical across exposure conditions. Additional intermittent tests---opaque to participants---then assess incremental changes up to the first 144 informative exposure trials. The use of physically identical test trials across both blocks and exposure conditions facilitates assumption-free comparison of cumulative exposure effects. As we detail under Methods, the use of incremental testing deviates from previous work [@clayards2008; @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019], and is not without challenges. 

The incremental exposure-test design lets us assess how the joint effect of exposure amount and exposure distribution---corresponding to predictions (2a) and (2b)---unfolds *incrementally*. And, by comparing the direction of adaptation not only across conditions, but also relative to the distribution of phonetic cues in listeners' prior experience, we begin to assess prediction (1). We test these predictions in a Bayesian mixed-effects psychometric model (a mixture model extension of generalized linear mixed-effect models). Such models are commonly used in research on psychophysics to correct for attentional lapses and responses biases [see @prins2019bayesian], but remain underutilized within research on speech perception. Here, we extend the psychometric model to analyze incremental changes in participants' categorization functions across blocks and exposure conditions. To further guide the interpretation of results, we use normative models of adaptive speech perception [ideal observers and adaptors, @massaro1989; @feldman2009; @kleinschmidt-jaeger2015; @xie2023]. This enables predictions about---intentionally idealized---listeners and distributional learners, prior to considerations about memory or other cognitive limitations. Comparisons of participants' categorization functions against these normative models provides a principled and informative approach to identifying constraints on adaptive speech perception in human listeners.

Finally, we took several modest steps towards addressing concerns about ecological validity that might limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distribution in the experiment [see discussion in @baese-berk2018]. For example, previous distributional learning studies have often used highly unnatural, 'robotic'-sounding, speech. Beyond raising questions about what types of expectations listeners apply to such speech, these stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. Similarly, LGPL/VGPL studies have often used perceptually ambiguous stimuli obtained by 'acoustic blending'---mixing recordings of two words (e.g., "sin" and "shin") at different relative intensity. This, too, can create acoustic properties that are rarely, if ever, observed in human speech (Rachel Theodore, p.c.).  We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.

## Open science
All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). Following @xie2023, both this article and its supplementary information (SI) are written in R Markdown. This allows other researchers to replicate and validate our analyses with the press of a button using freely available software [R, @R; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an annual undergraduate class at the University of Rochester (BCS206/207), in which students replicate and extend previous work in the cognitive sciences. The ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work [@kleinschmidt-jaeger2015; @tan2021; @xie2023]. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.


