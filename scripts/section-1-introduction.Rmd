```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

integrate: magnuson-nusbaum2007; magnuson2020

# Introduction
Adaptivity is a hallmark of human speech perception. When exposed to an unfamiliar accent, listeners might initially experience processing difficulty, but this difficulty alleviates with exposure, supporting faster and more accurate speech recognition [e.g., @bradlow2008; @bradlow2023; @clarke-garrett2004; @sidaras2009; @xie2018jasa; for review, see @baeseberk2018; @xie2021jep]. Such adaptivity is now recognized as a critical component of robust language understanding. Research over the last few decades has made strides in identifying its fundamental properties, including the conditions required for successful adaptation, its generalizability across talkers, and its longevity [for reviews, see @bent-baeseberk2021; @cummings-theodore2023]. Progress has also been made in understanding how adaptation proceeds. In particular, it is now clear that listeners' categorization function---the mapping from acoustic or phonetic cues to phonetic categories---changes based on the phonetic properties of recent input [@bertelson2003; @clayards2008; @cole2011; @eisner-mcqueen2005; @idemaru-holt2011; @kraljic-samuel2005; @kurumada2013; @mcmurray-jongman2011; @norris2003; @reinisch-holt2014; @xie2018jep; for review, see @schertz-clare2020; @xie2023]. This has led to the development of stronger theories and models of adaptive speech perception that explicitly link the phonetic properties of recent speech input to changes in subsequent speech recognition [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @nearey-assman2007; @winter-lancia2013; @sohoglu-davis2016]. 

While these theories differ in important aspects [for review, see @xie2023], they share critical predictions that have remained largely untested. In particular, adaptation is predicted to develop *incrementally*, *accumulate* over exposure, at each point depending *gradiently* on both the amount and the statistics of the speech input experienced so far. We report initial results from a novel repeated exposure-test paradigm that aims to test these predictions during the early moments of adaptation. In the long run, a clearer picture of how speech recognition changes with exposure, how those changes accumulate, and whether there are constraints on this accumulation has been identified as critical in developing stronger tests for theories of adaptive speech perception [@martin2023; @xie2023]. 

The experiment we report builds on computational and behavioral findings in research on unsupervised learning during speech perception [@clayards2008; @kleinschmidt-jaeger2016] and visually- or lexically-guided perceptual learning [@cummings-theodore2023; @kleinschmidt-jaeger2012; @vroomen2007]. The two paradigms have complementing strengths. <!-- TO DO: consider briefly elaborating here? --> Perhaps the clearest evidence that adaptation to unfamiliar speech depends on the statistics of the input---specifically, the distribution of phonetic cues---comes from the former paradigm [@bejjanki2011; @clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019]. In an important early study, Clayards and colleagues exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial /b/-/p/, /d/-/t/, /g/-/k/ in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the "bees"-"peas" continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning the category statistics of the exposure input---in this case, the distribution of VOT for /b/ and /p/---they were predicted to change their categorization function along VOT such that listeners in the wide variance group should exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found [see also @nixon2016; @theodore-monto2019]. Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], this suggests that the *outcome* of adaptation qualitatively follows the predictions of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015]. 

It leaves open, however, how adaptation *incrementally accumulates* with increasing exposure, and whether it does so in line with predictions of distributional learning models. Initial evidence that speaks to this question comes from research on lexically- or visually-guided perceptual learning [@bertelson20023; @norris2003; @kraljic-samuel2005]. In these paradigms, listeners are exposed to phonetically manipulated instances of a sound category (e.g., making the "s" in "embassy" sound almost like an "sh"), mixed with many filler words without that sound. Following such exposure, listeners are known to shift their categorization function. For example, after being exposed to instances of "sh"-like "s" listeners categorize more tokens along the "s"-"sh" continuum as "s". Recent work within those paradigms has found that the magnitude of the boundary shift increases for listeners who are exposed to more instances of the shifted sound [@liu-jaeger2018; @liu-jaeger2019; @cummings2023; @vroomen2007, up to a point, @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @liu-jaeger2018; @vroomen2007]. This suggest that adaptation accumulates with exposure, rather than being an all or nothing process [@cummings2023]. There are, however, important limitations to these findings. Perceptual recalibration paradigms, at least as used traditionally, limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to be perceptually ambiguous (e.g., between "s" and "sh"), not to exhibit specific phonetic distributions. To the extent that researchers have aimed to understand the consequences of phonetic properties on the degree of boundary shift following exposure, this has been limited to post-hoc analyses [@drouin2016; @kaljic-samuel2007; @other-cummings?]. It is thus an open question to what extent the boundary shifts observed in such experiments reflect not only the quantity, but also the distribution of phonetic properties, during exposure (as predicted by distributional learning models).

This motivates the present study. We modify the distributional learning paradigm of @clayards2008 to shed light on the cumulative effects of incremental adaptation. We exposure participants to instances of "d" and "t", and manipulate the distribution of VOT between participants, while intermittently testing within-participants how listeners' categorization functions change with exposure. The resulting repeated exposure=test design is shown in Figure \@ref(fig:block-design-figure). 

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Exposure-test design of the experiment. Test blocks presented identical stimuli within and across conditions"}
knitr::include_graphics("../figures/block_design.png")
```

The use of repeated testing deviates from previous work, and not without challenges. Previous work has instead employed 'batch testing' designs, in which changes in categorization responses are assessed only after extended exposure to hundreds of trials or by averaging over similarly extended exposure [@clayards2008; @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019]. By introducing intermittent testing we aim to assess how increasing exposure affects listeners' perception without making strong assumptions about the nature of these changes (such as assuming linearity, or penalizing non-linearity, of changes over trials). However, we cannot afford *extended* intermittent testing for three reasons. First, listeners' attention span is limited. Even prior to additional testing, typical distributional learning experiments span 200-400+ trials. Extending them further risks increasing attentional lapses and deteriorating data quality. Second, previous work has found that repeated testing over uniform test continua can reduce or undo the effects of informative exposure [@liu-jaeger2018; @liu-jaeger2019; @cummings-theodore2023].<!-- TO DO: Maryann, I don't think the 2023 paper shows that, right? is it the 2022 paper or an even more recent one by Shawn? --> Third, holding the distribution of test stimuli constant across exposure condition inevitably means that the relative unexpectedness of these test stimuli differs between the exposure conditions By keeping tests short relative exposure (12 vs. 48 trials), we aimed to minimize the influence of test trials on adaptation. The final three test blocks were intended to ameliorate the potential risks of this novel design: in case adaptation remains stable despite repeated testing, those additional test blocks were meant to provide additional statistical power to detect the effects of cumulative exposure.

We also made several additional adjustments to the paradigms used in previous work, meant to increase the ecologically validity of both stimuli and exposure distributions. This serves the longer-term goal of bridging the gap between research paradigms that afford control over phonetic properties at the cost of ecological validity, and paradigms that afford high ecological validity (e.g. adaptation to natural accents) at the cost of control. We describe the adjustments in more detail under Methods but briefly anticipate them here. The pioneering works we build on employed speech stimuli that were clearly identifiable as synthesized, sounding robotic, and did not exhibit natural correlations between phonetic cues  [@clayards2008; @kleinschmidt-jaeger2016]. We instead created natural sounding stimuli [building on @theodore-monto2019] that exhibited correlations between VOT and other cues to word-initial "d"-"t" that typical to everyday speech [@REF]. Previous work also *designed* rather than *sampled* exposure distributions. As a consequence, exposure distributions in these experiments were symmetrically balanced around the category means [see also @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @vroomen2007; a.o.]---unlike in everyday speech input which constitutes heterogeneous *random samples* of the underlying phonetic distributions. Indeed, all previous studied we build on exposed listeners to categories with *identical* variances [e.g., identical variance along VOT for /b/ and /p/, @clayards2008; @kleinschmidt-jaeger2016; or /g/ and /k/, @theodore-monto2019]. This, too, is highly atypical for everyday speech input [@lisker-abramson1964]. We instead expose listeners to random samples of phonetic cues that exhibit natural asymmetries in category variance based on a phonetically annotated database of word-initial /d/ and /t/ in US English [@chodroff-wilson2018]. 









## Other notes 

The predominant paradigms in research on adaptive speech perception are, however, not well-suited to address this question. As @cummings-theodore2023 summarize, "most research [...] has focused on identifying the conditions that are necessary for adaptation to occur" and "consistent with [this goal], outcomes [...] are most often considered as a binary result---does any learning occur, or not?" As a consequence, much remains unknown about how exposure comes to affect perception. It is unclear, for example, whether adaptive changes accumulate depending on both the amount of speech input and its statistical properties in the way predicted by the most explicit theoretical frameworks [e.g., the ideal adaptor, @kleinschmidt-jaeger2015; C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]. 

Typical paradigms manipulate exposure between listeners, and then assess the effects of exposure on subsequent test stimuli that are identical for all groups [for review, see @baese-berk2018; @schertz-clare2020]. These types of paradigms have provided evidence that adaptation to an unfamiliar talker can be rapid. For example, a thought-provoking finding by @clarke2004 suggests that exposure to eighteen sentences from an L2-accented talker---less than two minutes of speech---can be sufficient to facilitate significantly faster processing of that speech. This finding has since been replicated and extended to show that equally short exposure can facilitate recognition that is both faster and more accurate [@xie2018; for related results, see also @bradlow2023; @xie2017; @xie2021]. Other work has traded the ecological validity of natural L2 accents against increased control over the phonetic properties of exposure and test stimuli---a critical step towards stronger tests, as competing hypotheses about the mechanisms underlying adaptive speech perception require strong linking hypotheses mapping the acoustic input onto listeners' responses [@martin2023; @xie2023]. One such paradigm is lexically- or visually-guided perceptual recalibration [@bertelson20023; @norris2003; @kraljic-samuel2005], in which listeners are exposed to phonetically manipulated instances of a sound (e.g., making the "s" in "embassy" sound almost like an "sh"), mixed with many filler words without that sound. Following such exposure, listeners are known to shift their categorization function, so as to categorize more tokens along the "s"-"sh" continuum as "s". Recent work within those paradigms has found that as little as four phonetically shifted instances of a sound category can be sufficient to significantly alter listeners' categorization boundary [@liu-jaeger2018; @liu-jaeger2019; @cummings2023; @vroomen2007]. The same studies have found that exposure seems to accumulate, leading to larger boundary shifts for listeners who were exposed to more instances of the shifted sound [up to a point, @liu-jaeger2018; @vroomen2007; see also @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. Findings like these suggest that even rapid adaptation can be cumulative, rather than being an all or nothing process.

There are, however, important limitations to what perceptual recalibration paradigms can tell us about incremental adaptation. As is typical for such paradigms, all of the above experiments exposed listeners to shifted pronunciations that were always lexically or visually labeled stimuli (e.g., embedding the "sh"-like "s" in the word "embassy", which effectively labels it as an "s"). Such labeling is known to facilitate adaptation [@burchill2018; @burchill2023]---indeed, if shifted pronunciations are embedded in minimal pair or nonce-word context, listeners do no longer shift their categorization boundary [@norris2003; @REF-theodore?]. In everyday speech perception, however, listeners often have uncertainty about the word they are hearing, and must either use contextual information to label the input or adapt from unlabeled input. Perceptual recalibration paradigms, at least as used traditionally, also limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to sound ambiguous between, e.g., "s" and "sh", not based on their phonetic properties. To the extent that researchers have aimed to understand the consequences of those phonetic properties on the degree of boundary shift following exposure, this has involved post-hoc analyses [@drouin2016; @other-cummings?]. It is thus an open question to what extent the boundary shifts observed in such experiments reflect not only the quantity, but also the distribution of phonetic properties, during exposure [as would be expected under, e.g., the ideal adaptor framework].

The present work thus employs a novel repeated-exposure-test paradigm that explicitly control the distribution of phonetic properties during exposure.  [clayards, bejjanki; kj16, k20; see also theodore-monto2019]


[existing evidence comes from paradigms that emphasize ecological validity at the cost of less control: accent adaptation. Recent work expands on these findings using PR. Here we ]

for distributional only exposure-test or exposure/test

Contributions:
 + ecological validity of stimuli:
   + how they sound
   + correlation of VOT and f0
   + anti-correlation of VOT and vowel duration
 + ecological validity of distribution
   + variances that mimic those found in natural speech
   + samples drawn from theoretical distributions, rather than presentation of perfectly symmetrical stimulus samples

Possible framings:
 1a) mechanisms remain unknown
 1b) need for stronger tests of theories
 2) incremental cumulative effects









## Maryann's most recent intro
Recent reviews have identified distributional learning of marginal cue statistics ['normalization', @apfelbaum-mcmurray2015; @mcmurray-jongman2011; @magnuson-nusbaum2007] or the statistics of cue-to-category mappings as an important mechanism affording this adaptivity ['representational learning', @clayards2008; @idemaru-holt2011; @kleinschmidt-jaeger2015; @davis-sohoglu2020; for review, @schertz-clare2020; @xie2023]. This hypothesis has gained considerable influence over the past decade, with findings that changes in listener perception are qualitatively predicted by the statistics of exposure stimuli [@bejjanki2011; @clayards2008; @idemaru-holt2020; @kleinschmidt2012; @munson2011; @nixon2016; @theodore-monto2019; @tan2021; for important caveats, see @harmon2019].

Viewing speech perception as an adaptive process has been pivotal in our understanding of how human listeners overcome the lack of invariance problem; a problem fully appreciated when one begins to map out the variability of acoustic-phonetic cues that point to a single linguistic category [e.g. @delattre1955; @peterson-barney1952; @newman2001]; compounded when talker sex, age, social class, dialect and a host of other contexts are factored into consideration. Listeners' aptitude at speech comprehension however, belie this challenge. Given the uncertainty involved it is not surprising models of spoken word recognition that allow for probabilistic outcomes have left a lasting impression [@mcllelland-elman1986; @vitevitch-luce; @norris-mcqueen2008].

Over the past 20 years there have been prolific investigations into how and when listeners adjust their phonological categories after hearing acoustically manipulated speech sounds. These manipulations take place at the margins of linguistic categories where perception can be heavily influenced by the contexts in which they are presented [@norris2003; @mcqueen2006]. A sound that is ambiguous between /s/ and /sh/ presented in the utterance *contradiction* would bias its interpretation as /sh/ since *contradicson* is not a word. Repeated exposure to the sound in such biasing word contexts reliably elicits a shift in perception along the /s/-/sh/ continuum in subsequent testing -- those having heard the sound in /sh/-biasing words tend to give more /sh/ responses; vice-versa for those who were exposed to it in /s/-contexts. This perceptual recalibration of less prototypical category members has also been induced under audio-visual manipulations [@bertelson2003; @vroomen2007]. The paradigm has been exploited to its fullest to investigate, among other things, the sustainability of perceptual changes [@eisner-mcqueen2006; @kraljic-samuel2005], its generalizability to members of the same phonological class [@kraljic-samuel2006], and its generalizability to other talkers [@kraljic-samuel2007; @reinisch-holt2014].

In general, these findings are compatible with exemplar and other probabilistic updating frameworks that link the distributions of cues to changes in category mappings hence perceptual recalibration findings can to an extent inform general understanding of talker adaptation. But the mechanisms that underlie the perceptual changes observed are still not well understood and therefore remain a point of debate. Some positions remain less specified than others. For instance the proposal that listeners expand their categories when confronted with unfamiliar accents or that they "relax their criteria" for category membership (@zheng-samuel2020; @schmale2012; @floccia2006; @bent2016). While it is possible that apparent perceptual shifts post-exposure can be explained by processes independent of distributional learning [@clarke-davidson2008; see @xie2023 for simulations] what is needed are better specified hypotheses coupled with stronger predictions and tests to weigh the evidence [@schertz-clare2020; @xie2023; @bent-baese-berk2021].

Analytic frameworks that facilitate modelling of perceptual processes conditioned on different assumptions offer a way forward. If robust speech recognition involves learning from the input under varying contexts in a rational manner, it has to account for the implicit assumptions that listeners seem to bring to any speech perception task  with regard to cue-category mappings, and be able to explain how they reconcile these assumptions with recent input. Theories that explicitly bring this to bear include the influential exemplar models [@johnson1996; @pierrehumbert2001; @apfelbaum-mcmurray2015], Bayesian inference models [@feldman2009; @kronrod2016; @kleinschmidt-jaeger2015; @hitczenko-feldman2016], and error-driven learning [@harmon2019].

In a recent example @cummings-theodore2023 working within the ideal adaptor framework, predicted that perceptual recalibration could have graded effects. This logic follows from the general premise that adaptation is the outcome of weighted updates of listener prior expectations of cue-category mappings with the statistics of talker input. By manipulating the number of times an ambiguous sound between /s/ and /sh/ was heard between participants and within each biasing context (1, 4, 10 or 20 occurences) they showed that the size of the putative perceptual recalibration effect correlated with the frequency of the ambiguous tokens. Model simulations qualitatively predicted behavioral results and provided strong evidence of a mechanism that is sensitive to cue statistics. This result corroborates earlier modelling efforts of @kleinschmidt-jaeger2011 which demonstrated that incremental bayesian belief-updating is a possible mechanism behind what has been believed to be dichotomous perceptual phenomena -- selective adaptation and perceptual recalibration.

The present study was devised in similar spirit to past studies guided by an understanding of language as inference and learning under uncertain conditions [@fine2010; @kleinschmidt-jaeger2016; @kleinschmidt-jaeger2011; @clayards2008]. In particular we aim to subject the hypothesis that talker adaptation results from distributional learning with incremental belief updating to a stronger test. While studies of perceptual recalibration that demonstrate graded learning effects based on the quantity of evidence support this hypothesis, there are limitations to the paradigm that preclude deeper investigation. Talker-specific learning involves inferring the means and variances of her cue-category mappings. This task is made more difficult for talkers with extreme cue shifts that fall beyond the prior expectations of listeners because an entire remapping of the cue space is required [@sumner2011]. In perceptual recalibration listeners are presented with maximally informative instances of the same ambiguous acoustic-phonetic token essentially providing ideal but very unnatural circumstances for learning to occur. However even this has a limit -- exposure to a certain number of critical trials (about 20 trials in lexical context studies [@cummings-theodore2022; @tzeng2021]; 64 trials in audio-visual context studies[@vroomen2007]) -- do not bring additive learning effects.

Here we build on the pioneering work of @clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019; @kleinschmidt2020 with some design innovations that we believe affords a productive test of the core claims of an ideal adaptor account of speech perception. In @kleinschmidt-jaeger2016 L1-US English listeners heard recordings of /b/-/p/ minimal pair words like *beach* and *peach* that were acoustically manipulated. Separate groups of listeners were exposed to different distributions of voice onset times (VOTs)---the primary cue distinguishing word-initial voicing ---that were shifted by up to +30 ms, relative to what one might expect from a 'typical' talker (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)A). In line with the distributional learning hypothesis, listeners' category boundary or point of subjective equality (PSE)---i.e., the VOT for which listeners are equally likely to respond "b" or "p"---shifted in the same direction as the exposure distribution (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)B). @kleinschmidt-jaeger2016 and closely related work have been able to show perceptual shifts move qualitatively in the direction of the manipulated distributions but so far none of them were designed to test incremental adaptation. We propose to fill that gap with a novel test-exposure-test design. In doing so we aim to estimate listeners prior expectations about the category mappings for our test talker before they receive further informative exposure and to document how quickly, from the onset of exposure, does the distributional learning effect emerge. The latter point is something that remains opaque in previous work because of the lack of test blocks. Given the substantial evidence that adaptation is rapid (e.g. under 5 mins in L2 accent adaptation; 4-10 trials in perceptual recalibration) listeners may show learning effects very early on in distributional learning as well. On the other hand, given the comparatively more naturalistic task of inferring talker distributions over a range of cues, learning effects may take longer to show.   

In experimental work researchers often have to consider the generalizability of their results which leads to questions about ecological validity. There is a trade-off between ecological validity of the experimental design and the desired degree of control over the variables. Questions about ecological validity of prior work in distributional learning pertain to two features. First, the stimuli which were generated with a synthesiser, had an obvious machine-like quality[@kleinschmidt-jaeger2016; @clayards2008]. Second, the pairs of distributions of voiced and voiceless categories were always identical in their variances [see also @theodore-monto2019] which adds to the artificiality of the experiment. In our description of methods below we show how we can begin to improve on these features through the stimuli and the setting of exposure conditions.


## Previous intro



<!--
However, Kleinschmidt and Jaeger also observed a previously undocumented property of these adaptive changes: shifts in the exposure distribution had less than proportional (sublinear) effect on shifts in PSE (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)C). While this finding is broadly compatible with the hypothesis of distributional learning, it points to important not well-understood constraints on adaptive speech perception.-->


(ref:kleinschmidt-jaeger-2016-replotted) Design and results of @kleinschmidt-jaeger2016 replotted. **Panel A:** Different groups of participants were exposed to different shifts in the mean VOT of /b/ and /p/. **Panel B:** categorization functions fitted to the last 1/6th of all trials depending on the exposure condition (shift in VOT means of /b/ and /p/). For reference, the black dashed line shows the categorization function of the 0-shift condition. The colored dashed lines shows the categorization function expected for an ideal observer that has fully learned the exposure distributions. **Panel C:** Mean and 95% CI of participants' points of subjective equality (PSEs), relative to the PSE of the ideal observers.

```{r kleinschmidt-jaeger-2016-refitted}
# load K&J2016 data and filter to all semi-supervised rows
d.KJ16 <-
  supunsup_clean %>%
  filter(supCond == "mixed" | supCond == "supervised") %>%
  # rename variables so that they match the naming conventions employed in the remainder
  # of this paper.
  rename(
    ParticipantID = subject,
    Item.VOT = vot,
    Condition.Exposure = bvotCond,
    Response.Voiceless = respP,
    Item.MinimalPair = wordClass,
    category = respCategory) %>%
  mutate(
    Condition.Exposure = factor(case_when(
      Condition.Exposure == 0 ~ "+0ms",
      Condition.Exposure == 10 ~ "+10ms",
      Condition.Exposure == 20 ~ "+20ms",
      Condition.Exposure == 30 ~ "+30ms")),  
    Condition.Exposure = fct_relevel(
      Condition.Exposure, c("+0ms", "+10ms", "+20ms", "+30ms")))

d.KJ16_unlabeled <-
  d.KJ16 %>%
  arrange(trial) %>%
  filter(labeled == "unlabeled") %>%
  group_by(ParticipantID) %>%
  # filter to final 6th of total trials
  slice_tail(n = 37)

VOT.mean_d.KJ16_unlabeled <- mean(d.KJ16_unlabeled$Item.VOT)
VOT.sd_d.KJ16_unlabeled <- sd(d.KJ16_unlabeled$Item.VOT)
d.KJ16_unlabeled %<>% mutate(VOT_gs = (Item.VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled))

contrasts(d.KJ16_unlabeled$Condition.Exposure) <-
  cbind("10vs0" = c(-3/4, 1/4, 1/4, 1/4),
        "20vs10" = c(-1/2, -1/2, 1/2, 1/2),
        "30vs20" = c(-1/4, -1/4, -1/4, 3/4))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 1 + VOT_gs * Condition.Exposure +
        (1 + VOT_gs | ParticipantID) +
        (1 + VOT_gs * Condition.Exposure | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .99),
    file = "../models/KJ16-semisupervised-GLMM.rds")

# fit nested model to extract slopes and intercepts
fit_nested_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~  0 + (Condition.Exposure) / VOT_gs +
        (0 + VOT_gs | ParticipantID) +
        (0 + (Condition.Exposure) / VOT_gs | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .995),
    file = "../models/KJ16-semisupervised-nested-GLMM.rds")

cond_fit_KJ16 <- fit_KJ16 %>%
  conditional_effects(
  effects = "VOT_gs:Condition.Exposure",
  method = "posterior_epred",
  ndraws = 500,
  re_formula = NA) %>%
  .[[1]] %>%
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled))

d.KJ16_PSE <-
  fit_nested_KJ16 %>%
  gather_draws(`b_mu2_Condition.Exposure.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(
    term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept"),
    .variable = gsub("b_mu2_Condition.ExposureP(\\d{1,2}ms).*$", "\\1", .variable)) %>%
  pivot_wider(names_from = term, values_from = ".value") %>%
  rename(Condition.Exposure = .variable) %>%
  relocate(c(Condition.Exposure, Intercept, slope, .chain, .iteration, .draw)) %>%
  mutate(
    PSE = descale(-Intercept/slope, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled),
    Condition.Exposure = paste0("+", Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), mean = mean, upper = ~ quantile(.x, probs = .975))))
```


```{r io-categorization-kleinschmidt-jaeger-2016}
# Test points by condition
x <-
  d.KJ16_unlabeled %>%
  group_by(Condition.Exposure) %>%
  distinct(Item.VOT) %>%
  rename(x = Item.VOT)

# get io categorizations of the test points by condition
io.d.KJ16 <-
  make_MVG_ideal_observer_from_data(
  d.KJ16 %>%
    rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
  nest(io = -c(Condition.Exposure)) %>%
  left_join(x) %>%
  mutate(x = map(x, ~ c(.x))) %>%
  nest(x = x) %>%
  mutate(categorization = map2(
    x, io,
    ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>%
    mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>%
  unnest(cols = categorization, names_repair = "unique") %>%
  pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_b = round(`response_b` * 10^12), n_p = 10^12 - n_b) %>%
  group_by(Condition.Exposure) %>%
  nest(data = -c(Condition.Exposure)) %>%
  mutate(
    model_unscaled = map(
      data, ~ glm(
      cbind(n_p, n_b) ~ 1 + VOT,
      family = binomial,
      data = .x)),
    intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_unscaled = map_dbl(
      model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
    model_scaled = map(data, ~ glm(
      cbind(n_p, n_b) ~ 1 + I((VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled)),
      family = binomial,
      data = .x)),
    intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
    PSE = -intercept_unscaled/slope_unscaled)

# get io with lapse-accounted categorization
io.d.KJ16.lapse_rate <-
  make_MVG_ideal_observer_from_data(
    data = d.KJ16 %>%
      rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT")),
    lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = -c(Condition.Exposure)) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist()))) %>%
  unnest(categorization, names_repair = "unique")

# get io of typical talker and its categorization (+0 condition)
d.typical_talker <-
  io.d.KJ16[[2]][[1]][1, 1] %>%
  unnest(io) %>%
  mutate(lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = everything()) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist())),
    line = map(
      categorization,
      ~ geom_line(
          data = .x,
          mapping = aes(x = VOT, y = response),
          linetype = 2,
          linewidth = 0.6,
          alpha = .8,
          colour = "black")))
```




```{r kleinschmidt-jaeger-2016-replotted, fig.height=base.height*3.5, fig.width=base.width*5.5, fig.cap="(ref:kleinschmidt-jaeger-2016-replotted)"}
# make histograms of exposure distributions
p.KJ16.histogram <-
  d.KJ16 %>%
  group_by(Condition.Exposure) %>%
  slice_head(n = 222) %>%
  ggplot(aes(x = Item.VOT,
             fill = paste(Condition.Exposure, trueCat))) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous(breaks = c(0, 25, 50)) +
  scale_fill_manual(
    "Category",
    values = c("+0ms b" = "#f8766d",
               "+0ms p" = "#fdd1ce",
               "+10ms b" = "#7cae00",
               "+10ms p" = "#d4ff66",
               "+20ms b" = "#00bfc4",
               "+20ms p" = "#99fcff",
               "+30ms b" = "#c77cff",
               "+30ms p" = "#e9ccff"),
    aesthetics = "fill",
    labels = c("/b/", "/p/", "", "", "", "", "", "")) +
   guides(
    fill = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", NA, NA, NA, NA, NA, NA),
        values = c("b", "p", NA, NA, NA, NA, NA, NA)), nrow = 1)) +
  facet_wrap(~ Condition.Exposure, nrow = 1) +
  theme(legend.justification = "left") +
  remove_x_guides

p.KJ16.fit <-
  cond_fit_KJ16 %>%
  rename(Condition = Condition.Exposure) %>%
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    ymin = lower__, ymax = upper__, fill = Condition),
    alpha = .1,
    show.legend = F) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    color = Condition),
    linewidth = .7,
    alpha = 0.6,
    show.legend = F) +
  stat_summary(
    data = d.KJ16_unlabeled %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      group = Condition,
      colour = Condition),
    geom = "pointrange",
    size = 0.15,
    show.legend = F) +
  geom_line(
    data = io.d.KJ16.lapse_rate %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    mapping = aes(x = VOT, y = response, group = Condition, colour = Condition),
    linetype = 2,
    linewidth = 1,
    alpha = .6,
    inherit.aes = F,
    show.legend = F) +
  d.typical_talker$line +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Proportion \"p\"-responses", breaks = c(0, .5, 1)) +
  facet_wrap(~ Condition, nrow = 1)

p.KJ16.PSE <-
  d.KJ16_PSE %>%
  left_join(io.d.KJ16) %>%
  rename(PSE.io = PSE) %>%
  ggplot(aes(y = PSE_mean, x = Condition.Exposure, colour = Condition.Exposure)) +
  geom_hline(
    yintercept = c(20, 30, 40, 50),
    linewidth = 1.5,
    alpha = .4,
    linetype = 2,
    colour = scales::hue_pal()(4)) +
  geom_linerange(
    aes(ymin = PSE_lower, ymax = PSE_upper), size = 1, alpha = .8, show.legend = F) +
  geom_label(size = 4, show.legend = F, aes(label = paste(round((PSE_mean - 20) / (c(20, 30, 40, 50) - 20) * 100, 1), "%"))) +
  scale_x_discrete("Condition") +
  scale_y_continuous("PSE") +
  theme(axis.text.x = element_text(angle = 22.5, hjust = .8))

layout <- "
AAAA#
BBBBC"

p.KJ16.histogram + p.KJ16.fit + p.KJ16.PSE +
  plot_layout(design = layout,
              guides = "collect") +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(legend.position = "top",
        plot.tag = element_text(face = "bold"))
```




```{r remove-objects-unused-beyond-previous-chunk}
rm(d.KJ16, d.KJ16_unlabeled, d.KJ16_PSE, cond_fit_KJ16, fit_KJ16, fit_nested_KJ16, p.KJ16.PSE)
```

For example, influential models of adaptive speech perception predict proportional, rather than sublinear, shifts (for proof, see SI \@ref(sec:ibbu-proof)). This is the case both for incremental Bayesian belief-updating model [@kleinschmidt-jaeger2011] and general purpose normalization accounts [@mcmurray-jongman2011]---models that have been found to explain listeners' behavior well in experiments with less substantial changes in exposure. There are, however, proposals that can accommodate this finding. Some proposals distinguish between two types of mechanisms that might underlie representational changes, <!--- link representational change and distributional learning --> *model learning* and *model selection* [@xie2018, p. 229]. The former refers to the learning of a new category representations---for example, learning a new generative model for the talker [@kleinschmidt-jaeger2015, Part II] or storage of new talker-specific exemplars [@johnson1997; @sumner2011]. Xie and colleagues hypothesized that this process might be much slower than is often assumed in the literature, potentially requiring multiple days of exposure and memory consolidation during sleep [see also @fenn2013; @tamminen2012; @xie2018sleep]. Rapid adaptation that occurs within minutes of exposure might instead be achieved by selecting between *existing* talker-specific representations that were learned from previous speech input---e.g., previously learned talker-specific generative models [see mixture model in @kleinschmidt-jaeger2015, p. 180-181] or previously stored exemplars from other talkers [@johnson1997]. Model learning and model selection both offer explanations for the sublinear effects observed in @kleinschmidt-jaeger2016. But they suggest different predictions for the evolution of this effect over the course of exposure.

Under the hypothesis of model learning, sublinear shifts in PSEs can be explained by assuming a hierarchical prior over talker-specific generative models [$p(\Theta)$ in @kleinschmidt-jaeger2015, p. 180]. This prior would 'shrink' adaptation towards listeners' priors---similar to the effect of random by-subject or by-item effects in generalized linear mixed-effect models, which shrink group-level effect estimates towards the population mean of the data [@baayen2008]. Critically, as long as these priors attribute non-zero probability to even extreme shifts (e.g., the type of Gaussian prior used in mixed-effects models), this predicts listeners' PSEs will continue to change with increasing exposure until they have converged against the PSE that is ideal for the exposure statistics. In contrast, the hypothesis of model selection predicts that rapid adaptation is more strictly constrained by previous experience: listeners can only adapt their categorization functions up to a point that corresponds to (a mixture of) previously learned talker-specific generative models. This would imply that at least the earliest moments of adaptation are subject to a hard limit (Figure \@ref(fig:prediction)): exposure helps listeners to adapt their interpretation to more closely aligned with the statistics of the input, but only to a certain point.

(ref:prediction) Contrasting predictions of model learning and model selection hypotheses about the incremental effects of exposure on listeners' categorization function. Both hypothesis predict incremental adaptation towards the statistics of the input, as well as constraints on this adaptation. The two hypotheses differ, however, in that model selection predicts a hard limit on how far listeners' can adapt during initial encounters with an unfamiliar talker.

```{r prediction, fig.height=base.height+1/4, fig.width=base.width*2, fig.cap="(ref:prediction)"}
k <- 10^-1

crossing(
  Exposure = 0:100,
  Shift = c(20, 40),
  Hypothesis = c("model learning", "model selection")) %>%
  mutate(
    PSE = 25 + ifelse(Hypothesis == "model learning", Shift, pmin(Shift, 25)) *
      (1 - exp(-k * Exposure))) %>%
  ggplot(aes(x = Exposure, y = PSE, color = factor(Shift))) +
  geom_hline(
    data = crossing(Shift = c(0, 20, 40), Hypothesis = c("model learning", "model selection")),
    aes(yintercept = Shift + 25, color = factor(Shift)), linetype = 2) +
  geom_line(alpha = .5) +
  scale_y_continuous("PSE (in ms VOT)") +
  scale_color_manual(breaks = c("0", "20", "40"), values = c("gray", "green", "blue")) +
  facet_wrap(~ Hypothesis) +
  guides(color = "none") +
  theme(panel.grid = element_blank())
```

The present study employs a novel incremental exposure-test paradigm to address two questions. We test whether the sublinear effects of exposure observed in recent work replicate for exposure that (somewhat) more closely resembles the type of speech input listeners receive on a daily basis. And, we evaluate the predictions of the model learning and selection hypotheses against human perception. We take this question to be of interest beyond the specific hypotheses we contrast: whether there are hard limits to the benefits of exposure to unfamiliar speech patterns ultimately has consequences for education and medical treatment.

All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models (see SI, \@ref(sec:software)).
