```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Adaptivity is a hallmark of human speech perception, supporting faster and more accurate speech recognition. On first encounters with an unfamiliar accent, listeners might initially experience processing difficulty. This difficulty does, however, tend to alleviate with exposure, sometimes quite rapidly [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. As few as eighteen short sentences from a talker with an unfamiliar accent---even a comparatively strong non-native, second language accent---have been found to significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Findings like these suggests that speech perception can be remarkably adaptive, enabling listeners to flexibly adjust the mapping from the acoustic signal to phonetic categories and word meanings. While such adaptivity might seem obvious in hindsight, its discovery was a major breakthrough in the field of speech perception, and spurred the development of new paradigms and theories. Research over the last few decades has, for example, made substantial advances in identifying the conditions required for adaptation, its generalizability across talkers, and its longevity [for reviews, see @bent-baeseberk2021; @cummings-theodore2023; @schertz-clare2020; @zheng-samuel2023]. 

Despite this progress, the mechanisms underlying adaptive speech perception remain largely unknown. Even some of their most basic properties are as of yet unknown. We do not know *how* speech perception comes to incrementally change during the initial moments of listening to an unfamiliar talker. Does adaptation unfold gradually, or is it more or less spontaneous? Does it depend on listeners' prior expectations based on lifelong experiences, and if so, how are these expectations integrated with the incoming unfamiliar speech? How quickly does adaptation occur, and are there limits to listeners' ability to fully adapt to a new talker? Theories of speech perception make precise, quantifiable predictions about all of these questions that remain untested [for review, see @xie2023]. Perhaps the most developed of these theories are known as *distributional learning* models [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @assmann-nearey2007; @lancia-winter2013; @sohoglu-davis2016]. While these models differ in important aspects, they share the central assumption that listeners incrementally learn and store information the talker's speech. Specifically, listeners are assumed to learn statistical properties of the distribution of phonetic cues in the talker's speech, such as the average values of phonetic cues, their variability, or even the full cue distributions for each phonetic category. These learned statistical properties are then used to interpret subsequent speech input from the talker [for reviews, see @bent-baeseberk2021; @schertz-clare2020; @xie2023]. <!-- For example, some of the most parsimonious distributional learning accounts---normalization accounts---assume that listeners adapt their expectations for the phonetic cues the talker produces. Talkers with faster speech rates, for example, might have overall shorter durational cues. Normalization accounts propose that listeners learn, say, the average value that a talker produces for a cue, and interpret subsequent speech from the talker relative to this expectation [@assmann-nearey2007; @mcmurray-jongman2011]. Less parsimonious distributional learning accounts propose that listeners even learn the cue distributions that the talker produces for each category [@johnson1997; @kleinschmidt-jaeger2015]. --> 

As a consequence of this shared assumption, distributional learning models also share several critical predictions about adaptation. Listeners' categorizations are predicted to change incrementally with exposure, and the direction and magnitude of that change should gradiently depend on (1) listeners' prior expectations based on relevant previously experienced speech input from other talkers, and both (2a) the amount and (2b) distribution of phonetic cues in the exposure input from the unfamiliar talker [for review, see @xie2023]. In particular, listeners' categorization functions---the mapping from phonetic cues to phonetic categories---should gradually shift from a starting point that reflects listeners' prior expectations to a target that reflects the statistical properties of the new talker's speech. Additionally, some theories commit to specific learning mechanisms that further constrain how adaptation is expected towards accumulate: both error-driven learning theories [@harmon2019; @olejarczuk2018; @sohoglu-davis2016] and theories of ideal information integration [@kleinschmidt-jaeger2015; @kleinschmidt2020] predict that (3) adaptation initially proceeds quickly and then slows down as the listener approaches the correct mapping from the acoustic signal to phonetic categories. Finally, incremental adaptation is predicted to (4) proceed until the listener has fully learned the statistical properties of the new talker's speech, or until the listeners' expectations no longer deviate from the input they receive from the new talker. Figure \@ref(fig:predictions) illustrates these predictions and contrasts them with other possible scenarios.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally. The top row shows how listeners' categorization functions along a phonetic continuum change with increasing exposure, from the dashed starting point to the solid black target expected from an idealized learner that has fully learned the relevant statistics. The bottom row shows the same scenarios but as changes in the point of subjective equality (PSE), the point along the phonetic continuum at which listeners are equally likely to identify a sound as an instance of category "A" or "B". The highlighted column illustrates the predictions of models that assume error-based learning or ideal information integration. The rightmost column shows a scenario in which learners adapt but do not reach the correct mapping from the acoustic signal to phonetic categories. <!-- TO DO: linear to convergence, immediate convergence, gradiently diminishing returns to convergence, plateau prior to convergence. gray categorization functions, solid black learning target, dashes light gray prior; x-axis "phonetic continuum" from 0 to 85 (top) "Trial" (bottom), y-axis "Probability of responding category A" from 0 to 1 (top) "PSE" (bottom) -->

Compared to more informal or descriptive hypotheses such as "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @kraljic2006; @mcqueen2006; @mitterer2013; @norris2003; @reinisch-holt2014; @schmale2012; @vroomen-baart2009], distributional learning models thus make rather specific predictions. Yet, even some of the most foundational of these predictions remain largely untested.^[This tendency is hardly specific to research on speech perception. Seminal papers from the early days of cognitive science described the field as predominantly phenomenon-, rather than theory-driven, and insightfully discussed the consequences of this tendency [@newell1974; @platt1964]. Recent reviews have generally found this trend continued [@norris-cutler2021; @yarkoni-westfall2017].] This is in part because experiments on adaptive speech perception are not typically designed to test quantitative predictions about the effects of exposure. By far the most common designs expose one group of listeners to an unfamiliar speech pattern, and a second group of listeners to familiar speech patterns. Following exposure, both groups are tested on their ability to recognize instances of the unfamiliar speech pattern. Such designs are consistent with the goal of first generation studies to establish the *existence* of adaptive speech perception. They do, however, constitute rather weak tests for theories of adaptive speech perception [for demonstration, see @xie2023]. Additionally, the observed changes in listeners' perception are rarely ever analyzed relative to the quantitative predictions of existing theories. For example, there is now increasing evidence that changes in listeners' categorization functions depend *in some way* on the specific acoustic and phonetic properties of the speech input [e.g., @bertelson2003; @chladkova2017; @eisner-mcqueen2005; @kraljic-samuel2005; @kurumada2013; @norris2003; @reinisch-holt2014]. However, the extent to which these effects are consistent with the predictions of distributional learning models remains largely unexplored (we discuss notable exceptions below). Put simply, it is one thing to find that differences in inputs lead to differences in behavior; it is another thing to test whether the direction and magnitude of changes in behavior can be consistently predicted by existing models. Recent reviews of the field have thus called for the development of paradigms that provide 'richer' data that more strongly inform and constrain theories of adaptive speech perception [@bent-baeseberk2021; @schertz-clare2020; @xie2023]. 

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

The present study responds to this call for new paradigms, to address knowledge gaps identified above. We present a novel incremental exposure-test paradigm, and use it to test predictions (1)-(4) of distributional learning models. As we discuss below in more detail, the paradigm integrates, and builds on, computational and behavioral findings from separate lines of research on unsupervised distributional learning during speech perception [DL, @clayards2008; @colby2018; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [LGPL, @cummings-theodore2023; VGPL, @kleinschmidt-jaeger2012; @vroomen2007], and accent adaptation [AA, @hitczenko-feldman2016; @tan2021]. Figure \@ref(fig:block-design-figure) illustrates our approach. Between groups of participants, we manipulate the amount and distribution of phonetic cues in the exposure input. We focus on a phonetic contrast that is known to be subject to adaptive changes in perception [syllable-initial /d/-/t/ in US English, @kleinschmidt2015; @kraljic-samuel2006]. The three exposure distributions we use are shifted to different degrees both relative to each other, and relative to listeners' prior expectations. We measure listeners' categorization functions at multiple points during exposure, and test whether the direction and magnitude of the observed changes in behavior are consistent with the predictions of distributional learning models. To further guide the interpretation of results, we use normative models of adaptive speech perception [ideal observers and adaptors, @massaro1989; @feldman2009; @kleinschmidt-jaeger2015; @xie2023]. This enables predictions about---intentionally idealized---listeners and distributional learners, prior to considerations about memory or other cognitive limitations. Comparisons of participants' categorization functions against these normative models provides a principled and informative approach to identifying constraints on adaptive speech perception in human listeners.

To anticipate our results, we find that the changes in listeners' categorization behaviors largely follow the predictions of distributional learning models. In particular, we present the first evidence that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (predictions 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find that changes in rate of adaptation across exposure are consistent with the predictions of error-driven learning theories and theories of ideal information integration (prediction 3). We show that a Bayesian model of adaptation that is based on principles of ideal information integration [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses with very high accuracy ($R^2 = XXX$). However, not all observations we make fit the predictions of distributional learning models. In particular, we find little support for prediction 4. Rather, we find evidence for a previously unrecognized constraint on adaptive speech perception: changes in listeners' behavior seem to plateau long before listeners achieve the categorization functions and accuracy expected from an idealized learner that fully learns the talkers' phonetic distributions. We further find evidence that this constraint on adaptation is asymmetric, depending on the direction of the shift in the exposure input relative to listeners' prior expectations. We discuss the implications of our findings for theories of adaptive speech perception, and suggest how our paradigm can be used to test a wider range of predictions. Before we describe our study in more detail, we summarize the relevant literature in more detail.

## Previous work and how it motivates the present study
Previous work provides some support for predictions (2a) and (2b). For example, recent findings from LGPL and VGPL provide evidence in support of prediction (2a)---that the *amount* of phonetic evidence during exposure gradiently affects the magnitude of shifts in listeners' categorization boundary [@cummings-theodore2023; see also @liu-jaeger2018; @liu-jaeger2019]. In such paradigms, listeners are exposed to natural recordings of one phonetic category (e.g., /s/) and shifted instances of a second category that are manipulated to be perceptually more similar to the first category (e.g., /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/), mixed with many filler stimuli that do not contain either sound. Both the typical and the shifted sound instances are lexically or visually labeled by their context. For example, in an LGPL study, the lexical context will disambiguate the intended category of both the typical sounds (e.g., "dino*s*aur") and the shifted sounds (e.g., "medi*sh*ine"). Studies like this typically compare two groups of listeners that differ only in which of the two sounds was shifted. For example, one group of listeners might be exposed to 20 typical /s/ and 20 shifted /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/, mixed with 160 filler words. The other group of listeners might be exposed to 20 typical /`r linguisticsdown::cond_cmpl("ʃ")`/ and 20 shifted /`r linguisticsdown::cond_cmpl("ʃ")`/-like /s/, mixed with the same 160 filler words. Such exposure is known to reliably affect listeners' subsequent perception: following exposure, listeners will categorize more sounds along an unlabeled test continuum (e.g., "asi" to "ashi") as belonging to the category that was shifted during exposure [@kraljic-samuel2005; @kraljic-samuel2006]. 

In a particularly informative study, Cummings and Theodore compared shifts in categorization function between groups of listeners after exposure to 1, 4, 10, or 20 lexically labeled shifted /s/ or /`r linguisticsdown::cond_cmpl("ʃ")`/ tokens (each matched by an equal number of unshifted tokens from the opposite category). Shifts in listeners' categorization functions increased with the number of exposure to tokens, in line with prediction (2a) of distributional learning models. @vroomen2007 found similarly increasing shifts in categorization functions *within* participants, comparing the effects of 1, 2, 4, ..., 32 exposures to visually labeled shifted tokens [see also @kleinschmidt-jaeger2012]. LGPL/VGPL paradigms---at least as used traditionally---do, however, limit experimenters' control over the phonetic properties of the exposure stimuli: consistent with the goals of those studies, shifted sound instances are selected to be perceptually ambiguous between two categories, rather than to exhibit specific phonetic distributions. This limits the extent to which such paradigms can inform predictions (1) and (2b) about the effects of phonetic distributions in prior and recent experience. To the extent that LGPL/VGPL research has assessed the effects of phonetic properties, this has thus largely been limited to qualitative post-hoc analyses [@drouin2016; @kraljic-samuel2007; @tzeng2021; for quantitative tests, see @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. 

Support for prediction (2b) has thus primarily come from DL studies. In an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the *bees*-*peas* continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning how /b/ and /p/ are distributed along VOT, listeners in the wide variance group were predicted to exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found, providing support for prediction (2b) that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior in ways consistent with distributional learning theories [see also @nixon2016; @theodore-monto2019].

Other studies have exposed different groups of listeners to distributions of VOT that were shifted leftward or rightward along the VOT continuum---like the three exposure conditions in Figure \@ref(fig:block-design-figure) [@chladkova2017; @colby2018; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020]. For example, Kleinschmidt and Jaeger used the stimuli developed by Clayards to expose five different groups of listeners to VOT distributions for /b/ and /p/ that were shifted to different degrees. The five different exposure conditions were each shifted by 10msecs in VOT relative to the other, but held constant the distance between the /b/ and /p/ mean (always 40ms) and the variance of /b/ and /p/ (both always 8.3ms$^2$). All listeners were exposed to 222 trials of exposure input. Unlike the present study, Kleinschmidt and Jaeger did not include a pre-test or incremental intermittent testing. Instead, the effect of exposure was estimated by estimating listeners' categorization functions over the last third of the 222 trials. This revealed that listeners' categorization functions were shifted in the direction of the exposure input, and that the magnitude of this shift increased with the degree of exposure. The fact that all five exposure conditions elicited the predicted effect relative to each other constitutes particularly strong support consistent with prediction (2b) that the distribution of phonetic cues in the exposure input affects the magnitude of adaptive changes in listeners' categorization functions. 

Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], these findings suggests that the *outcome* of adaptation is qualitatively compatible with predictions (2a) and (2b) of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015].^[A related line of work has used distributional learning and training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mcclelland1999; @pajak-levy2012; @pisoni1982]. These studies, too, have observed learning behavior qualitatively compatible with distributional learning models [for review, see @pajak2016].] Previous studies have, however, relied on tests that averaged over, and/or followed, hundreds of exposure trials. This leaves open how adaptation incrementally unfolds throughout the earliest moments of exposure---i.e., whether listeners' categorization behavior indeed changes in the way predicted by models of adaptive speech perception, developing from expectations based on previously experienced phonetic distributions (prediction 1) to increasing integration of the phonetic distributions observed during exposure to the unfamiliar talker (predictions 2a,b). Without incremental testing, it is also difficult to assess whether incremental adaptation follows the predictions of distributional learning models that are based on principles of error-based learning or ideal information integration (prediction 3). Finally, the absence of incremental test makes it impossible to known whether potential constraints on changes in listeners' behavior reflect hard limits on adaptivity or simply the incremental learning outcome (prediction 4)---'how far the learner has gotten'---at the only point at which adaptation is assessed [for discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020].























## REST



The incremental exposure-test paradigm in Figure \@ref(fig:block-design-figure) begins to address these knowledge gaps. The experiment starts with a test block that assesses listeners' state prior to informative exposure---often assumed, but not tested, to be identical across exposure conditions. Additional intermittent tests---opaque to participants---then assess incremental changes up to the first 144 informative exposure trials. This lets us assess how the joint effect of exposure amount and exposure distribution---corresponding to predictions (2a) and (2b)---unfolds *incrementally*. And, by comparing the direction of adaptation not only across conditions, but also relative to the distribution of phonetic cues in listeners' prior experience, we begin to assess prediction (1). The use of physically identical test trials across both blocks and exposure conditions facilitates assumption-free comparison of cumulative exposure effects. As we detail under Methods, the use of incremental testing deviates from previous work [@clayards2008; @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019], and is not without challenges. 

We test predictions in a Bayesian mixed-effects psychometric model (a mixture model extension of generalized linear mixed-effect models). Such models are commonly used in research on psychophysics to correct for attentional lapses and responses biases [see @prins2019bayesian], but remain underutilized within research on speech perception. Here, we extend the psychometric model to analyze incremental changes in participants' categorization functions across blocks and exposure conditions. To further guide the interpretation of results, we use normative models of adaptive speech perception [ideal observers and adaptors, @massaro1989; @feldman2009; @kleinschmidt-jaeger2015; @xie2023]. This enables predictions about---intentionally idealized---listeners and distributional learners, prior to considerations about memory or other cognitive limitations. Comparisons of participants' categorization functions against these normative models provides a principled and informative approach to identifying constraints on adaptive speech perception in human listeners.

Finally, we took several modest steps towards addressing concerns about ecological validity that might limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distribution in the experiment [see discussion in @baese-berk2018]. For example, previous distributional learning studies have often used highly unnatural, 'robotic'-sounding, speech. Beyond raising questions about what types of expectations listeners apply to such speech, these stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. Similarly, LGPL/VGPL studies have often used perceptually ambiguous stimuli obtained by 'acoustic blending'---mixing recordings of two words (e.g., "sin" and "shin") at different relative intensity. This, too, can create acoustic properties that are rarely, if ever, observed in human speech (Rachel Theodore, p.c.).  We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.

## Open science
All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). Following @xie2023, both this article and its supplementary information (SI) are written in R Markdown. This allows other researchers to replicate and validate our analyses with the press of a button using freely available software [R, @R; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an annual undergraduate class at the University of Rochester (BCS206/207), in which students replicate and extend previous work in the cognitive sciences. The ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work [@kleinschmidt-jaeger2015; @tan2021; @xie2023]. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.


