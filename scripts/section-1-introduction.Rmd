```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Adaptivity is a hallmark of human speech perception, supporting faster and more accurate speech recognition. When exposed to an unfamiliar accent, the processing difficulty listeners might initially experience tends to alleviate with exposure [e.g., @bradlow2008; @bradlow2023; @clarke-garrett2004; @sidaras2009; @xie2018jasa; for review, see @baeseberk2018; @xie2021jep]. Research over the last few decades has made strides in identifying the conditions required for successful adaptation, its generalizability across talkers, and its longevity [for reviews, see @bent-baeseberk2021; @cummings-theodore2023; @zheng-samuel2020]. It is now clear that listeners' categorization function---the mapping from acoustic or phonetic inputs to linguistic categories and, ultimately, word meanings---changes based on the phonetic properties of recent input [e.g., @bertelson2003; @clayards2008; @cole2011; @eisner-mcqueen2005; @idemaru-holt2011; @kraljic-samuel2005; @kurumada2013; @mcmurray-jongman2011; @norris2003; @reinisch-holt2014; @xie2018jep; for review, @schertz-clare2020; @xie2023]. This has led to the development of stronger theories and models of adaptive speech perception that explicitly link the distribution of phonetic properties in recent speech input to changes in subsequent speech recognition [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assman2007; @winter-lancia2013; @sohoglu-davis2016; @xie2023]. 

As @cummings-theodore2023 point out, previous work has typically framed questions as an 'either-or'---adaptation is either observed or not---consistent with the focus on identifying the necessary conditions for adaptation and generalization. Recent reviews of the field instead emphasize the need to move towards stronger tests of existing theories, requiring the development of paradigms that support quantitative comparison and yield data that more strongly constrain the space of theoretical possibilities [@baeseberk2018; @schertz-clare2020; @xie2023]. This includes the need for data that characterize how adaptation develops *incrementally* as a function of both the *amount of exposure* and the *distribution of phonetic cues in the exposure input*. While existing theories differ in important aspects, they share critical predictions about incremental adaptation that have remained largely untested: listeners' categorizations are predicted to change incrementally with exposure, and the direction and magnitude of that change should gradiently depend on (1) listeners' prior expectations based on previously experienced speech input from other talkers, and both (2a) the amount and (2b) distribution of phonetic evidence in the exposure input from the unfamiliar talker [for review, see @xie2023]. We report initial results from a novel repeated exposure-test paradigm designed to test these predictions during the early moments of adaptation. 

Figure \@ref(fig:block-design-figure) illustrates our approach. The experiment builds on computational and behavioral findings from separate lines of research on unsupervised distributional learning during speech perception [DL, @clayards2008; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [LGPL, @cummings-theodore2023; VGPL, @kleinschmidt-jaeger2012; @vroomen2007], and accent adaptation [@hitczenko-feldman2016; @tan2021]. These paradigms and findings have complementing strengths that we seek to combine and extend. <!-- TO DO: consider briefly elaborating here? -->  Following previous work on distributional learning in speech perception, we expose different groups of listeners to phonetic distributions that are shifted to different degrees [@bejjanki2011; @clayards2008; @kleinschmidt2015; @munson2011; @nixon2016; @theodore-monto2019]. Unlike this work, we incrementally assess changes in listeners' categorization from pre-exposure onward.

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap='Exposure-test design of the experiment. Exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to word-initial /d/ and /t/ in English (e.g., "dip" vs. "tip"). Test blocks presented identical VOT stimuli within and across conditions.'}
knitr::include_graphics("../figures/block_design.png")
```

Following previous DL studies, we use of phonetically manipulated stimuli. This gives researchers control over the distribution of acoustic-phonetic properties that listeners experience during exposure and test (unlike AA, LGPL, and VGPL paradigms). This control is an important prerequisite for stronger tests of predictions (1) and (2a,b). For example, recent findings from LGPL and VGPL provide evidence in support of prediction (2a)---that the amount of phonetic evidence during exposure gradiently affects the magnitude of subsequent changes in listeners' categorization response [@cummings2023; see also @liu-jaeger2018; @liu-jaeger2019]. This includes some initial evidence that these changes accumulate incrementally [@kleinschmidt-jaeger2012; @vroomen2007], in ways consistent with models of adaptive speech perception. LGPL and VGPL paradigms---at least as used traditionally---do, however, limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to be perceptually ambiguous (e.g., between "s" and "sh"), not to exhibit specific phonetic distributions. To the extent that LGPL and VGPL research has assessed the effects of phonetic properties on the degree of boundary shift following exposure, this has been limited to qualitative post-hoc analyses [@drouin2016; @kaljic-samuel2007; @other-cummings?]. This makes it difficult to test predictions (1) and (2b) about the effects of phonetic distributions in prior and recent experience.

Support for prediction (2b) has thus primarily come from research in DL paradigms. In an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial /b/-/p/, /d/-/t/, /g/-/k/ in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the "bees"-"peas" continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning the VOT distributions of /b/ and /p/, listeners in the wide variance group were predicted to exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found [see also @nixon2016; @theodore-monto2019]. Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], this important finding suggests that the *outcome* of adaptation qualitatively follows the predictions of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015]. However, all findings in this line of work relied on tests that either averaged over, or followed, hundreds of trials of exposure. This leaves open how adaptation proceeds from the earliest moments of exposure---i.e., whether listeners categorization behavior indeed changes in the way predicted by models of adaptive speech perception, developing from expectations based on previously experienced phonetic distributions to increasing integration of the phonetic distributions observed during exposure to the unfamiliar talker. It also leaves open whether potential constraints on the extent to which listeners' behavior changes with exposure [for initial evidence and discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020] reflect hard limits on adaptivity or simply the incremental learning outcome---'how far the learner has gotten'---at the only point at which adaptation is assessed (i.e., following exposure).

The repeated exposure-test paradigm in Figure \@ref(fig:block-design-figure) aims to address this knowledge gap. The experiment starts with a test block that assesses listeners' state prior to informative exposure---often assumed, but not tested, to be identical across exposure conditions. Additional intermittent tests---opaque to participants---then assess incremental changes up to the first 144 informative exposure trials. By employing physically identical test trials both across block within exposure conditions and across exposure conditions, we aim to facilitate assumption-free comparison of cumulative exposure effects (we additionally also measure adaptation during exposure). As we detail under Methods, the use of repeated testing deviates from previous work [@clayards2008; @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019], and is not without challenges.


Finally, we took several modest steps towards addressing concerns about ecological validity that have been argued to limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distributions in the experiment [see discussion in @baseberk2018].<!-- TO DO: check --> For example, previous distributional learning studies have used highly unnatural, 'robotic'-sounding, speech [but see @theodore-monto2019]. Beyond raising questions about what types of expectations listeners apply to such speech, stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.

# NOTES
+ strong test of predictions about effects of exposure
  + linking back to exposure distributions in the input (unlike all other work except for KJ16)
  + control over phonetic distributions (unlike LGPL, VGPL, AA)  
  + can fit categorization function (unlike VGPL; DSL); phonetic distribution during test and the way we analyze our data
    + (identical test stimuli across and within subject: theory-free testing)
    + (Bayesian mixed-effects psychometric model to avoid bias in estimation of cat fun)    

+ strong test of incrementality (also highlighted in xie2023)
  + pre-exposure test (unlike most other work)
  + test short / early moments of exposure (unlike DL)
  + test incremental accumulation (unlike DL, LGPL, AA) --> amount of evidence

+ move towards increased ecolological validity
  + ecological validity of stimuli (unlike some of DL, VGPL)
  + ecological validity of distributions (unlike DL; unlike LGPL/VGPL not always maximally ambiguous tokens)

# BREAK

Perhaps the clearest evidence that adaptation to unfamiliar speech depends on the statistics of the input---specifically, the distribution of phonetic cues---comes from the former paradigm [@bejjanki2011; @clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019]. In an important early study, Clayards and colleagues exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial /b/-/p/, /d/-/t/, /g/-/k/ in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the "bees"-"peas" continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning the category statistics of the exposure input---in this case, the distribution of VOT for /b/ and /p/---they were predicted to change their categorization function along VOT such that listeners in the wide variance group should exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found [see also @nixon2016; @theodore-monto2019]. Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], this suggests that the *outcome* of adaptation qualitatively follows the predictions of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015]. 

It leaves open, however, how adaptation *incrementally accumulates* with increasing exposure, and whether it does so in line with predictions of distributional learning models. Initial evidence that speaks to this question comes from research on lexically- or visually-guided perceptual learning [@bertelson20023; @norris2003; @kraljic-samuel2005]. In these paradigms, listeners are exposed to phonetically manipulated instances of a sound category (e.g., making the "s" in "embassy" sound almost like an "sh"), mixed with many filler words without that sound. Following such exposure, listeners are known to shift their categorization function. For example, after being exposed to instances of "sh"-like "s" listeners categorize more tokens along the "s"-"sh" continuum as "s". Recent work within those paradigms has found that the magnitude of the boundary shift increases for listeners who are exposed to more instances of the shifted sound [up to a point, @cummings2023; @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @liu-jaeger2018; @liu-jaeger2019;  @liu-jaeger2018; @vroomen2007]. This suggest that adaptation accumulates with exposure, rather than being an all or nothing process [@cummings2023]. There are, however, important limitations to these findings. Perceptual recalibration paradigms, at least as used traditionally, limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to be perceptually ambiguous (e.g., between "s" and "sh"), not to exhibit specific phonetic distributions. To the extent that researchers have aimed to understand the consequences of phonetic properties on the degree of boundary shift following exposure, this has been limited to post-hoc analyses [@drouin2016; @kaljic-samuel2007; @other-cummings?]. It is thus an open question to what extent the boundary shifts observed in such experiments reflect not only the quantity, but also the distribution of phonetic properties, during exposure (as predicted by distributional learning models).

This motivates the present study. We modify the distributional learning paradigm of @clayards2008 to shed light on the cumulative effects of incremental adaptation. We expose participants to instances of "d" and "t", and manipulate the distribution of VOT between participants, while intermittently testing within- and across-participants how listeners' categorization functions change with exposure. The resulting repeated exposure-test design is shown in Figure \@ref(fig:block-design-figure). 

```{r block-design-figure-, fig.height=3, fig.width=5, fig.cap="Exposure-test design of the experiment. Test blocks presented identical stimuli within and across conditions"}
knitr::include_graphics("../figures/block_design.png")
```

The use of repeated testing deviates from previous work, and is not without challenges. Previous work has instead employed 'batch testing' designs, in which changes in categorization responses are assessed only after extended exposure to hundreds of trials or by averaging over similarly extended exposure [@clayards2008; @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt-jaeger2016; @kleinschmidt2020; @munson2011; @nixon2016; @theodore-monto2019]. By introducing intermittent testing we aim to assess how increasing exposure affects listeners' perception without making strong assumptions about the nature of these changes (such as assuming linearity, or penalizing non-linearity, of changes over trials). However, we cannot afford *extended* intermittent testing for three reasons. First, listeners' attention span is limited. Even prior to additional testing, typical distributional learning experiments span 200-400+ trials. Extending them further risks increasing attentional lapses and deteriorating data quality. Second, previous work has found that repeated testing over uniform test continua can reduce or undo the effects of informative exposure [@liu-jaeger2018; @liu-jaeger2019; @cummings-theodore2023].<!-- TO DO: Maryann, I don't think the 2023 paper shows that, right? is it the 2022 paper or an even more recent one by Shawn? --> Third, holding the distribution of test stimuli constant across exposure condition inevitably means that the relative unexpectedness of these test stimuli differs between the exposure conditions By keeping tests short relative exposure (12 vs. 48 trials), we aimed to minimize the influence of test trials on adaptation. The final three test blocks were intended to ameliorate the potential risks of this novel design: in case adaptation remains stable despite repeated testing, those additional test blocks were meant to provide additional statistical power to detect the effects of cumulative exposure.

We also made several additional adjustments to the paradigms used in previous work, meant to increase the ecologically validity of both stimuli and exposure distributions. This serves the longer-term goal of bridging the gap between research paradigms that afford control over phonetic properties at the cost of ecological validity, and paradigms that afford high ecological validity (e.g. adaptation to natural accents) at the cost of control. We describe the adjustments in more detail under Methods but briefly anticipate them here. The pioneering works we build on employed speech stimuli that were clearly identifiable as synthesized, sounding robotic, and did not exhibit natural correlations between phonetic cues  [@clayards2008; @kleinschmidt-jaeger2016]. We instead created natural sounding stimuli [building on @theodore-monto2019] that exhibited correlations between VOT and other cues to word-initial "d"-"t" that typical to everyday speech [@REF]. Previous work also *designed* rather than *sampled* exposure distributions. As a consequence, exposure distributions in these experiments were symmetrically balanced around the category means [see also @harmon2019; @idemaru-holt2011; @idemaru-holt2020; @vroomen2007; a.o.]---unlike in everyday speech input which constitutes heterogeneous *random samples* of the underlying phonetic distributions. Indeed, all previous studied we build on exposed listeners to categories with *identical* variances [e.g., identical variance along VOT for /b/ and /p/, @clayards2008; @kleinschmidt-jaeger2016; or /g/ and /k/, @theodore-monto2019]. This, too, is highly atypical for everyday speech input [@lisker-abramson1964]. We instead expose listeners to random samples of phonetic cues that exhibit natural asymmetries in category variance based on a phonetically annotated database of word-initial /d/ and /t/ in US English [@chodroff-wilson2018]. 









## Other notes 

The predominant paradigms in research on adaptive speech perception are, however, not well-suited to address this question. As @cummings-theodore2023 summarize, "most research [...] has focused on identifying the conditions that are necessary for adaptation to occur" and "consistent with [this goal], outcomes [...] are most often considered as a binary result---does any learning occur, or not?" As a consequence, much remains unknown about how exposure comes to affect perception. It is unclear, for example, whether adaptive changes accumulate depending on both the amount of speech input and its statistical properties in the way predicted by the most explicit theoretical frameworks [e.g., the ideal adaptor, @kleinschmidt-jaeger2015; C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]. 

Typical paradigms manipulate exposure between listeners, and then assess the effects of exposure on subsequent test stimuli that are identical for all groups [for review, see @baese-berk2018; @schertz-clare2020]. These types of paradigms have provided evidence that adaptation to an unfamiliar talker can be rapid. For example, a thought-provoking finding by @clarke2004 suggests that exposure to eighteen sentences from an L2-accented talker---less than two minutes of speech---can be sufficient to facilitate significantly faster processing of that speech. This finding has since been replicated and extended to show that equally short exposure can facilitate recognition that is both faster and more accurate [@xie2018; for related results, see also @bradlow2023; @xie2017; @xie2021]. Other work has traded the ecological validity of natural L2 accents against increased control over the phonetic properties of exposure and test stimuli---a critical step towards stronger tests, as competing hypotheses about the mechanisms underlying adaptive speech perception require strong linking hypotheses mapping the acoustic input onto listeners' responses [@martin2023; @xie2023]. One such paradigm is lexically- or visually-guided perceptual recalibration [@bertelson20023; @norris2003; @kraljic-samuel2005], in which listeners are exposed to phonetically manipulated instances of a sound (e.g., making the "s" in "embassy" sound almost like an "sh"), mixed with many filler words without that sound. Following such exposure, listeners are known to shift their categorization function, so as to categorize more tokens along the "s"-"sh" continuum as "s". Recent work within those paradigms has found that as little as four phonetically shifted instances of a sound category can be sufficient to significantly alter listeners' categorization boundary [@liu-jaeger2018; @liu-jaeger2019; @cummings2023; @vroomen2007]. The same studies have found that exposure seems to accumulate, leading to larger boundary shifts for listeners who were exposed to more instances of the shifted sound [up to a point, @liu-jaeger2018; @vroomen2007; see also @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. Findings like these suggest that even rapid adaptation can be cumulative, rather than being an all or nothing process.

There are, however, important limitations to what perceptual recalibration paradigms can tell us about incremental adaptation. As is typical for such paradigms, all of the above experiments exposed listeners to shifted pronunciations that were always lexically or visually labeled stimuli (e.g., embedding the "sh"-like "s" in the word "embassy", which effectively labels it as an "s"). Such labeling is known to facilitate adaptation [@burchill2018; @burchill2023]---indeed, if shifted pronunciations are embedded in minimal pair or nonce-word context, listeners do no longer shift their categorization boundary [@norris2003; @REF-theodore?]. In everyday speech perception, however, listeners often have uncertainty about the word they are hearing, and must either use contextual information to label the input or adapt from unlabeled input. Perceptual recalibration paradigms, at least as used traditionally, also limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to sound ambiguous between, e.g., "s" and "sh", not based on their phonetic properties. To the extent that researchers have aimed to understand the consequences of those phonetic properties on the degree of boundary shift following exposure, this has involved post-hoc analyses [@drouin2016; @other-cummings?]. It is thus an open question to what extent the boundary shifts observed in such experiments reflect not only the quantity, but also the distribution of phonetic properties, during exposure [as would be expected under, e.g., the ideal adaptor framework].

The present work thus employs a novel repeated-exposure-test paradigm that explicitly control the distribution of phonetic properties during exposure.  [clayards, bejjanki; kj16, k20; see also theodore-monto2019]






## Previous intro



(ref:kleinschmidt-jaeger-2016-replotted) Design and results of @kleinschmidt-jaeger2016 replotted. **Panel A:** Different groups of participants were exposed to different shifts in the mean VOT of /b/ and /p/. **Panel B:** categorization functions fitted to the last 1/6th of all trials depending on the exposure condition (shift in VOT means of /b/ and /p/). For reference, the black dashed line shows the categorization function of the 0-shift condition. The colored dashed lines shows the categorization function expected for an ideal observer that has fully learned the exposure distributions. **Panel C:** Mean and 95% CI of participants' points of subjective equality (PSEs), relative to the PSE of the ideal observers.

```{r kleinschmidt-jaeger-2016-refitted}
# load K&J2016 data and filter to all semi-supervised rows
d.KJ16 <-
  supunsup_clean %>%
  filter(supCond == "mixed" | supCond == "supervised") %>%
  # rename variables so that they match the naming conventions employed in the remainder
  # of this paper.
  rename(
    ParticipantID = subject,
    Item.VOT = vot,
    Condition.Exposure = bvotCond,
    Response.Voiceless = respP,
    Item.MinimalPair = wordClass,
    category = respCategory) %>%
  mutate(
    Condition.Exposure = factor(case_when(
      Condition.Exposure == 0 ~ "+0ms",
      Condition.Exposure == 10 ~ "+10ms",
      Condition.Exposure == 20 ~ "+20ms",
      Condition.Exposure == 30 ~ "+30ms")),  
    Condition.Exposure = fct_relevel(
      Condition.Exposure, c("+0ms", "+10ms", "+20ms", "+30ms")))

d.KJ16_unlabeled <-
  d.KJ16 %>%
  arrange(trial) %>%
  filter(labeled == "unlabeled") %>%
  group_by(ParticipantID) %>%
  # filter to final 6th of total trials
  slice_tail(n = 37)

VOT.mean_d.KJ16_unlabeled <- mean(d.KJ16_unlabeled$Item.VOT)
VOT.sd_d.KJ16_unlabeled <- sd(d.KJ16_unlabeled$Item.VOT)
d.KJ16_unlabeled %<>% mutate(VOT_gs = (Item.VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled))

contrasts(d.KJ16_unlabeled$Condition.Exposure) <-
  cbind("10vs0" = c(-3/4, 1/4, 1/4, 1/4),
        "20vs10" = c(-1/2, -1/2, 1/2, 1/2),
        "30vs20" = c(-1/4, -1/4, -1/4, 3/4))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 1 + VOT_gs * Condition.Exposure +
        (1 + VOT_gs | ParticipantID) +
        (1 + VOT_gs * Condition.Exposure | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .99),
    file = "../models/KJ16-semisupervised-GLMM.rds")

# fit nested model to extract slopes and intercepts
fit_nested_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~  0 + (Condition.Exposure) / VOT_gs +
        (0 + VOT_gs | ParticipantID) +
        (0 + (Condition.Exposure) / VOT_gs | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .995),
    file = "../models/KJ16-semisupervised-nested-GLMM.rds")

cond_fit_KJ16 <- fit_KJ16 %>%
  conditional_effects(
  effects = "VOT_gs:Condition.Exposure",
  method = "posterior_epred",
  ndraws = 500,
  re_formula = NA) %>%
  .[[1]] %>%
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled))

d.KJ16_PSE <-
  fit_nested_KJ16 %>%
  gather_draws(`b_mu2_Condition.Exposure.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(
    term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept"),
    .variable = gsub("b_mu2_Condition.ExposureP(\\d{1,2}ms).*$", "\\1", .variable)) %>%
  pivot_wider(names_from = term, values_from = ".value") %>%
  rename(Condition.Exposure = .variable) %>%
  relocate(c(Condition.Exposure, Intercept, slope, .chain, .iteration, .draw)) %>%
  mutate(
    PSE = descale(-Intercept/slope, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled),
    Condition.Exposure = paste0("+", Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), mean = mean, upper = ~ quantile(.x, probs = .975))))
```


```{r io-categorization-kleinschmidt-jaeger-2016}
# Test points by condition
x <-
  d.KJ16_unlabeled %>%
  group_by(Condition.Exposure) %>%
  distinct(Item.VOT) %>%
  rename(x = Item.VOT)

# get io categorizations of the test points by condition
io.d.KJ16 <-
  make_MVG_ideal_observer_from_data(
  d.KJ16 %>%
    rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
  nest(io = -c(Condition.Exposure)) %>%
  left_join(x) %>%
  mutate(x = map(x, ~ c(.x))) %>%
  nest(x = x) %>%
  mutate(categorization = map2(
    x, io,
    ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>%
    mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>%
  unnest(cols = categorization, names_repair = "unique") %>%
  pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_b = round(`response_b` * 10^12), n_p = 10^12 - n_b) %>%
  group_by(Condition.Exposure) %>%
  nest(data = -c(Condition.Exposure)) %>%
  mutate(
    model_unscaled = map(
      data, ~ glm(
      cbind(n_p, n_b) ~ 1 + VOT,
      family = binomial,
      data = .x)),
    intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_unscaled = map_dbl(
      model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
    model_scaled = map(data, ~ glm(
      cbind(n_p, n_b) ~ 1 + I((VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled)),
      family = binomial,
      data = .x)),
    intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
    PSE = -intercept_unscaled/slope_unscaled)

# get io with lapse-accounted categorization
io.d.KJ16.lapse_rate <-
  make_MVG_ideal_observer_from_data(
    data = d.KJ16 %>%
      rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT")),
    lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = -c(Condition.Exposure)) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist()))) %>%
  unnest(categorization, names_repair = "unique")

# get io of typical talker and its categorization (+0 condition)
d.typical_talker <-
  io.d.KJ16[[2]][[1]][1, 1] %>%
  unnest(io) %>%
  mutate(lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = everything()) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist())),
    line = map(
      categorization,
      ~ geom_line(
          data = .x,
          mapping = aes(x = VOT, y = response),
          linetype = 2,
          linewidth = 0.6,
          alpha = .8,
          colour = "black")))
```




```{r kleinschmidt-jaeger-2016-replotted, fig.height=base.height*3.5, fig.width=base.width*5.5, fig.cap="(ref:kleinschmidt-jaeger-2016-replotted)"}
# make histograms of exposure distributions
p.KJ16.histogram <-
  d.KJ16 %>%
  group_by(Condition.Exposure) %>%
  slice_head(n = 222) %>%
  ggplot(aes(x = Item.VOT,
             fill = paste(Condition.Exposure, trueCat))) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous(breaks = c(0, 25, 50)) +
  scale_fill_manual(
    "Category",
    values = c("+0ms b" = "#f8766d",
               "+0ms p" = "#fdd1ce",
               "+10ms b" = "#7cae00",
               "+10ms p" = "#d4ff66",
               "+20ms b" = "#00bfc4",
               "+20ms p" = "#99fcff",
               "+30ms b" = "#c77cff",
               "+30ms p" = "#e9ccff"),
    aesthetics = "fill",
    labels = c("/b/", "/p/", "", "", "", "", "", "")) +
   guides(
    fill = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", NA, NA, NA, NA, NA, NA),
        values = c("b", "p", NA, NA, NA, NA, NA, NA)), nrow = 1)) +
  facet_wrap(~ Condition.Exposure, nrow = 1) +
  theme(legend.justification = "left") +
  remove_x_guides

p.KJ16.fit <-
  cond_fit_KJ16 %>%
  rename(Condition = Condition.Exposure) %>%
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    ymin = lower__, ymax = upper__, fill = Condition),
    alpha = .1,
    show.legend = F) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    color = Condition),
    linewidth = .7,
    alpha = 0.6,
    show.legend = F) +
  stat_summary(
    data = d.KJ16_unlabeled %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      group = Condition,
      colour = Condition),
    geom = "pointrange",
    size = 0.15,
    show.legend = F) +
  geom_line(
    data = io.d.KJ16.lapse_rate %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    mapping = aes(x = VOT, y = response, group = Condition, colour = Condition),
    linetype = 2,
    linewidth = 1,
    alpha = .6,
    inherit.aes = F,
    show.legend = F) +
  d.typical_talker$line +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Proportion \"p\"-responses", breaks = c(0, .5, 1)) +
  facet_wrap(~ Condition, nrow = 1)

p.KJ16.PSE <-
  d.KJ16_PSE %>%
  left_join(io.d.KJ16) %>%
  rename(PSE.io = PSE) %>%
  ggplot(aes(y = PSE_mean, x = Condition.Exposure, colour = Condition.Exposure)) +
  geom_hline(
    yintercept = c(20, 30, 40, 50),
    linewidth = 1.5,
    alpha = .4,
    linetype = 2,
    colour = scales::hue_pal()(4)) +
  geom_linerange(
    aes(ymin = PSE_lower, ymax = PSE_upper), size = 1, alpha = .8, show.legend = F) +
  geom_label(size = 4, show.legend = F, aes(label = paste(round((PSE_mean - 20) / (c(20, 30, 40, 50) - 20) * 100, 1), "%"))) +
  scale_x_discrete("Condition") +
  scale_y_continuous("PSE") +
  theme(axis.text.x = element_text(angle = 22.5, hjust = .8))

layout <- "
AAAA#
BBBBC"

p.KJ16.histogram + p.KJ16.fit + p.KJ16.PSE +
  plot_layout(design = layout,
              guides = "collect") +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(legend.position = "top",
        plot.tag = element_text(face = "bold"))
```




```{r remove-unused-objects-section1}
rm(d.KJ16, d.KJ16_unlabeled, d.KJ16_PSE, cond_fit_KJ16, fit_KJ16, fit_nested_KJ16, p.KJ16.histogram, p.KJ16.fit, p.KJ16.PSE)
```

For example, influential models of adaptive speech perception predict proportional, rather than sublinear, shifts (for proof, see SI \@ref(sec:ibbu-proof)). This is the case both for incremental Bayesian belief-updating model [@kleinschmidt-jaeger2011] and general purpose normalization accounts [@mcmurray-jongman2011]---models that have been found to explain listeners' behavior well in experiments with less substantial changes in exposure. There are, however, proposals that can accommodate this finding. Some proposals distinguish between two types of mechanisms that might underlie representational changes, <!--- link representational change and distributional learning --> *model learning* and *model selection* [@xie2018, p. 229]. The former refers to the learning of a new category representations---for example, learning a new generative model for the talker [@kleinschmidt-jaeger2015, Part II] or storage of new talker-specific exemplars [@johnson1997; @sumner2011]. Xie and colleagues hypothesized that this process might be much slower than is often assumed in the literature, potentially requiring multiple days of exposure and memory consolidation during sleep [see also @fenn2013; @tamminen2012; @xie2018sleep]. Rapid adaptation that occurs within minutes of exposure might instead be achieved by selecting between *existing* talker-specific representations that were learned from previous speech input---e.g., previously learned talker-specific generative models [see mixture model in @kleinschmidt-jaeger2015, p. 180-181] or previously stored exemplars from other talkers [@johnson1997]. Model learning and model selection both offer explanations for the sublinear effects observed in @kleinschmidt-jaeger2016. But they suggest different predictions for the evolution of this effect over the course of exposure.

Under the hypothesis of model learning, sublinear shifts in PSEs can be explained by assuming a hierarchical prior over talker-specific generative models [$p(\Theta)$ in @kleinschmidt-jaeger2015, p. 180]. This prior would 'shrink' adaptation towards listeners' priors---similar to the effect of random by-subject or by-item effects in generalized linear mixed-effect models, which shrink group-level effect estimates towards the population mean of the data [@baayen2008]. Critically, as long as these priors attribute non-zero probability to even extreme shifts (e.g., the type of Gaussian prior used in mixed-effects models), this predicts listeners' PSEs will continue to change with increasing exposure until they have converged against the PSE that is ideal for the exposure statistics. In contrast, the hypothesis of model selection predicts that rapid adaptation is more strictly constrained by previous experience: listeners can only adapt their categorization functions up to a point that corresponds to (a mixture of) previously learned talker-specific generative models. This would imply that at least the earliest moments of adaptation are subject to a hard limit (Figure \@ref(fig:prediction)): exposure helps listeners to adapt their interpretation to more closely aligned with the statistics of the input, but only to a certain point.

(ref:prediction) Contrasting predictions of model learning and model selection hypotheses about the incremental effects of exposure on listeners' categorization function. Both hypothesis predict incremental adaptation towards the statistics of the input, as well as constraints on this adaptation. The two hypotheses differ, however, in that model selection predicts a hard limit on how far listeners' can adapt during initial encounters with an unfamiliar talker.

```{r prediction, fig.height=base.height+1/4, fig.width=base.width*2, fig.cap="(ref:prediction)"}
k <- 10^-1

crossing(
  Exposure = 0:100,
  Shift = c(20, 40),
  Hypothesis = c("model learning", "model selection")) %>%
  mutate(
    PSE = 25 + ifelse(Hypothesis == "model learning", Shift, pmin(Shift, 25)) *
      (1 - exp(-k * Exposure))) %>%
  ggplot(aes(x = Exposure, y = PSE, color = factor(Shift))) +
  geom_hline(
    data = crossing(Shift = c(0, 20, 40), Hypothesis = c("model learning", "model selection")),
    aes(yintercept = Shift + 25, color = factor(Shift)), linetype = 2) +
  geom_line(alpha = .5) +
  scale_y_continuous("PSE (in ms VOT)") +
  scale_color_manual(breaks = c("0", "20", "40"), values = c("gray", "green", "blue")) +
  facet_wrap(~ Hypothesis) +
  guides(color = "none") +
  theme(panel.grid = element_blank())
```

The present study employs a novel incremental exposure-test paradigm to address two questions. We test whether the sublinear effects of exposure observed in recent work replicate for exposure that (somewhat) more closely resembles the type of speech input listeners receive on a daily basis. And, we evaluate the predictions of the model learning and selection hypotheses against human perception. We take this question to be of interest beyond the specific hypotheses we contrast: whether there are hard limits to the benefits of exposure to unfamiliar speech patterns ultimately has consequences for education and medical treatment.

Finally, we took several modest steps towards addressing concerns about ecological validity that have been argued to limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distributions in the experiment [see discussion in @baseberk2018].<!-- TO DO: check --> For example, previous distributional learning studies have used highly unnatural, 'robotic'-sounding, speech [but see @theodore-monto2019]. Beyond raising questions about what types of expectations listeners apply to such speech, such stimuli also fail to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.


All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models (see SI, \@ref(sec:software)).
