```{r setup, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Introduction
Human speech perception is a remarkable feat. Successful speech recognition requires that listeners map the acoustic signal onto words and meanings. But this signal-to-meaning mapping varies across talkers and context. The same word spoken by different talkers can sound quite different; and conversely, the same acoustic signal can imply different words depending on the talker. Yet, listeners typically recognize speech quickly and accurately across a wide range of talkers and acoustic conditions (after decades of advances, automatic speech recognition is just beginning to approach the recognition accuracy that most listeners display during everyday speech perception).

Research has identified *adaptivity* as a key component to the robustness of human speech perception. Although first encounters with an unfamiliar accent can cause initial processing difficulty, this difficulty diminishes with exposure, sometimes rapidly [e.g., @bradlow-bent2008; @bradlow2023; @sidaras2009; @xie2021jep]. Eighteen short sentences from a talker with an unfamiliar accent---even a moderately strong second language accent---have been found to significantly improve subsequent perception of that talker's speech [@clarke-garrett2004; @xie2018]. Findings like these suggest that speech perception can be highly malleable, allowing listeners to adjust the mapping from acoustics to phonetic categories and word meanings. While such adaptivity may seem obvious in hindsight, its discovery was a major breakthrough in the field of speech perception, spurring the development of new paradigms and theories [for reviews, see @bent-baeseberk2021; @cummings-theodore2023; @schertz-clare2020; @zheng-samuel2023]. 

We thus know *that* listeners adapt to unfamiliar talkers. What remains unclear, however, is *how* adaptation is achieved. How do listeners integrate information from a new talker, and how does this come to incrementally change their interpretation of that talker's speech? Research on adaptive speech perception tends to discuss informal---often descriptive, rather than explanatory---hypotheses [see also @norris-cutler2021].<!-- TO DO: revisit after initial submission: ^[This tendency is hardly specific to speech perception research. Seminal works from the early days of the cognitive sciences describe the field as often phenomenon-driven rather than theory-driven, and insightfully discussed the consequences of this tendency [@newell1973; @platt1964]. Recent reviews have found this trend continued [@yarkoni-westfall2017].] --> This includes references to "boundary re-tuning/shift", "perceptual/phonetic recalibration/retuning", "category shift/expansion" or similar ideas [e.g., @mcqueen2006; @mitterer2013; @reinisch-holt2014; @schmale2012; @vroomen-baart2009; @xie2017; @zheng-samuel2020]. Such hypotheses do not specify what mechanisms support adaptive speech perception, nor do they make predictions about how adaptation unfolds incrementally with each new observation from an unfamiliar talker. 

Viewed from this perspective, research on adaptive speech perception can appear to be an open-ended list of empirical questions. Is adaptation more or less immediate, or does it unfold gradually? If the latter, do changes in listeners' behavior accumulate additively, leading to more or less linear changes in behavior? If not a linear development, is adaptation first slow and then fast, or first fast and then slow? And, how do differences in listeners' prior experience affect how listeners adapt? Are there limits to listeners' ability to fully adapt to a new talker? Or can we adapt to more or less any accent provided sufficient input?

Critically, some existing theories do make clear, quantifiable predictions about all of these questions---including many basic predictions that remain largely untested. One of the most developed of these theories are *distributional learning* models [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @nearey-assmann2007; @lancia-winter2013; @sohoglu-davis2016]. These models make untested predictions about how adaptation unfolds incrementally during the initial moments of listening to an unfamiliar talker. While distributional learning models differ from each other in important aspects, they share the central assumption that listeners incrementally learn and store information about talkers' speech. This includes information about the phonetic distributions that characterize the talker's speech, such as the average values of phonetic cues, their variability, or even the full phonetic distributions of all speech categories. These statistical properties are then used to interpret subsequent speech from the talker, supporting robust speech recognition across talkers [for reviews, see @bent-baeseberk2021; @schertz-clare2020; @xie2023]. <!-- For example, some of the most parsimonious distributional learning accounts---normalization accounts---assume that listeners adapt their expectations for the phonetic cues the talker produces. Talkers with faster speech rates, for example, might have overall shorter durational cues. Normalization accounts propose that listeners learn, say, the average value that a talker produces for a cue, and interpret subsequent speech from the talker relative to this expectation [@nearey-assmann2007; @mcmurray-jongman2011]. Less parsimonious distributional learning accounts propose that listeners even learn the cue distributions that the talker produces for each category [@johnson1997; @kleinschmidt-jaeger2015]. --> 

With this shared assumption, distributional learning models also share several critical predictions. Consider a listener's initial encounter with an unfamiliar talker who produces some sounds in an unexpected way (Figure \@ref(fig:predictions)A). Listeners' perception is predicted to change incrementally with exposure (Figure \@ref(fig:predictions)B-C). Distributional learning models make four predictions about how these changes unfold incrementally. First, the direction and magnitude of that change should gradiently depend on listeners' prior expectations based on relevant previously experienced speech input from other talkers (**prediction 1 - *prior expectations***), and both the amount (**prediction 2a - *exposure amount***) and distribution of phonetic cues in the exposure input from the unfamiliar talker [**prediction 2b - *exposure distribution* **, for review, see @xie2023]. Specifically, listeners' categorization functions---the mapping from acoustics to phonetic categories and words---should gradually shift from a starting point that reflects the statistics of previously experienced speech towards a target that reflects the statistics of the new talker's speech. Existing models further predict that this shift proceeds until the listener has fully learned the statistics of the new talker's speech (**prediction 3 - *learn to convergence***). Finally, some distributional learning models further commit to specific learning mechanisms that constrain how exactly adaptation is expected to accumulate incrementally: both error-driven theories [@harmon2019; @olejarczuk2018; @sohoglu-davis2016] and theories of ideal information integration [@kleinschmidt-jaeger2015; @kleinschmidt2020] predict that adaptation initially proceeds quickly and then slows down as the listener approaches the correct mapping from the acoustic signal to phonetic categories (**prediction 4 - *diminishing returns***).^[Predictions (1)-(4) assume that listeners *know* that they are listening to the same new talker. Talker recognition is itself an active inference process that we do not further discuss here [but see @kleinschmidt-jaeger2015; @magnuson-nusbaum2007].] Figure \@ref(fig:predictions)D illustrates these predictions and contrasts them with other possible scenarios.

(ref:predictions) Some hypothetical ways in which adaptive changes in listeners perception might unfold incrementally, using the pronunciation of US English word-initial /d/ and /t/ as an example (as in "dip" vs. "tip"). **Panel A:** Transparent lines indicate cross-talker variability in the realization of /d/ and /t/ along the primary cue used to distinguish them (voice onset timing or VOT). Shown are 20 random talkers from a database of connected speech [@chodroff-wilson2018]. The thicker solid lines indicate a 'typical' talker (averaging over all talkers in the database). Dashed lines indicate a hypothetical unfamiliar talker with a noticeably different distribution of VOT values. **Panel B:** Ideal categorization functions along the phonetic VOT continuum for speech from a typical talker (*idealized pre-exposure listener*, SI \@ref(sec:idealized-prior-listeners)) and speech from the unfamiliar talker (*idealized learner* that has fully learned that talkers distributions, SI \@ref(sec:idealized-learners)). Grey arrows point to the points of subjective equality (PSE), the point along the phonetic VOT continuum at which listeners are equally likely to identify a sound as an instance of /d/ or /t/. **Panel C:** The same as in Panel B but just showing the PSE. **Panel D:** Different ways in which listeners' PSEs along the phonetic VOT continuum might incrementally change with increasing exposure to the unfamiliar talker (from more transparent to less transparent). The horizontal lines indicates the ideal PSEs from Panel C. 

```{r predictions, fig.height=base.height*3+2/3, fig.width=base.width*5, fig.cap="(ref:predictions)", fig.pos="H"}
# Create all talker-specific IOs for connected speech data, using only the VOT cue
d.talker_IO.VOT <- 
  make_IOs_from_data (
    data = d.chodroff_wilson.connected,
    cues = c("VOT"),
    groups = "Talker") 

# Add x, PSE, categorization, and get gaussian geoms
d.talker_IO.VOT %<>%
  nest(io = -Talker) %>%
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  unnest(io) %>%
  add_gaussians_as_geoms_to_io(alpha = .2, linetype = 1, linewidth = .5) %>%
  bind_rows(
    # Do the same after aggregating all talker-specific IOs into a single 'typical' talker IO
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(Talker = "typical") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = 1, linewidth = 1.2),
    # Do the same after aggregating all talker-specific IOs into a single talker that is shifted by 35ms relative to the 'typical' talker
    aggregate_models_by_group_structure(d.talker_IO.VOT, group_structure = "Talker") %>%
      mutate(
        mu = map(mu, ~ .x + 35),
        Talker = "unfamiliar") %>%
      nest(io = -Talker) %>%
      add_x_to_IO() %>%
      add_PSE_and_categorization_to_IO() %>%
      unnest(io) %>%
      add_gaussians_as_geoms_to_io(alpha = 1, linetype = "longdash", linewidth = 1.2))

p.gaussian <- 
  ggplot() +
  (d.talker_IO.VOT %>% 
     filter(
       !(Talker %in% c("typical", "unfamiliar")),
       Talker %in% sample(unique(Talker), 20)) %>% 
     pull(gaussian)) + 
  (d.talker_IO.VOT %>% 
     filter(Talker == "typical") %>% .$gaussian) +
  scale_colour_manual(values = c(colours.category_greyscale)) +
  guides(colour = "none") +
  new_scale_colour() +
  (d.talker_IO.VOT %>% 
     filter(Talker == "unfamiliar") %>% .$gaussian) +
  scale_colour_manual("Category", values = c("#02427e", "#b4dafe")) +
  guides(color = guide_legend(override.aes = list(size = 0.5, colour = c(colours.category_greyscale), values = c("/d/", "/t/")))) +
  labs(x = "VOT (ms)", y = "Density") +
  theme(
    legend.background = element_rect(fill='transparent'),
    legend.box.background = element_rect(fill='transparent'),
    legend.key = element_rect(fill = "transparent"),
    legend.key.size = unit(0.5, "cm"),
    legend.key.spacing = unit(0.05, "cm"),
    legend.key.height = unit(0.01, "cm"),
    legend.title = element_text(size = 7), 
    legend.text = element_text(size = 6),
    legend.position = "inside",
    legend.position.inside = c(0.79,0.8))

arrow_PSE <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>%
  mutate(x = PSE, xend = PSE, y = .5, yend = .01)

p.cat <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>% 
  select(Talker, category, categorization, PSE) %>% 
  filter(category == "/t/") %>% 
  unnest(categorization, names_repair = "unique") %>% 
  ggplot(aes(x = VOT, y = response, group = Talker, colour = Talker, linetype = Talker)) +
  geom_line(linewidth = 1.2) +
  scale_colour_manual(values = c("grey", "#02427e")) +
  scale_linetype_manual(values = c("solid", "longdash")) +
  #scale_x_continuous("VOT (ms)", limits = c(-25, 130), breaks = c(0, 37, 72, 100)) +
  scale_y_continuous('Proportion "t"-responses', breaks = c(0, .5, 1)) +
  guides(linetype = "none", colour = "none") +
   geom_segment(
    data = arrow_PSE,
    mapping = aes(x = x, xend = xend, y = y , yend = yend),
    colour = "grey",
    alpha = 0.5,
    arrow = arrow(type = "open" , length = unit(0.04, "npc")),
    inherit.aes = F) +
  scale_x_continuous(limits = c(15, 90), breaks = c(37, 72))


p.PSE <- 
  d.talker_IO.VOT %>% 
  filter(Talker %in% c("typical", "unfamiliar")) %>%
  select(Talker, category, PSE) %>% 
  group_by(Talker) %>% 
  filter(category == "/t/") %>% 
  ggplot(aes(x = Talker, y = PSE, colour = Talker)) +
  geom_point(size = 1.5) +
  scale_colour_manual(values = c("grey", "#02427e"), guide = "none") + 
  scale_x_discrete("Talker") +
  scale_y_continuous("Ideal PSE (ms VOT)", limits = c(30, 75), breaks = c(37, 72))
 
PSE_change <- 
  expand_grid(
    PSE = c(),
    learning_pattern = factor(c("immediate", "linear", "diminished", "premature")),
    exposure_amount = factor(c("none", "1/3", "2/3", "complete"))) %>%
  mutate(
    learning_pattern = fct_relevel(learning_pattern, "immediate", "linear", "diminished", "premature"),
    exposure_amount = fct_relevel(exposure_amount, "none", "1/3", "2/3", "complete"),
    PSE = case_when(
      exposure_amount == "none" ~ 37,
      exposure_amount == "complete" & learning_pattern %in% c("immediate", "linear") ~ 72,
      exposure_amount %in% c("1/3", "2/3") & learning_pattern == "immediate" ~ 72,
      exposure_amount == "1/3" & learning_pattern == "linear" ~ 37 + ((72 - 37 + 1)/3),
      exposure_amount == "2/3" & learning_pattern == "linear" ~ 37 + 2 * ((72 - 37 + 1)/3),
      exposure_amount == "1/3" & learning_pattern == "diminished" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount == "2/3" & learning_pattern == "diminished" ~ 37 + 1.6 * ((72 - 37 + 1)/2),
      exposure_amount == "complete" & learning_pattern == "diminished" ~ 37 + 1.85 * ((72 - 37 + 1)/2) ,
      exposure_amount == "1/3" & learning_pattern == "premature" ~ 37 + ((72 - 37 + 1)/2),
      exposure_amount %in% c("2/3", "complete") & learning_pattern == "premature" ~ 37 + 25))

p.PSE_change <- 
  PSE_change %>% 
  ggplot(aes(exposure_amount, PSE, alpha = exposure_amount, group = 1), colour = "#02427e") +
  geom_rect(
    data = PSE_change  %>% 
      nest(data = c(exposure_amount, PSE)) %>% 
      group_by(learning_pattern) %>% 
      slice_sample(n = 1) %>% 
      select(learning_pattern) %>% 
      mutate(ymin = ifelse(learning_pattern == "diminished", -Inf, NA),
             ymax = ifelse(learning_pattern == "diminished", Inf, NA)),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "yellow",
    alpha = .15,
    inherit.aes = F) +
  geom_point(size = 1.8, colour = "#02427e") +
  geom_line(alpha = .3) +
  geom_hline(aes(yintercept = 37), linetype = 1, alpha = .8, colour = "grey") +
  geom_hline(aes(yintercept = 72), linetype = 2, alpha = .6, colour = "#02427e") +
  scale_x_discrete("Exposure amount", breaks = c("none", "complete"), labels = c("none", "full")) +
  scale_y_continuous("PSE (ms VOT)", limits = c(25, 75), breaks = c(30, 40, 50, 60, 70)) +
  guides(alpha = "none") +
  facet_wrap(~learning_pattern, nrow = 1, labeller = labeller(learning_pattern = c("immediate" = "immediate", "linear" = "linear", "diminished" = "diminishing \nreturns", "premature" = "premature \nconvergence")))


((p.gaussian | p.cat | p.PSE) / p.PSE_change) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

Although predictions (1)-(4) are specific and testable, they remain largely untested. This is in part due to limitations of the designs and paradigms used in research on adaptive speech perception. The most common paradigms expose one group of listeners to one speech pattern (e.g., rightward shifted VOT distributions as in Figure \@ref(fig:predictions)A), and a second group of listeners to another speech pattern (e.g., leftward shifted VOT distributions). Following exposure, both groups are tested on their ability to recognize one of the two speech patterns. Such designs were effective in establishing the *existence* of adaptive speech perception [see also @cummings-theodore2023]. They do, however, offer only weak tests of existing theories [for demonstration, see @xie2023].  Put simply, it is one thing to show that differences in exposure lead to differences in behavior; it is another thing to test whether the direction and magnitude of changes in behavior can be consistently explained by existing theories. 

Recent reviews have thus called for the development of paradigms that can more strongly constrain theories of adaptive speech perception [@bent-baeseberk2021; @schertz-clare2020; @xie2023]. Based on computational simulations, Xie and colleagues argue that strong tests require (a) information about the distribution of phonetic cues in both listeners' prior experience and during exposure, (b) paradigms that measure incremental changes in listeners' behavior both within and across exposure conditions, and (c) analyses that link the latter to the former. The present study responds to this call. We present a novel incremental exposure-test paradigm, and use it to test predictions (1)-(4), repeated in Table \@ref(tab:predictions). 

```{r block-design-figure, fig.height=3, fig.width=5, fig.cap="Incremental exposure-test design of our experiment. The three exposure conditions (rows) differed in the distribution of voice onset time (VOT), the primary phonetic cue to syllable-initial /d/ and /t/ in English (e.g., \"dip\" vs. \"tip\"). Test blocks assessed L1-US English listeners' categorization functions over VOT stimuli that were held identical within and across conditions.", fig.pos="H"}
knitr::include_graphics("../figures/block_design.png")
```

Figure \@ref(fig:block-design-figure) illustrates our approach. Between groups of participants, we manipulate the amount and distribution of phonetic cues in the exposure input. <!-- We focus on a phonetic contrast that is known to be subject to adaptive changes in perception [syllable-initial /d/-/t/ in US English, @kleinschmidt2015; @kraljic-samuel2006]. --> The three exposure distributions we use are shifted to different degrees both relative to each other, and relative to listeners' prior expectations. This allows us to test predictions (1) and (2a,b) that direction and magnitude of that change should gradiently depend on how and how much the current talker's speech deviates from the listeners’ prior expectations. We measure listeners' categorization functions at multiple points during exposure, and determine whether the direction and magnitude of the observed changes in behavior are consistent with the predictions of distributional learning models, including prediction (4) about the diminishing rate of changes in listeners' behavior. To further guide the interpretation of results, we use normative models of adaptive speech perception [ideal observers and adaptors, @feldman2009; @kleinschmidt-jaeger2015; @massaro1989; @xie2023]. This enables predictions about---intentionally idealized---listeners and distributional learners, prior to considerations about memory or other cognitive limitations. Comparisons of participants' categorization functions against these normative models provides a principled and informative approach to identifying constraints on adaptive speech perception, addressing prediction (3) about learning to convergence.


\begin{table}[!ht]
\begin{small}
\begin{tabular}{p{0.28\textwidth}p{0.7\textwidth}}
\hline
Prediction & Evidence that the {\em outcome} of learning is compatible with this prediction \\
\hline
(1) - {\em prior expectations} & (Kang \& Schertz, 2021; Schertz et al., 2016; Tan et al., 2021; Xie et al., 2021) \\

(2a) - {\em exposure amount}  & (Vroomen et al., 2007; Cummings \& Theodore, 2023; Kleinschmidt \& Jaeger, 2011; Liu \& Jaeger, 2018) \\

(2b) - {\em exposure distribution} & (Chl\'adkov\'a et al., 2017; Clayards et al., 2008; Colby et al., 2018; Hitczenko \& Feldman, 2016; Idemaru \& Holt, 2011; Kleinschmidt, 2020; Theodore \& Monto, 2019) \\

(3) - {\em learn to convergence}  & --- \\

(4) - {\em diminishing returns} & --- \\

\hline
\end{tabular}
\caption{Predictions of distributional learning models about incremental adaptation to an unfamiliar talker. To the best of our knowledge, only prediction (2a) has been tested against {\em incremental} changes in listeners' behavior, and predictions (3) and (4) have not been tested at all.}
\label{tab:predictions}
\end{small}
\end{table}

<!-- Prediction 1 ({\bf prior expectations}) & Listeners' categorization function prior to informative exposure to an unfamiliar talker's speech is determined by the statistics of the speech input they have previously experienced from other talkers. & AA, \cite{kang-schertz2021, schertz2016, tan2021, xie2021cognition} \\ -->

<!-- Prediction 2a ({\bf exposure amount}) & \multirow{2}{.6\textwidth}{With increasing exposure to the new talker, listeners' adapt their prior expectations by integrating information about the talker's phonetic distributions. The direction and magnitude of changes in listeners' categorization function relative to their pre-exposure behavior are determined by the amount and distribution of phonetic cues relative the statistics of previously experienced speech input}. & LGPL/VGPL, \cite{cummings-theodore2023, kleinschmidt-jaeger2012, liu-jaeger2018, vroomen2007} \\ -->

<!-- Prediction 2b ({\bf exposure distribution}) & & AA, \cite{xie2021cognition, tan2021}; DL, \cite{clayards2008, chladkova2017, colby2018, idemaru-holt2011, kleinschmidt-jaeger2016, theodore-monto2019} \\ -->

<!-- Prediction 3 ({\bf learn to convergence}) &  With additional exposure, listeners will continue to adapt until they have fully learned the exposure distribution. & \\ -->

<!-- Prediction 4 ({\bf diminishing returns}) & With each new observation, changes in listeners' behavior depend on the prediction error (or equivalently, the amount of new information) associated with that observation. As a consequence, listeners' behavior should initially change quickly and then less and less as listeners converge against the exposure distribution. & LGPL \cite{liu-jaeger2018} \\ -->

Our paradigm integrates, and builds on, advances in separate lines of research on unsupervised distributional learning during speech perception [@clayards2008; @colby2018; @kleinschmidt2020; @theodore-monto2019], lexically- or visually-guided perceptual learning [@cummings-theodore2023; @kleinschmidt-jaeger2012; @vroomen2007], and adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition]. In the general discussion, we return to these and related works in more detail. For now, we make three observations about previous work that motivated our approach.

First, research on adaptation to natural accents does not typically investigate how the phonetic properties of the exposure input relate to changes in listeners' behavior. This is in part due to the methodological challenges inherent to data that are high in ecological validity, but also of high dimensionality: it is simply more complicated to model the acoustic consequences of natural accents. Even notable exception to this trend have thus mostly been limited to broad qualitative comparisons [e.g., @schertz2016; @xie2017; see also, @schertz-clare2020], leaving open whether the direction and magnitude of changes in listeners' behavior can be predicted by existing models [but see @hitczenko-feldman2016; @tan2021; @xie2021cognition]. This limitation is generally shared with research on lexically- or visually-guided perceptual learning. Tests of distributional learning models have thus largely relied on paradigms that afford researchers with fine-grained control over the distribution of phonetic properties that listeners experience in the experiment [e.g., @chladkova2017; @clayards2008; @colby2018; @idemaru-holt2011; @kleinschmidt2020; @theodore-monto2019]. We follow this approach here. We do, however, take several modest steps towards addressing concerns about the ecological validity of such approaches. This includes concerns about the ecological validity of both the speech stimuli and their distribution in the experiment [see discussion in @baese-berk2018]. For example, previous distributional learning studies have often used highly unnatural, 'robotic'-sounding, speech. Beyond raising questions about what types of expectations listeners apply to such speech, these stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. <!-- Similarly, lexically- or visually-guided perceptual learning studies have often used perceptually ambiguous stimuli obtained by 'acoustic blending'---mixing recordings of two words (e.g., "sin" and "shin") at different relative intensity. This, too, can create acoustic properties that are rarely, if ever, observed in human speech (Rachel Theodore, p.c.). --> We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the exposure *distributions* under Methods.

Second, the few studies that have tested predictions of existing models have investigated the *outcome* of learning, leaving open whether adaptive speech perception unfolds over time in ways consistent with distributional learning models. For example, in an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g., /b/, /d/, or /g/) is produced with lower VOT than the voiceless category (/p/, /t/, /k/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Using a distributional learning model similar to the idealized learners we presented below, Clayards and colleagues predicted that listeners in the wide variance group would exhibit a more shallow categorization function than the narrow variance group. This is precisely what they found, providing support for prediction (2b) that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior [see also @nixon2016; @theodore-monto2019]. Findings like these suggests that the outcome of adaptation is qualitatively compatible with predictions (2a) and (2b) of distributional learning models [see also @hitczenko-feldman2016; @tan2021; @xie2021cognition]. Previous studies have, however, relied on tests that averaged over, and/or followed, hundreds of exposure trials.^[A related line of work has used distributional learning or explicit training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mcclelland1999; @pajak-levy2012; @pisoni1982]. These studies, too, have observed outcomes predicted by distributional learning models [for review, see @pajak2016], but have left untested the incremental unfolding of learning, or assessed it at time scales much longer than the ones tested here. <!-- TO DO after submission--- (1) could consider returning to these works in the general discussion, highlighting the link to L1 constraints on L2 learning; (2) could add this back in: Here, we focus on adaptation to shifts along *known* phonetic continua---i.e., the type of input that is generally believed to be easiest to adapt to. --> ] This leaves open whether listeners’ categorization behavior follows the change pattern predicted by models of adaptive speech perception: where categorization first reflects expectations based on previously experienced phonetic distributions (prediction 1) and with increasing exposure, integrates the phonetic distributions of the input from the unfamiliar talker (predictions 2a,b). Previous studies also focused on one prediction at a time, leaving open how the effect of prior expectations and the statistics of the unfamiliar input *jointly* explain adaptation. 

Third and finally, we are not aware of any previous tests of predictions (3 - *learn to convergence*) and (4 - *diminishing returns*): without incremental testing it is difficult to assess whether there are hard limits on adaptation or simply 'how far the learner has gotten' with the exposure input they have received so far [for discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020]. For the same reasons, it is difficult to assess whether the build-up of adaptation follows the predictions of error-driven learning or ideal information integration (prediction 4). 

The incremental exposure-test paradigm in Figure \@ref(fig:block-design-figure) begins to address these knowledge gaps. To anticipate our results, we find that the changes in listeners' categorization behavior *largely* follow the predictions of distributional learning models. In particular, we present the first direct evidence that the direction and magnitude of changes in listeners' categorization functions is jointly determined by their prior expectations (prediction 1) and the amount and distribution of phonetic cues in the exposure input (predictions 2a,b). We also find initial---though not decisive---evidence that changes in rate of adaptation across exposure are consistent with the predictions of error-driven learning theories and theories of ideal information integration (prediction 4). We show that a Bayesian model of adaptation that is based on principles of ideal information integration [the ideal adaptor, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016] predicts participants' responses with very high accuracy ($R^2 = 97\%$). However, not all observations we make are predicted by existing models, providing new insights into previously unrecognized limits of adaptation. In particular, we find little support for prediction (3 - *learn to convergence*). Rather, changes in listeners' behavior seem to plateau long before listeners achieve the categorization functions and accuracy that would be expected if they fully learned the talkers' phonetic distributions (cf. the *premature convergence* panel of Figure \@ref(fig:predictions)C). We also find that this constraint on adaptation seems to be asymmetric, depending on the direction of the shift in the exposure input relative to listeners' prior expectations. We discuss the implications of our findings for theories of adaptive speech perception, and suggest how future variants of our paradigm can be used to further contrast different models of adaptive speech perception. 

## Open science
All data and code for this article are available on OSF at [https://osf.io/hxcy4/](https://osf.io/hxcy4/). Following @xie2023, both this article and its supplementary information (SI) are written in R Markdown. This allows other researchers to replicate and revise our analyses with the press of a button using freely available software [R, @R; @RStudio, see also SI, \@ref(sec:software)]. 

This study was not publicly pre-registered. The design, participant recruitment, and procedure were internally pre-registered as part of an undergraduate class at the University of Rochester (BCS206/207). The experiment was originally designed to address predictions (1)-(3). Our analyses of prediction (4 - *diminishing returns*) are thus post-hoc, as are some of the analyses we present to understand the evidence against prediction (3 - *learn to convergence*). All post-hoc analyses are indicated as such. Finally, the ideal observer and adaptor models introduced below to guide interpretation of results follow our previous work [@kleinschmidt-jaeger2015; @tan2021; @xie2023]. However, the choice of phonetic data on which these models are trained constitute researcher degrees of freedom. Where relevant, we motivate our decisions.

```{r}
rm(
  d.talker_IO,
  d.talker_types,
  d.talker_cat,
  arrow_PSE,
  p.cat,
  p.PSE,
  p.PSE_change,
  p.gaussian)
```

