# Introduction
One of the hallmarks of human speech perception is its adaptivity. Listeners' interpretation of acoustic input can change within minutes of exposure to an unfamiliar talker, supporting robust speech recognition across talkers [@bradlow2008; @clarke2004; @xie2018; @xie2021]. Recent reviews have identified distributional learning of marginal cue statistics ['normalization', @apfelbaum-mcmurray2015; @mcmurray-jongman2011] or the statistics of cue-to-category mappings as an important mechanism affording this adaptivity ['representational learning', @clayards2008; @idemaru-holt2011; @kleinschmidt-jaeger2015; @davis-sohoglu2020; for review, @schertz-clare2020; @xie2023]. This hypothesis has gained considerable influence over the past decade, with findings that changes in listener perception are qualitatively predicted by the statistics of exposure stimuli [@bejjanki2011; @clayards2008; @idemaru-holt2020; @kleinschmidt2012; @munson2011; @nixon2016; @theodore-monto2019; @tan2021; for important caveats, see @harmon2019].

** POINTS FOR NEW ANGLE AND REFRAMING OF ARTICLE **
+ Theoretical implications from empirical findings within L2 accent adaptation [@bradlow2008; @clarke2004; @xie2018; @xie2021; @tan2021], context guided perceptual recalibration [@norris2003; @bertelson2003; @vroomen2004], and distributional learning (*THE EXPERIMENTAL PARADIGM) can be unified under the ideal adapter framework [@kleinschmidt-jaeger2015].  

+ The Ideal adapter framework posits that speech sounds are represented as distributions of acoustic cues under a given linguistic category or context. Each distribution specifies the conditional probability of a given cue -- the same cue may fall within a competing category but would have a different probability.  At any given moment a listener is in a state that holds prior beliefs about cue distributions formed over his or her long-term experience with talkers and the linguistic context. Because cue distributions differ across talkers and contexts the listener infers the intended linguistic category of a novel talker by integrating prior beliefs about cue-category mappings with the present input received from the talker. The IA is computationally specified with parameters that represent listeners' expected prior category means and variances, and their degree of uncertainty about those expected prior category means and variances. These variables influence the pace of adaptation and the resultant change in categorisation behavior with each additional piece of input. Under this framework researchers are able to make finer grain predictions and analyses about the apparent changes in listener perception.

+ We have seen this within the paradigm of lexically guided perceptual recalibration. By manipulating the number of times an ambiguous sound between /s/ and /sh/ was heard between participants (1, 4, 10 or 20 occurences), @cummings-theodore2023 showed that the size of the putative perceptual recalibration effect corresponded to the frequency of exposure. The finding that shifts in perception is a function of the amount of exposure to an atypical sound fits well with an IA account. This kind of statistically modulated change is rarely demonstrated not least because of how speech perception studies are typically designed; most studies dispense exposure in one block followed by a block of test trials [many examples]. One exception is @vroomen2007's perceptual recalibration study with visual labelling. In that experiment, subjects were tested prior to exposure to an ambiguous sound between /aba/ and /ada/ paired with videos of a person either clearly articulating "aba" or "ada". Subjects were tested cumulatively throughout the experiment. @kleinschmidt-jaeger2011 showed that the IA provided an good fit to the cumulative build-up in adaptation.

+ So far the studies discussed involve post-exposure behavioural change from exposure to a single ambiguous token embedded in labeled contexts. In the less commonly employed distributional learning paradigm experimenters construct clusters of cue values centered around specific mean values with specific variances along a single acoustic dimension to simulate different talker distributions. These cues are mapped onto adjacent sound categories and played to listeners in the context of minimal pair words [@clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019; @kleinschmidt2020]. The exposure to these distributions may be supervised or unsupervised with labelling information that signal the intended sound of the talker. Much on distributional learning has been empirically investigated through this paradigm. @kleinschmidt-jager2016 for instance exposed L1-US English listeners to recordings of /b/-/p/ minimal pair words like *beach* and *peach* that were acoustically manipulated. Separate groups of listeners were exposed to distributions of voice onset times (VOTs)---the primary cue distinguishing  words like *beach* and *peach*---that were shifted by up to +30 ms, relative to what one might expect from a 'typical' talker (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)A). In line with the distributional learning hypothesis, listeners' category boundary or point of subjective equality (PSE)---i.e., the VOT for which listeners are equally likely to respond "b" or "p"---shifted in the same direction as the exposure distribution (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)B). Also in line with the distributional learning hypothesis, these shifts were larger the further the exposure distributions were shifted. 

+ Although they were able to find differences in categorisation behaviour between exposure groups as well as computationally show that listeners updated their beliefs incrementally through the course of the categorization experiment several limitations remain to be addressed. Firstly, distributional learning studies have not typically been set up as exposure-test configurations. All exposure trials are in a sense test trials as well. This means that the estimated categorisation function is fit over a number of trials decided by the researcher -- for e.g. @kleinschmidt-jaeger2016 apportioned their 222 trials into 6 segments. A less contentious objection (*NOT SURE IF THIS IS EVEN A CRITICISM) is the lack of common test stimuli for a more neutral comparison between groups. 

+ Given the predictions of the IA -- that adaptation is incremental and a function of integrating priors with current input --  a reasonable next step in continuing the prioneering work of (@clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019) is to attempt to test for incremental changes in adaptation. Apart from examining whether incremental change indeed happens through the course of a distributional learning experiment, we can simultaneously document how soon after the onset of exposure do distributional learning effects emerge. This is something that remains opaque given that previous designs do not have test blocks interspersed during exposure[ except for Zach's experiment?]. Given the substantial amount of evidence that adaptation takes place rapidly (e.g. 5 mins from exposure in L2 accent adaptation; 4 - 10 trials in lexically guided perceptual recalibration) we might expect listeners to also show learning effects very early on in an experiment. On the other hand, it is possible that distributional learning effects may show up later than one would expect in perceptual recalibration since listeners have a presumably more challenging task of inferring the means of two categories over a range of cues.

+ In experimental work researchers often have to consider the generalizability of their results which leads to questions about ecological validity. There is a trade-off between ecological validity of experimental design and the desired degree of control over the variables. Questions about eco validity of prior work in distributional learning pertains to 2 features. First, the stimuli which were generated with a synthesiser, had an obvious machine-like quality [@kleinschmidt-jaeger2016; @clayards2008]. Secondly, the pairs of distributions of voiced and voiceless categories were always identical in their variances which adds to the artificiality of the experiment. 

+ This study's aim is to extend previous work in distributional learning by introducing small innovations as a first step towards improving its ecological validity. In doing so, it will also provide a stronger test of the effects of distributional learning discovered in prior work. Another important design change is the interspersing of frequent test blocks throughout exposure. This allows us to shed more light into the process of adapting to a novel talker. 




<!-- 
We investigate an important constraint on this type of adaptivity that is suggested by recent findings. @kleinschmidt-jaeger2016 exposed L1-US English listeners to recordings of /b/-/p/ minimal pair words like *beach* and *peach* that were acoustically manipulated. Separate groups of listeners were exposed to distributions of voice onset times (VOTs)---the primary cue distinguishing  words like *beach* and *peach*---that were shifted by up to +30 ms, relative to what one might expect from a 'typical' talker (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)A). In line with the distributional learning hypothesis, listeners' category boundary or point of subjective equality (PSE)---i.e., the VOT for which listeners are equally likely to respond "b" or "p"---shifted in the same direction as the exposure distribution (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)B). Also in line with the distributional learning hypothesis, these shifts were larger the further the exposure distributions were shifted. However, Kleinschmidt and Jaeger also observed a previously undocumented property of these adaptive changes: shifts in the exposure distribution had less than proportional (sublinear) effect on shifts in PSE (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)C). While this finding is broadly compatible with the hypothesis of distributional learning, it points to important not well-understood constraints on adaptive speech perception.-->


(ref:kleinschmidt-jaeger-2016-replotted) Design and results of @kleinschmidt-jaeger2016 replotted. **Panel A:** Different groups of participants were exposed to different shifts in the mean VOT of /b/ and /p/. **Panel B:** categorization functions of individual participants depending on the exposure condition (shift in VOT means of /b/ and /p/). For reference, the black dashed line shows the categorization function of the 0-shift condition. The colored dashed lines shows the categorization function expected for an ideal observer that has fully learned the exposure distributions. **Panel C:** Mean and 95% CI of participants' points of subjective equality (PSEs), relative to the PSE of the ideal observers.

```{r kleinschmidt-jaeger-2016-refitted}
# load K&J2016 data and filter to semi-supervised rows
d.KJ16 <- 
  supunsup_clean %>%
  filter(supCond == "mixed" | supCond == "supervised") %>%
  # rename variables so that they match the naming conventions employed in the remainder
  # of this paper.
  rename(
    ParticipantID = subject,
    Item.VOT = vot,
    Condition.Exposure = bvotCond,
    Response.Voiceless = respP,
    Item.MinimalPair = wordClass,
    category = respCategory) %>%
  mutate(
    Condition.Exposure = factor(case_when(
      Condition.Exposure == 0 ~ "+0ms",
      Condition.Exposure == 10 ~ "+10ms",
      Condition.Exposure == 20 ~ "+20ms",
      Condition.Exposure == 30 ~ "+30ms")),  
    Condition.Exposure = fct_relevel(
      Condition.Exposure, c("+0ms", "+10ms", "+20ms", "+30ms"))) 

d.KJ16_unlabeled <- 
  d.KJ16 %>% 
  filter(labeled == "unlabeled") %>%
  group_by(ParticipantID) %>%
  # filter to final 6th of total trials
  slice_tail(n = 37)

VOT.mean_d.KJ16_unlabeled <- mean(d.KJ16_unlabeled$Item.VOT)
VOT.sd_d.KJ16_unlabeled <- sd(d.KJ16_unlabeled$Item.VOT)
d.KJ16_unlabeled %<>% mutate(VOT_gs = (Item.VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled))

contrasts(d.KJ16_unlabeled$Condition.Exposure) <-
  cbind(" 10vs0" = c(-3/4, 1/4, 1/4, 1/4),
        " 20vs10" = c(-1/2, -1/2, 1/2, 1/2),
        " 30vs20" = c(-1/4, -1/4, -1/4, 3/4))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 1 + VOT_gs * Condition.Exposure +
        (1 + VOT_gs | ParticipantID) +
        (1 + VOT_gs * Condition.Exposure | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .99),
    file = "../models/KJ16-semisupervised-GLMM.rds")

# fit nested model to extract slopes and intercepts
fit_nested_KJ16 <- 
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~  0 + (Condition.Exposure) / VOT_gs +
        (0 + VOT_gs | ParticipantID) +
        (0 + (Condition.Exposure) / VOT_gs | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000, 
    warmup = 2000, 
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .995),
    file = "../models/KJ16-semisupervised-nested-GLMM.rds")

cond_fit_KJ16 <- fit_KJ16 %>% 
  conditional_effects(
  effects = "VOT_gs:Condition.Exposure",
  method = "posterior_epred",
  ndraws = 500,
  re_formula = NA) %>% 
  .[[1]] %>% 
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled))

d.KJ16_PSE <- 
  fit_nested_KJ16 %>% 
  gather_draws(`b_mu2_Condition.Exposure.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(
    term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept"),
    .variable = gsub("b_mu2_Condition.ExposureP(\\d{1,2}ms).*$", "\\1", .variable)) %>% 
  pivot_wider(names_from = term, values_from = ".value") %>%
  rename(Condition.Exposure = .variable) %>% 
  relocate(c(Condition.Exposure, Intercept, slope, .chain, .iteration, .draw)) %>% 
  mutate(
    PSE = descale(-Intercept/slope, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled),
    Condition.Exposure = paste0("+", Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), mean = mean, upper = ~ quantile(.x, probs = .975))))
```


```{r io-categorization-kleinschmidt-jaeger-2016}
# Test points by condition
x <- 
  d.KJ16_unlabeled %>% 
  group_by(Condition.Exposure) %>% 
  distinct(Item.VOT) %>% 
  rename(x = Item.VOT) 

# get io categorizations of the test points by condition
io.d.KJ16 <- 
  make_MVG_ideal_observer_from_data(
  d.KJ16 %>% 
    rename(VOT = Item.VOT) %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure", 
    cues = c("VOT"), 
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>% 
  nest(io = -c(Condition.Exposure)) %>% 
  left_join(x) %>% 
  mutate(x = map(x, ~ c(.x))) %>% 
  nest(x = x) %>% 
  mutate(categorization = map2(
    x, io,
    ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% 
    mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>% 
  unnest(cols = categorization, names_repair = "unique") %>% 
  pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_b = round(`response_b` * 10^12), n_p = 10^12 - n_b) %>% 
  group_by(Condition.Exposure) %>% 
  nest(data = -c(Condition.Exposure)) %>% 
  mutate(
    model_unscaled = map(
      data, ~ glm(
      cbind(n_p, n_b) ~ 1 + VOT, 
      family = binomial, 
      data = .x)),
    intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_unscaled = map_dbl(
      model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
    model_scaled = map(data, ~ glm(
      cbind(n_p, n_b) ~ 1 + I((VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled)), 
      family = binomial, 
      data = .x)),
    intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
    PSE = -intercept_unscaled/slope_unscaled)

# get io with lapse-accounted categorization 
io.d.KJ16.lapse_rate <- 
  make_MVG_ideal_observer_from_data(
    data = d.KJ16 %>% 
      rename(VOT = Item.VOT) %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure", 
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT")),
    lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>% 
  nest(io = -c(Condition.Exposure)) %>% 
  crossing(x = seq(-10, 70, .5)) %>% 
  nest(x = x) %>% 
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>% 
           filter(category == "p") %>% 
           mutate(VOT = map(x, ~.x[1]) %>% unlist()))) %>% 
  unnest(categorization, names_repair = "unique")

# get io of typical talker and its categorization (+0 condition)
d.typical_talker <- 
  io.d.KJ16[[2]][[1]][1, 1] %>% 
  unnest(io) %>% 
  mutate(lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>% 
  nest(io = everything()) %>% 
  crossing(x = seq(-10, 70, .5)) %>% 
  nest(x = x) %>% 
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>% 
           filter(category == "p") %>% 
           mutate(VOT = map(x, ~.x[1]) %>% unlist())),
    line = map(
      categorization, 
      ~ geom_line(
          data = .x,
          mapping = aes(x = VOT, y = response),
          linetype = 2,
          linewidth = 0.6,
          alpha = .8,
          colour = "black")))
```




```{r kleinschmidt-jaeger-2016-replotted, fig.height=base.height*3.5, fig.width=base.width*5.5, fig.cap="(ref:kleinschmidt-jaeger-2016-replotted)"}
# make histograms of exposure distributions
p.KJ16.histogram <-
  d.KJ16 %>% 
  group_by(Condition.Exposure) %>%
  slice_head(n = 222) %>% 
  ggplot(aes(x = Item.VOT,
             fill = paste(Condition.Exposure, trueCat))) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous(breaks = c(0, 25, 50)) +
  scale_fill_manual(
    "Category", 
    values = c("+0ms b" = "#f8766d",
               "+0ms p" = "#fdd1ce",
               "+10ms b" = "#7cae00",
               "+10ms p" = "#d4ff66",
               "+20ms b" = "#00bfc4",
               "+20ms p" = "#99fcff",
               "+30ms b" = "#c77cff",
               "+30ms p" = "#e9ccff"),
    aesthetics = "fill",
    labels = c("/b/", "/p/", "", "", "", "", "", "")) +
   guides(
    fill = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", NA, NA, NA, NA, NA, NA),
        values = c("b", "p", NA, NA, NA, NA, NA, NA)), nrow = 1)) +
  facet_wrap(~ Condition.Exposure, nrow = 1) +
  theme(legend.justification = "left") +
  remove_x_guides
  
p.KJ16.fit <- 
  cond_fit_KJ16 %>% 
  rename(Condition = Condition.Exposure) %>% 
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    ymin = lower__, ymax = upper__, fill = Condition), 
    alpha = .1,
    show.legend = F) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    color = Condition),
    linewidth = .7,
    alpha = 0.6,
    show.legend = F) +
  stat_summary(
    data = d.KJ16_unlabeled %>% 
      rename(Condition = Condition.Exposure) %>% 
      group_by(Condition),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      group = Condition,
      colour = Condition),
    geom = "pointrange",
    size = 0.15,
    show.legend = F) +
  geom_line(
    data = io.d.KJ16.lapse_rate %>% 
      rename(Condition = Condition.Exposure) %>% 
      group_by(Condition),
    mapping = aes(x = VOT, y = response, group = Condition, colour = Condition),
    linetype = 2,
    linewidth = 1,
    alpha = .6,
    inherit.aes = F,
    show.legend = F) +
  d.typical_talker$line +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Proportion \"p\"-responses", breaks = c(0, .5, 1)) +
  facet_wrap(~ Condition, nrow = 1)

p.KJ16.PSE <- 
  d.KJ16_PSE %>% 
  left_join(io.d.KJ16) %>% 
  rename(PSE.io = PSE) %>% 
  ggplot(aes(y = PSE_mean, x = Condition.Exposure, colour = Condition.Exposure)) +
  geom_hline(
    yintercept = c(20, 30, 40, 50), 
    linewidth = 1.5,
    alpha = .4,
    linetype = 2, 
    colour = scales::hue_pal()(4)) +
  geom_point(size = 2, show.legend = F) +
  geom_linerange(
    aes(ymin = PSE_lower, ymax = PSE_upper), size = 1, alpha = .8, show.legend = F) +
  scale_x_discrete("Condition") +
  scale_y_continuous("PSE") +
  theme(axis.text.x = element_text(angle = 22.5, hjust = .8))

layout <- "
AAAA#
BBBBC"

p.KJ16.histogram + p.KJ16.fit + p.KJ16.PSE +
  plot_layout(design = layout,
              guides = "collect") +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(legend.position = "top",
        plot.tag = element_text(face = "bold"))
```

For example, influential models of adaptive speech perception predict proportional, rather than sublinear, shifts (for proof, see SI \@ref(sec:ibbu-proof)). This is the case both for incremental Bayesian belief-updating model [@kleinschmidt-jaeger2011] and general purpose normalization accounts [@mcmurray-jongman2011]---models that have been found to explain listeners' behavior well in experiments with less substantial changes in exposure. There are, however, proposals that can accommodate this finding. Some proposals distinguish between two types of mechanisms that might underlie representational changes, <!--- link representational change and distributional learning --> *model learning* and *model selection* [@xie2018, p. 229]. The former refers to the learning of a new category representations---for example, learning a new generative model for the talker [@kleinschmidt-jaeger2015, Part II] or storage of new talker-specific exemplars [@johnson1997; @sumner2011]. Xie and colleagues hypothesized that this process might be much slower than is often assumed in the literature, potentially requiring multiple days of exposure and memory consolidation during sleep [see also @fenn2013; @tamminen2012; @xie2018sleep]. Rapid adaptation that occurs within minutes of exposure might instead be achieved by selecting between *existing* talker-specific representations that were learned from previous speech input---e.g., previously learned talker-specific generative models [see mixture model in @kleinschmidt-jaeger2015, p. 180-181] or previously stored exemplars from other talkers [@johnson1997]. Model learning and model selection both offer explanations for the sublinear effects observed in @kleinschmidt-jaeger2016. But they suggest different predictions for the evolution of this effect over the course of exposure.

Under the hypothesis of model learning, sublinear shifts in PSEs can be explained by assuming a hierarchical prior over talker-specific generative models [$p(\Theta)$ in @kleinschmidt-jaeger2015, p. 180]. This prior would 'shrink' adaptation towards listeners' priors---similar to the effect of random by-subject or by-item effects in generalized linear mixed-effect models, which shrink group-level effect estimates towards the population mean of the data [@baayen2008]. Critically, as long as these priors attribute non-zero probability to even extreme shifts (e.g., the type of Gaussian prior used in mixed-effects models), this predicts listeners' PSEs will continue to change with increasing exposure until they have converged against the PSE that is ideal for the exposure statistics. In contrast, the hypothesis of model selection predicts that rapid adaptation is more strictly constrained by previous experience: listeners can only adapt their categorization functions up to a point that corresponds to (a mixture of) previously learned talker-specific generative models. This would imply that at least the earliest moments of adaptation are subject to a hard limit (Figure \@ref(fig:prediction)): exposure helps listeners to adapt their interpretation to more closely aligned with the statistics of the input, but only to a certain point.

(ref:prediction) Contrasting predictions of model learning and model selection hypotheses about the incremental effects of exposure on listeners' categorization function. Both hypothesis predict incremental adaptation towards the statistics of the input, as well as constraints on this adaptation. The two hypotheses differ, however, in that model selection predicts a hard limit on how far listeners' can adapt during initial encounters with an unfamiliar talker.

```{r prediction, fig.height=base.height+1/4, fig.width=base.width*2, fig.cap="(ref:prediction)"}
k <- 10^-1

crossing(
  Exposure = 0:100,
  Shift = c(20, 40),
  Hypothesis = c("model learning", "model selection")) %>%
  mutate(
    PSE = 25 + ifelse(Hypothesis == "model learning", Shift, pmin(Shift, 25)) * 
      (1 - exp(-k * Exposure))) %>%
  ggplot(aes(x = Exposure, y = PSE, color = factor(Shift))) +
  geom_hline(
    data = crossing(Shift = c(0, 20, 40), Hypothesis = c("model learning", "model selection")),
    aes(yintercept = Shift + 25, color = factor(Shift)), linetype = 2) +
  geom_line(alpha = .5) +
  scale_y_continuous("PSE (in ms VOT)") +
  scale_color_manual(breaks = c("0", "20", "40"), values = c("gray", "green", "blue")) +
  facet_wrap(~ Hypothesis) +
  guides(color = "none") +
  theme(panel.grid = element_blank())
```

The present study employs a novel incremental exposure-test paradigm to address two questions. We test whether the sublinear effects of exposure observed in recent work replicate for exposure that (somewhat) more closely resembles the type of speech input listeners receive on a daily basis. And, we evaluate the predictions of the model learning and selection hypotheses against human perception. We take this question to be of interest beyond the specific hypotheses we contrast: whether there are hard limits to the benefits of exposure to unfamiliar speech patterns ultimately has consequences for education and medical treatment. 

All data and code for this article can be downloaded from [https://osf.io/hxcy4/](OSF). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models (see SI, \@ref(sec:software)).





