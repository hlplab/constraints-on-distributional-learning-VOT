```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")

# load required objects that were removed for memory efficiency
fit_test.simple_effects_condition <- readRDS("../models/test-nested-condition.rds")
fit_test.simple_effects_block <- readRDS("../models/test-nested-block.rds")


```

<!-- Do NOT knit this document. It is part of a larger document. Instead knit the main document (my-apa-formatted-article)
     If you want to separate the SI from the rest of the paper, we recommend you do so AFTER knitting them into a single PDF.
     This will make sure that all references to sections, figures, tables, etc. are working as intended. You can easily separate
     the PDF into two parts, using e.g., Acrobat PDF viewer. //-->

# Supplementary information {-}
\setcounter{section}{0}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via [OSF](). It is best viewed using Acrobat Reader.

\newcommand{\changelocaltocdepth}[1]{%
  \addtocontents{toc}{\protect\setcounter{tocdepth}{#1}}%
  \setcounter{tocdepth}{#1}%
}

\tableofcontents
\setcounter{tocdepth}{-10}
\changelocaltocdepth{-10}

# Required software {#sec:software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r}
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}).

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. The full session information is provided at the end of this document.


# Additional information about Materials {sec:stimulus-generation}
All stimuli are available as part of the OSF repository for this article.

## Recordings
An L1-US English female talker originally from New Hampshire was recruited for recording of the stimuli. She was recorded at the Human Language Processing lab at the Brain & Cognitive Sciences Department, University of Rochester with the help of research assistant (also an L1-US English speaker). She was 23 years old at the time of recording and was judged by the research assistant to have a generic US American accent known as "general American".

Four /d-t/ minimal pairs (dill-till, din-tin, dim-tim, dip-tip) were recorded together with 20 filler words. These fillers were made up of 10 minimal or near minimal pairs with different sounds at onset. The word pairs were separated into two lists so that they would appear in separate blocks during recording. Each critical pair was repeated 8 times while the filler pairs were repeated 5 times. Word presentation was delivered with PsychoPy [@peirce2007] and the presentation was controlled by the researcher from a computer located outside the recording room. The order of each block was randomized such that target words never appeared consecutively. The talker was instructed to speak clearly and confidently, and to maintain a consistent distance from the microphone.

## Annotation
All critical pairs of the talker's recordings were annotated. Durational measurements of voicing lead, VOT, and vowel were taken in addition to the mean F0 of the first 25% of the vowel duration. Annotations were made with a combination of listening to the audio file and inspection of the waveform and spectrogram. The annotation boundaries were made according to the following principles:

- pre-voicing (voicing during closure)
  -**start:** the first sign of periodicity in the waveform before closure release.
  -**End:** the point of closure release

- VOT
  -**start:** the point of closure release.
  -**End:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram.

- Vowel
  -**start:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram.
  -**End:** if before a stop, when periodicity becomes irregular or at closure onset; if before a lateral, when formant transition approaches steady state; if before a nasal, at point where formants show a step-wise shift and when intensity shows a steep decline.

- F0 at vowel onset
  -the average pitch measurement estimated at the beginning of the vowel.

## Resynthesis
The stimuli was created using the "progressive cutback and replacement method" by [@winn2020] implemented in Praat [@boersma2022praat]. This automates and greatly simplifies the process for generating highly natural sounding stimuli. Users of the script need only specify certain parameters to produce desired stimuli. Stimuli with pre-voicing were created separately from stimuli with positive VOT. This was because the script was not coded to automate the creation of tokens with pre-voicing that are natural sounding ^[It can however, produce pre-voicing sufficiently well for demonstration purposes, see video demo at https://www.youtube.com/watch?v=-QaQCsyKQyo.] As such, the pre-voicing stimuli were created by prepending pre-voicing generated from naturally produced tokens (described below) that were edited with a separate process.

### Tokens with positive VOTs
For each minimal pair a continuum of 31 tokens was generated between 0ms and 150ms with a step-size of 5ms. A token of the voiced category from each pair was selected to be the base sound file to make the continuum. All four minimal pair continua had an identical aspiration sound which was excised from one of the voiceless tokens produced by the talker.  
While the main manipulation of the recordings was done on VOT we set the fundamental frequency (F0) to covary with VOT according to the natural correlation exhibited by our talker. The F0 values were predicted by regressing the talker's F0 measurements on VOT. Target F0 values for each token were then generated by setting the predicted F0 values of the end-point VOT tokens (0ms and 150ms) in the Praat script.   
The vowel cut-back ratio was set at 0.33 which translates into .33ms vowel reduction for every 1ms of VOT. This ratio followed the estimated vowel duration-VOT trade-off for dip-tip minimal pair tokens reported in @allen-miller1999. The maximum allowed vowel cut-back was 0.5ms to avoid the short vowel in **dip** becoming too short.
Lastly, the rate of increase for aspiration intensity was kept at the default settings of the script.

### Token with pre-voicing ('negative VOTs')
Pre-voicing in 5ms increments were generated from a a clear pre-voicing waveform excised from a pre-voiced token produced by the talker. To achieve a desired duration a duration factor was first computed and then converted with the "lengthen (overlap-add)" function in Praat. For example, if the desired amount of pre-voicing was 50ms then the duration factor would be 50ms/length of the original pre-voicing sample. Each pre-voicing step was then prepended to a token with 0ms VOT. Each of these 0ms tokens was generated with @winn2020 Praat script by manually entering the expected F0 value for a given pre-voicing duration based on the predictions of the linear model. No vowel-cut back was implemented for pre-voiced tokens.

All the synthesised stimuli were subsequently annotated for pre-voicing, VOT, vowel duration and F0 at the first 5ms from vowel onset. This F0 measurement was made in order to align the data with the production database that we use for ideal observer analysis. Items' F0 in relation to VOT is plotted in Figure \@ref(fig:stimuli-cue-measurements). 

```{r stimuli-cue-measurements}
# Annotation information of synthesised stimuli
files <- list.files(path = "../materials/stimuli_AE/annotation_files/", pattern = "^d.*_stimuli_f0_measured.csv")

file_list <- seq(1:length(files))

d.stimuli <- map(file_list, ~ read_csv(file = str_c("../materials/stimuli_AE/annotation_files/", files[[.x]])))
d.stimuli %<>% bind_rows() %>%
  mutate(temp = filename) %>%
  separate(temp, c("word", "VOT", "target_f0"), sep = "_") %>%
  mutate(VOT = as.numeric(gsub("VOT(.*)$", "\\1", VOT))) %>%
  arrange(filename, word, vowel, VOT, target_f0, f0_5ms_into_vowel, f0_10ms_into_vowel)

# Using lm() to obtain linear relation between VOT and f0
talker_m <- lm(f0_5ms_into_vowel ~ 1 + VOT, data = d.stimuli)

p.stimuli.VOT_f0 <- d.stimuli %>%
  ggplot(aes(x = VOT, y = f0_5ms_into_vowel, colour = word)) +
  geom_point(alpha = .4, size = 2) +
  scale_x_continuous(breaks = seq(-100, 150, 15)) +
  scale_y_continuous(breaks = seq(240, 252, 0.5)) +
  scale_colour_discrete("Minimal pair")
p.stimuli.VOT_f0
```


# Additional information on participant exclusions {sec:exclusions}
We provide additional information on participants' performance during catch trials and on labelled trials (for which there is a clearly correct response). Both of these measures were used to exclude participants.

## Performance on catch trials

(ref:plot-catch-trial-performance) Of the 122 participants, 1 committed more than 3 errors (<84% accuracy)

```{r plot-catch-trial-performance, fig.height=4, fig.width=6, fig.cap="ref:plot-catch-trial-performance"}
d %>%
  group_by(ParticipantID) %>%
  summarise(Total_errors = 18 - sum(CatchTrial.Correct, na.rm = T)) %>%
  ggplot(aes(x = Total_errors)) +
  geom_bar(width = .5) +
  scale_x_continuous("Total catch trial errors", breaks = seq(0, 6, 1)) +
  scale_y_continuous("Number of participants", breaks = seq(0, 120, 5))
```

## Performance on labelled exposure trials

```{r labeled-trial-performance-group, fig.width=6.5, fig.height=4, message=F}
p.labelled <- d %>%
  filter(Phase == "exposure") %>%
  group_by(ParticipantID, Condition.Exposure) %>%
  summarise(LabeledTrial.Correct = 72 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>%
  group_by(Condition.Exposure, LabeledTrial.Correct) %>%
  summarise(freq = n()) %>%
  ggplot(aes(x = LabeledTrial.Correct, y = freq, fill = Condition.Exposure)) +
  geom_bar(stat = "identity", position = position_dodge2(width = .5, preserve = "single"), width = .5) +
  geom_text(aes(label = freq), position = position_dodge(width = .5), vjust = -.5) +
  scale_colour_manual("Exposure condition",
                      aesthetics = c("colour", "fill"),
                      labels = c("baseline", "+10", "+40"),
                      values = colours.condition) +
  scale_x_continuous("Total labelled trial errors") +
  scale_y_continuous("No. of participants", breaks  = seq(0, 115, 5)) +
  theme(legend.position = "none")

p.labelled.group <- d %>%
  filter(Phase == "exposure" & Item.Labeled == T) %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  summarise(LabeledTrial.Errors = 24 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>%
  ggplot(aes(x = LabeledTrial.Errors, fill = Condition.Exposure)) +
  geom_bar(position = position_dodge(width = .5, preserve = "single"), width = .5) +
  scale_colour_manual("Condition",
                      aesthetics = c("colour", "fill"),
                      labels = c("baseline", "+10", "+40"),
                      values = colours.condition) +
  scale_x_continuous("Total labelled trial errors") +
  theme(legend.position = "top") +
  facet_grid(Condition.Exposure ~ Block, labeller = labeller(
    .cols = c("2" = "Exposure 1", "4" = "Exposure 2", "6" = "Exposure 3"), 
    .rows = c(Shift0 = "baseline", Shift10 = "+10", Shift40 = "+40")))

p.labelled | p.labelled.group +
  plot_layout(guides = "collect") & theme(legend.position = "top", legend.box.just = "left")
```

```{r fig,width=8, fig.height=5, fig.width=6}
d %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  filter(Is.CatchTrial == FALSE) %>%
  mutate(Response.log_RT = log10(Response.RT)) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>%
  ggplot(aes(x = mean, y = sd, label = ParticipantID)) +
  geom_text(aes(colour = Condition.Exposure), alpha = .5) +
  geom_rug(aes(colour = Condition.Exposure), alpha = .5) +
  scale_x_continuous("mean log-RT (log10 of ms)") +
  scale_y_continuous("SD of log-RT") +
  scale_color_manual("Exposure condition",
                      aesthetics = c("colour", "fill"),
                      breaks = c("Shift0", "Shift10", "Shift40"),
                      values = colours.condition) +
  theme(legend.position = "top") +
  facet_wrap( ~ Block )
```

```{r, fig.width=8, fig.height=3}
d.for_analysis %>%
  group_by(ParticipantID, Block) %>%
  mutate(
    Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
    Response.log_RT.scaled = scale(Response.log_RT),
    Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>%
  ungroup() %>%
  mutate(across(c(ParticipantID, Condition.Exposure, Block), factor)) %>%
  group_by(ParticipantID, Condition.Exposure,Block, Trial) %>%
  summarise(Response.log_RT) %>%
  ggplot(aes(x = Trial, y = Response.log_RT, colour = ParticipantID, group = Condition.Exposure)) +
  geom_path(aes(group = ParticipantID), alpha = .3) +
  scale_x_continuous("Trial", breaks = c(12, 24, 36, 48)) +
  scale_y_continuous("mean log-RT (log10 of ms)") +
  facet_grid(Condition.Exposure ~ Block, scales = "free_x", space = "free_x") +
  theme(legend.position = "none")
```

```{r, fig.width=8, fig.height=3}
d %>%
  group_by(Trial, Condition.Exposure, Block) %>%
  mutate(Response.log_RT = log10(Response.RT)) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>%
  ggplot(aes(x = Trial, y = mean)) +
  geom_ribbon(aes(ymin = mean - 2*sd, ymax = mean + 2*sd), colour = "lightgrey", alpha = .1) +
  geom_line(aes(group = Condition.Exposure, color = Condition.Exposure), size = 1) +
  scale_x_continuous("Trial", breaks = c(12, 24, 36, 48)) +
  scale_y_continuous("mean log-RT (log10 of ms)") +
  coord_trans(y = "log10") +
      scale_color_manual("Exposure condition",
                      aesthetics = c("colour", "fill"),
                      labels = c("baseline", "+10", "+40"),
                      values = colours.condition) +
  facet_grid(Condition.Exposure~ Block, scales = "free_x", space = "free_x") +
  theme(legend.position = "top")
```


## Additional information about the Bayesian psychometric mixed-effects model {sec:analysis-approach}
We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric model [@prins2011]. The psychometric model is an extension of mixed-effects logistic regression that also takes into account attentional lapses.

The mixed-effects psychometric model describes the probability of "t"-responses as a weighted mixture of a perceptual and a lapsing model. The perceptual model predicts responses on trials where participants pay attention and respond based on the stimulus. We implemented the perceptual model as used mixed-effects logistic regression, predicting "t"-responses from exposure condition (backward difference coded, comparing the +10ms against the +0ms shift condition, and the +40ms against the +10ms shift condition), test block (backward difference coded from the first to last test block), VOT (Gelman scaled), and their full factorial interaction. The model included by-participant random intercepts and slopes for all within-participant maniplations (block and VOT) and by-item random intercepts and slopes for all within-participant manipulations (exposure condition, block, VOT).

The lapsing model predicts participant responses that are made independent of the stimulus---for example, responses that result from attentional lapses. These responses depend only on participants' response bias. We used mixed-effects logistic regression with only a population-level intercept, allowing non-uniform responses bias but assuming that response biases did not vary across participants. Finally, the relative weight of the perceptual and lapsing model is determined by the lapse rate. We again used mixed-effects logistic regression with only a population-level intercept, inferring lapse rates from that data while assuming that lapse rates did not vary across participants or blocks. This assumption is validated by Figures \@ref(fig:plot-fit-intercept-slope-PSE)A-B, which shows that participants' responses (the point ranges) in all exposure and test blocks approach 0 and 100% "t"-responses for small and large VOTs, respectively [unlike in @kleinschmidt2020]. One possible explanation for these consistently small lapse rate [also compared to @clayards2008] is that the present experiment recruited participants from the Prolific platform, which has been found [@REFS] to deliver higher data quality than the Amazon's Mechanical Turk crowdsourcing platform used by @kleinschmidt-jaeger2016 and @kleinschmidt2020.

We fit the psychometric model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. To faciltiate comparison of effect sizes across predictors, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008]. Following previous work from our lab [@horberg2021; @xie2021], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we used Student priors centered around zero with a scale of 2.5 units [following @gelman-prior2008] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $1 < \hat{R} < 1.01$.

# Obtaining predictions of idealized listeners
In the main text, we employ ideal observers [specifically, the ASP framework described in @xie2023] to relate our results to those expected from idealized listeners. This includes predictions based on a 'typical' listener's prior experience and predictions based on idealized learners for each exposure condition that have fully learned the exposure distributions. The former provides a baseline against which to compare listeners' behavior at the start of the experiment. The latter provides a baseline against which to compare changes in listeners' behavior as a function of exposure. In this section, we describe how we derived those predictions.




## Idealized pre-exposure listeners {sec:prior-idealized-listeners}
We follow other work in deriving estimates of listeners' prior expectations about the realization of phonetic categories from a linguistic database [e.g., @feldman2009; @kronrod2016; @norris-mcqueen2008; @persson-jaeger2023; @tan2021; for a helpful review, see @schertz-clare2020]. Such estimates are only as good as the assumptions that they are based on. One important pre-requisite for a reasonable estimate is that the phonetic database employed to estimate listeners' prior experience sufficiently closely approximates the type of input a typical adult listener of the language would have previously experienced. Here, we considered two databases to approximate listeners' prior expectations about the phonetic realization of /d/ and /t/ by a female L1 talkers of US English (as the one employed in our experiment). The two databases contain productions of word-initial /d/ and /t/ in isolated word productions and in connected read speech, respectively [@chodroff-wilson2017], and available by Eleanor Chodroff on OSF at https://osf.io/k6djr/ ^[We thank Eleanor Chodroff for making these data available, and for prompt and helpful responses to our questions about the data.] The subsets of the data employed in the present study---described in more detail below---are available via the OSF for our project.

Committing to a database is, however, not a *sufficient* assumption. Estimates of listeners' prior expectations also depend strongly on assumptions about how listeners represent the mapping from phonetic cues to phonetic categories. For example, previous work has often derived estimates under the implicit assumption that listener learn and maintain a *single* representation for each phonetic category across talkers. This assumption typically has taken one of two forms. One approach is to entirely ignore talker identity during the estimation of phonetic categories. This is illustrated in Figure \@ref(plot-database-pairwise-cue-correlations) which shows the distribution of /d/ and /t/ over VOT, f0, and vowel duration in isolated (top left) and connected (bottom left) speech. Ellipses indicate the phonetic category representations that would be estimated from these data. <!-- TO DO: make 3d plot, f0 is mel; use color for /d/ vs. /t/; fat point for mean, transparent ellipses for 95% multivariate gaussian. Caption should say this. Figure has multiple cols A, B --> Another approach is to first normalize each cue within each talker---e.g., by subtracting the talker's cue mean from each token [e.g., @mcmurray2011; @mcmurray-jongman2011; for review, see @apfelbaum-mcmurray2015; @weatherholtz-jaeger2016]. Phonetic categories are then estimated over these normalized cues. The resulting representations are shown in Figure \@ref(plot-database-pairwise-cue-correlations)C and \@ref(plot-database-pairwise-cue-correlations)D . 

The two approaches have different trade-offs. When cues are not talker-normalized, the resulting category representations inherit not only within-category variability but also variability across talker, over-estimating the actual category covariances for any given talker. Normalizing cues addresses this problem. It does, however, remove potentially useful information about the correlation between cues from the signal: compared to unnormalized cues in Panel A, the talker-normalized cues in Panel B contain less information about the covariance of VOT, f0, and vowel duration.

The two approaches also share potentially important shortcomings. Specifically, neither approach is well-suited if the correlation between cues *within* talkers differs from the correlation between talker means of those cues. The /d/ and /t/ categories in Panel A conflate the two source of covariance; the categories in Panel B completely removed any information about covariance in talker means. This is problematic, as there is strong evidence that such correlations exists [@chodroff-wilson2017; @chodroff-wilson2019; @theodore2009; @sonderegger2020], and that listeners have strong expectations about, at least some types of, correlations between talker means [@idemaru-holt2011; @idemaru-holt2020; @schertz; @OTHERs].^[Additionally, the category means for /d/ and /t/ (solid points) for unnormalized cues are the simply the weighted average of the talkers available in the data. If talkers contribute different amounts of data to the database, this means that the category means might disproportionately depend on a subset of the data.]. This issue is illustrated in Figure XXX, which shows that correlation between phonetic cues *within* talkers (Panel A) and the correlations of talker means of the phonetic cues *across* talkers (Panel B).<!-- TO DO: add figure. think about visualization -->  XXX <!-- TO DO: add description . NOTE that even when those two types of correlations are similar, one would have to somehow USE that knowledge when generalizing prior expectations to a new talker. --> For the same reasons, neither of the two approaches considered above captures correlations between talkers' category means and category (co)variances. Both approaches would, for example, miss if category variance generally increases for larger category means (or vice versa).

```{r}
p.database <- d.chodroff_wilson %>% 
  group_by(category) %>%
  ggplot(aes(x = .panel_x, y = .panel_y, color = category, fill = category)) +
  geom_autopoint(alpha = .1) +
  geom_autodensity(aes(), position = "identity", alpha = .5) +
  stat_ellipse(
    aes(
      group = interaction(Talker, category)),
    alpha = .1) +
  stat_ellipse() +
  geom_autopoint(
    data = d.for_analysis %>% 
      filter(Phase == "test") %>% distinct(VOT, f0_Mel, vowel_duration) %>% 
      mutate(category = "test"),
    color = "black", alpha = .5, size = 1, inherit.aes = F) +
  geom_autopoint(
    data = d.for_analysis %>%
      select(-c(VOT, f0_Mel, vowel_duration)) %>%
      rename(VOT = VOT.CCuRE, f0_Mel = f0_Mel.CCuRE, vowel_duration = vowel_duration.CCuRE) %>%
      filter(Phase == "test") %>% distinct(VOT, f0_Mel, vowel_duration) %>%
      mutate(category = "test"),
    color = "gray", alpha = .5, size = 1, inherit.aes = F) +
  scale_color_manual(
    "Category",
    labels = c("/d/", "/t/"),
    values = colours.category,
    aesthetics = c("color", "fill")) +
  facet_matrix(vars(c(VOT, f0_Mel, vowel_duration)), layer.lower = c(3, 4:5), layer.diag = 2,
               layer.upper = c(1, 4:5),
               labeller = labeller(.rows = c(VOT = "VOT", f0_Mel = "F0 (Mel)", vowel_duration = "Vowel duration"),
               .cols = c(VOT = "VOT (ms)", f0_Mel = "f0 (Mel)", vowel_duration = "Vowel duration (ms)"))) +
  theme(legend.position = "top")

p.matrix_isolated <- p.database %+%  
  subset(d.chodroff_wilson %>% filter(speechstyle == "isolated")) +
  theme(strip.text.y = element_blank())

p.matrix_connected <- p.database %+%  
  subset(d.chodroff_wilson %>% filter(speechstyle == "connected")) +
  theme(strip.text.x = element_blank(),
        strip.text.y = element_blank())
```

```{r}
# make a normalised version of the plots 
d.chodroff_wilson.connected.norm <-
  d.chodroff_wilson %>%
  filter(speechstyle == "connected") %>% 
  mutate(
    across(
      c("VOT", "f0_Mel", "vowel_duration"),
      function(x) apply_ccure(data = ., cue = substitute(x)))) %>%
  ungroup()

d.chodroff_wilson.isolated.norm <-
  d.chodroff_wilson %>%
  filter(speechstyle == "isolated") %>% 
  mutate(
    across(
      c("VOT", "f0_Mel", "vowel_duration"),
      function(x) apply_ccure(data = ., cue = substitute(x)))) %>%
  ungroup()

d.chodroff_wilson.norm <-
  bind_rows(
    d.chodroff_wilson.connected.norm,
    d.chodroff_wilson.isolated.norm)

p.database.norm <- d.chodroff_wilson.norm %>% 
  group_by(category) %>%
  ggplot(aes(x = .panel_x, y = .panel_y, color = category, fill = category)) +
  geom_autopoint(alpha = .1) +
  geom_autodensity(aes(), position = "identity", alpha = .5) +
  stat_ellipse(
    aes(
      group = interaction(Talker, category)),
    alpha = .1) +
  stat_ellipse() +
  geom_autopoint(
    data = d.for_analysis %>% 
      filter(Phase == "test") %>% distinct(VOT, f0_Mel, vowel_duration) %>% 
      mutate(category = "test"),
    color = "black", alpha = .5, size = 1, inherit.aes = F) +
  geom_autopoint(
    data = d.for_analysis %>%
      select(-c(VOT, f0_Mel, vowel_duration)) %>%
      rename(VOT = VOT.CCuRE, f0_Mel = f0_Mel.CCuRE, vowel_duration = vowel_duration.CCuRE) %>%
      filter(Phase == "test") %>% distinct(VOT, f0_Mel, vowel_duration) %>%
      mutate(category = "test"),
    color = "gray", alpha = .5, size = 1, inherit.aes = F) +
  scale_color_manual(
      "Category",
      labels = c("/d/", "/t/"),
      values = colours.category,
      aesthetics = c("color", "fill")) +
  facet_matrix(vars(c(VOT, f0_Mel, vowel_duration)), layer.lower = c(3, 4:5), layer.diag = 2,
               layer.upper = c(1, 4:5),
               labeller = labeller(.rows = c(VOT = "VOT", f0_Mel = "f0 (Mel)", vowel_duration = "Vowel duration"),
               .cols = c(VOT = "VOT (ms)", f0_Mel = "f0 (Mel)", vowel_duration = "Vowel duration (ms)"))) +
  theme(legend.position = "top") 

p.matrix_isolated.norm <- p.database.norm %+%  
  subset(d.chodroff_wilson.norm %>% filter(speechstyle == "isolated")) 

p.matrix_connected.norm <- p.database.norm %+%  
  subset(d.chodroff_wilson.norm %>% filter(speechstyle == "connected")) +
  theme(strip.text.x = element_blank())
```

(ref:plot-database-pairwise-cue-correlations) Distribution of L1 US-English talker cues against test tokens, unnormalized (black) and normalized (gray). Test tokens are normalized according to overall cue means of both corpora.  **Panel A:** Isolated speech of initial /d/ and /t/ syllables by 13 females. **Panel B:** Same as **A** but normalised. **Panel C:** Connected speech of word-initial /d/ and /t/ by 102 females. **Panel D:** Same as **C** but normalised within talker. In  **B** and **D** the mean of talker means was added to each token in order to keep tokens in the familiar phonetic space [following @xie2023]

```{r plot-database-pairwise-cue-correlations, fig.height=8.5, fig.width=10}
(p.matrix_isolated + p.matrix_isolated.norm) / (p.matrix_connected + p.matrix_connected.norm) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") + 
  plot_layout(guides = "collect") &
  theme(legend.position = "top",
        legend.justification = "right",
        legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold"))
```



### DMMM: A distributional multivariate mixed-effects model
Here we use a distributional multivariate mixed-effects model (DMMM) to fit the data from both isolated and connected speech, separately for /d/ and /t/. The model fits not only the mean but higher-level statistical moments of the *distribution* of cues for each category. Specifically, we assume /d/ and /t/ categories that are normally distributed with unknown mean and variance but also can exhibit unknown skew. The ability to model skew should help to fit distributions along cues with soft or hard bounds, such VOT [especially, if 'negative VOTs' are considered the combination of a VOT of 0 and the presence of a second cue, pre-voicing, @REFS].

The model is multivariate because we fit the data simultaneously to all three cues (VOT, f0, and vowel duration). This allows us to capture correlations between these cues, as well as correlations between any of the parameters of the talker-specific cue distributions for each category (e.g., the category means and variances).^[Since we fit separate models to each category, our implementation only captures such cross-talker correlations *within* each category, but not those that are known to exist *across* categories [@REF]. We made this decision since current implementations of DMMMs assume that residual (population-level) correlations between cues are identical across the data. As Figure XXX above shows, <!-- TO DO: figure that shows pairwise cue correlations --> this is clearly not the case for the, for instance, the correlation between VOT and vowel duration across /d/ and /t/.]

Finally, the model is a multilevel or mixed-effects model since we recognize the hierarchical grouping structure of the data (talkers).

```{r DMMM-setup}
d.chodroff_wilson.for_analysis <-
  d.chodroff_wilson %>%
  select(Talker, category, VOT, f0_Mel, vowel_duration) %>%
  # Remove pitch-halving and extreme vowel duration outlier
  # (vowel duration is also truncated at > 30ms because of the automatic alignment that
  # was used by Chodroff and Wilson to get those measures)
  filter(f0_Mel > 200, vowel_duration < 450)

scale_f0 <- scale(d.chodroff_wilson.for_analysis$f0_Mel)
scale_vowel_duration <- scale(d.chodroff_wilson.for_analysis$vowel_duration)
d.chodroff_wilson.for_analysis %<>%
  mutate(
    across(
      c(VOT, f0_Mel, vowel_duration),
      ~ (.x - mean(.x, na.rm = T)) / (2 * sd(.x, na.rm = T))))

# Recognizing that f0 was censored at lower end
# Recognizing that vowel duration was censored at lower and upper end
bounds <- list()
bounds[["f0_Mel"]] <- range(d.chodroff_wilson.for_analysis$f0_Mel) * 1.0001
bounds[["vowel_duration"]] <- range(d.chodroff_wilson.for_analysis$vowel_duration) * 1.0001

my.priors.DMMM <-
  c(
    # Uncomment if we'd like to include speechstyle as a predictor, or if we'd like to fit
    # the data from both categories in one model (in which case, we'd need to add category
    # as predictor)
    #
    # MU: COEFFICIENTS IN LINEAR PREDICTOR
    prior(student_t(3, 0, 2.5), class = b, resp = VOT, dpar = mu),
    prior(student_t(3, 0, 2.5), class = b, resp = f0Mel, dpar = mu),
    prior(student_t(3, 0, 2.5), class = b, resp = vowelduration, dpar = mu),
    prior(cauchy(0, 2.5), class = sd, resp = VOT, dpar = mu),
    prior(cauchy(0, 2.5), class = sd, resp = f0Mel, dpar = mu),
    prior(cauchy(0, 2.5), class = sd, resp = vowelduration, dpar = mu),
    # SIGMA: VARIANCE OF RANDOM EFFECTS
    prior(student_t(3, 0, 2.5), class = b, resp = VOT, dpar = sigma),
    prior(student_t(3, 0, 2.5), class = b, resp = f0Mel, dpar = sigma),
    prior(student_t(3, 0, 2.5), class = b, resp = vowelduration, dpar = sigma),
    prior(cauchy(0, 2.5), class = sd, resp = VOT, dpar = sigma),
    prior(cauchy(0, 2.5), class = sd, resp = f0Mel, dpar = sigma),
    prior(cauchy(0, 2.5), class = sd, resp = vowelduration, dpar = sigma),
    prior(student_t(3, 0, 2.5), class = b, resp = VOT, dpar = alpha),
    prior(student_t(3, 0, 2.5), class = b, resp = vowelduration, dpar = alpha),
    # prior(cauchy(0, 2.5), class = sd, resp = VOT, dpar = alpha),
    # prior(cauchy(0, 2.5), class = sd, resp = vowelduration, dpar = alpha),
    # COR: CORRELATION OF RANDOM EFFECTS
    prior(lkj(1), class = cor)
)

# TO DO: consider using a gaussian after all.
# formula + set_rescor(rescor = TRUE) is only implemented for gaussian and student models
#
# holding grouping index p constant across mu, sigma, alpha allows us to fit correlations
# across statistical moments (mu, log sigma, and skew). Fitting across both categories
# allows us to do so across categories.
bf_VOT <-
  bf(VOT ~ 1 + category + (1 + category | p | Talker)) +
  lf(sigma ~ 1 + category + (1 + category | q | Talker)) +
  # lf(alpha ~ 1 + category + (1 + category | o | Talker)) +
  lf(alpha ~ 1 + category) +
  brmsfamily(
    "skew_normal",
    link = "identity",
    link_sigma = "log",
    link_alpha = "identity")

bf_f0 <-  
  # bf(paste0("f0_Mel | trunc(lb = ", bounds[["f0_Mel"]][1], ") ~ 1 + category + (1 + category | p | Talker)")) +
  bf(f0_Mel ~ 1 + category + (1 + category | p | Talker)) +
  lf(sigma ~ 1 + category + (1 + category | q | Talker)) +
  brmsfamily(
    "gaussian",
    link = "identity",
    link_sigma = "log")

bf_vowel_duration <-  
  # bf(paste0("vowel_duration | trunc(lb = ", bounds[["vowel_duration"]][1], ", ub = ", bounds[["vowel_duration"]][2], ") ~ 1 + category + (1 + category | p | Talker)")) +
 bf(vowel_duration ~ 1 + category + (1 + category | p | Talker)) +
  lf(sigma ~ 1 + category + (1 + category | q | Talker)) +
#  lf(alpha ~ 1 + category + (1 + category | o | Talker)) +
  lf(alpha ~ 1 + category) +
  brmsfamily(
    "skew_normal",
    link = "identity",
    link_sigma = "log",
    link_alpha = "identity")

inits <- 
  
  list(
    b_alpha_VOT_Intercept = 5,
    b_alpha_VOT_categoryD = 1,
    b_alpha_vowelduration_Intercept = 5,
    b_alpha_duration_categoryD = 1,
  )

# TO DO: consider using *all* talkers here (currently subset to female talkers)
# (inclusion of male talkers would require taking care of pitch-halving separately
# for both sexes, and it probably would require including talker sex in the f0 model)
contrasts(d.chodroff_wilson.for_analysis$category) <- cbind("D" = c(-1, 1))
DMMM <-
  brm(
  formula = bf_VOT + bf_f0 + bf_vowel_duration + set_rescor(rescor = FALSE),
  data = d.chodroff_wilson.for_analysis,
  prior = my.priors.DMMM,
  chains = 4,
  cores = 4,
  warmup = 1500,
  iter = 4000,
  thin = 1,
  threads = threading(threads = 2),
  control = list(adapt_delta = .85, max_treedepth = 15))

save_model = "../models/DMMM.stan"
saveRDS(DMMM, file = "../models/DMMM-attempt4.rds")
DMMM <- readRDS(file = "../models/DMMM-attempt3.rds")
```

```{r checking-posterior-predictive}
pp_check(DMMM, resp = "VOT")
pp_check(DMMM, resp = "f0Mel")
pp_check(DMMM, resp = "vowelduration")
```

```{r draw-samples-from-DMMM}
# 1. Get estimates of each talker model. This could be a. or b.
#   a. use mean/median estimates
#   b. marginalize over all parameters that go into each talker model.
# 2. Calculate marginal density of test tokens under each (draw of each) talker model. 
#    This could be done in a number of ways:
#   a. for each test token, take density under category that maximizes the density
#   b. for each test token, take average density by weighting density under each 
#      category proporotional to the density of the observation under that category.
#   c. ...
#   
# OR select model based on stimulus *mean* (mean generated under talker model is most 
# similar to currently observed mean)  
#   
# 3. Calculate categorization function over test tokens conditional on the relative 
#    probability of the different talker models, by either
#   a. selecting the talker model that achieved the highest density, or 
#   b. weighting each talker model relative to the density the test tokens have under 
#      that model.
s.DMMM <-
  DMMM %>%
  spread_draws(
    # fixed effects for mu, sigma, and alpha 
    !!! syms(pmap_chr(
      expand_grid(
        parameter = c("b", "b_sigma", "b_alpha"), 
        cue = c("VOT", "f0Mel", "vowelduration"), 
        coef = c("Intercept", "categoryD")) %>%
        # Filter an variables for which alpha was not modeled
        filter(!(cue == "f0Mel" & parameter == "b_alpha")),
      ~ paste(..., sep = "_"))),
    # random effects for mu, sigma, and alpha
    r_Talker__VOT[TalkerID, coef],
    r_Talker__f0Mel[TalkerID, coef],
    r_Talker__vowelduration[TalkerID, coef],
    r_Talker__sigma_VOT[TalkerID, coef],  
    r_Talker__sigma_f0Mel[TalkerID, coef],
    r_Talker__sigma_vowelduration[TalkerID, coef]) %>%
  # The followoing only applies the indices to the first element to the list of random effect names
  # { !!! syms(pmap_chr(
  #   expand_grid(
  #     parameter = c("r_Talker_", "r_Talker__sigma", "r_Talker__alpha"), 
  #     cue = c("VOT", "f0Mel", "vowelduration")) %>%
  #     # Filter an variables for which *random effects* of alpha were not modeled
  #     filter(!(cue %in% c("VOT", "f0Mel", "vowelduration") & parameter == "r_Talker__alpha")),
  #   ~ paste(..., sep = "_"))) }[TalkerID, coef] 
  # Spread the random effects
  pivot_wider(
    values_from = starts_with("r_"),
    names_from = coef) %>%
  # Add random effects to fixed effects 
  # (and add 0 if no random effect matching the fixed effect is found)
  mutate(
    across(
      starts_with("b_"),
      function(x) {
        r_name <- gsub("^b", "r_Talker_", cur_column())
        r_value <- if (r_name %in% names(.)) get(r_name) else 0
        x + r_value 
      })) %>%
  select(-starts_with("r_")) %>%
  # Get the predicted values for all category parameters
  mutate(
    across(
      ends_with("Intercept"),
      list(
        "catD" = function(x) {
          r_name <- gsub("Intercept", "categoryD", cur_column())
          x + get(r_name) 
        },
        "catT" = function(x) {
          r_name <- gsub("Intercept", "categoryD", cur_column())
          x - get(r_name) 
        }),
      .names = "{.col}_{.fn}")) %>%
  select(TalkerID, .chain, .draw, ends_with(c("catD", "catT"))) %>%
  rename_with(~ gsub("Intercept_", "", .x)) %>%
  rename_with(~ gsub("b_(VOT|f0Mel|vowelduration)", "b_mu_\\1", .x)) 

s.DMMM %>%
  # Add sampling functions for all three cues
  mutate(
    marginal_VOT = 
      pmap(
        list(b_mu_VOT_catD, b_sigma_VOT_catD, b_alpha_VOT_catD, b_mu_VOT_catT, b_sigma_VOT_catT, b_alpha_VOT_catT),
        ~ function(muD, sigmaD, alphaD, muT, sigmaT, alphaT) {
          dD <- dskew_normal(x, mu = muD, sigma = sigmaD, alpha = alphaD)
          dT <- dskew_normal(x, mu = muT, sigma = sigmaT, alpha = alphaT)
          (dD + dT)
          }),
    posterior_VOT_T = 
      pmap(
        list(b_mu_VOT_catD, b_sigma_VOT_catD, b_alpha_VOT_catD, b_mu_VOT_catT, b_sigma_VOT_catT, b_alpha_VOT_catT),
        ~ function(muD, sigmaD, alphaD, muT, sigmaT, alphaT) {
          dD <- dskew_normal(x, mu = muD, sigma = sigmaD, alpha = alphaD)
          dT <- dskew_normal(x, mu = muT, sigma = sigmaT, alpha = alphaT)
          dT / (dD + dT)
          }),
    marginal_f0Mel = 
      pmap(
        list(b_mu_f0Mel_catD, b_sigma_f0Mel_catD, b_mu_f0Mel_catT, b_sigma_f0Mel_catT),
        ~ function(muD, sigmaD, muT, sigmaT) {
          dD <- dnorm(x, mu = muD, sigma = sigmaD)
          dT <- dnorm(x, mu = muT, sigma = sigmaT)
          (dD + dT)
          }),
    posterior_f0Mel_T = 
      pmap(
        list(b_mu_f0Mel_catD, b_sigma_f0Mel_catD, b_mu_f0Mel_catT, b_sigma_f0Mel_catT),
        ~ function(muD, sigmaD, muT, sigmaT) {
          dD <- dnorm(x, mu = muD, sigma = sigmaD)
          dT <- dnorm(x, mu = muT, sigma = sigmaT)
          dT / (dD + dT)
          }),
    marginal_vowelduration = 
      pmap(
        list(b_mu_vowelduration_catD, b_sigma_vowelduration_catD, b_alpha_vowelduration_catD, b_mu_vowelduration_catT, b_sigma_vowelduration_catT, b_alpha_vowelduration_catT),
        ~ function(muD, sigmaD, alphaD, muT, sigmaT, alphaT) {
          dD <- dskew_normal(x, mu = muD, sigma = sigmaD, alpha = alphaD)
          dT <- dskew_normal(x, mu = muT, sigma = sigmaT, alpha = alphaT)
          (dD + dT)
          }),
    posterior_vowelduration_T = 
      pmap(
        list(b_mu_vowelduration_catD, b_sigma_vowelduration_catD, b_alpha_vowelduration_catD, b_mu_vowelduration_catT, b_sigma_vowelduration_catT, b_alpha_vowelduration_catT),
        ~ function(muD, sigmaD, alphaD, muT, sigmaT, alphaT) {
          dD <- dskew_normal(x, mu = muD, sigma = sigmaD, alpha = alphaD)
          dT <- dskew_normal(x, mu = muT, sigma = sigmaT, alpha = alphaT)
          dT / (dD + dT)
          })) %>%
    select(TalkerID, .chain, .draw, ends_with(c("_D", "_T")))
```

advantages for regression-based approach fitting categories
- understands structure of data; doesn't assume balanced data
- weights talkers with less data appropriate.
- considers cues relative to the category the come from
- can model correlation of category means across cues and talkers, while modeling correlation within categories separately (and separately for each category, if fit separately to each category)
- can model correlation of category variances across cues and talkers
- can generate hypotheses about new talkers



### Phonetic database [@chodroff-wilson2017]
The production data we used in our analysis is a subset of two annotated phonetic databases from @chodroff-wilson2017 (downloadable at https://osf.io/k6djr/). The databases differed in size and the way the productions were elicited; the larger of the two was made up of 180 adult L1-US English talkers (102 female) producing stop-initial words in connected speech  while the smaller was made up of recordings of isolated utterances of stop-initial CVC syllables by 24 L1-US English talkers (13 female) [see @chodroff-wilson2017 pp. 33; 37- 39 for details]. For the connected speech database we filtered the databases to /d/ and /t/ tokens and removed tokens that showed pitch-doubling by running a dip test for bimodality. We also restricted the talkers to those with a minimum of 10 tokens per category. 
For the isolated database we removed talkers with no data (3 talkers), tokens that were not stop-initial utterances, tokens that had anomalous labeling (e.g. "xxxGOOT" or "POAT0"), and tokens with NA for f0 measurements. We filtered the data to only female talkers and removed tokens with f0s less than 150 Hz (as advised by Eleanor Chodroff, these tokens were likely due to measurement error). 
For the combined databases we ensured that the talkers we sampled had sufficient amount of data and that the number of tokens from each category were balanced. We then took only the female talkers to match the gender of our test talker. This left us with a total of 5,122 tokens from 40 talkers: 4,324 tokens by 27 female talkers from the connected speech database and 798 tokens by 13 female talkers from the isolated speech database.

## Idealized learners that have fully learned the exposure distribution
For each exposure condition we constructed ideal observers to simulate a learner that fully learned the exposure distribution (i.e. the cumulative distribution after undergoing all 144 trials) and who did not suffer from attentional lapses. Such an ideal learner would therefore categorize the stimuli at test based purely on the statistics of exposure input. The optimal categorizations of the test stimuli by each of these IOs were obtained following Bayes' rule under the assumption of equal prior probability for both categories. Following this we multiplied each IOs' responses at each stimulus to simulate categorization over a large number of trials and fitted three separate logistic regressions to the responses with VOT as the sole predictor. We obtained the predicted PSEs from each of the linear models which represent the ideal boundary of each exposure condition. These converged with the boundaries computed from the parameters of the Gaussian distributions that underlie each category within each condition. 


```{r IO-predicted-PSE-bias-correction}
io <-    
  make_VOT_IOs_from_exposure(
    d.for_analysis %>% 
      filter(Phase == "exposure") %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      # Subset data to a single participant per exposure condition. This is sufficient since 
      # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics 
      # (mean and SD) at the end of the experiment, which were identical across participants 
      # in each condition. 
      # Note that this approach trains the ideal observers on the *actual* VOT distribution 
      # that participants heard, not on the theoretical distributions these VOTs were sampled
      # from. This is in line with our goal to simulate behavior of an idealized participant
      # who has fully learned the exposure distributions.
      slice_sample(n = 1) %>% 
      unnest(data) %>% 
      mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))) %>% 
  crossing(Phase = c("test", "exposure")) %>% 
      # only VOT is used for categorising stimuli
  left_join(
    bind_rows(
      d.for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Phase, Item.VOT) %>% 
        rename(x = Item.VOT),
      # get the unlabeled exposure stimuli and their 3 cues
      # Since blocks differ in which VOTs are unlabeled, we include all blocks
      # (we are simulating the expected parameters across all exposure blocks)
      d.for_analysis %>%  
        filter(Phase == "exposure", Item.Labeled == F) %>%
        group_by(Condition.Exposure) %>%
        filter(ParticipantID == first(ParticipantID)) %>%
        select(Condition.Exposure, Phase, Item.VOT) %>% 
        rename(x = Item.VOT)) %>% 
      select(Condition.Exposure, Phase, x) %>% 
      nest(x = x)) %>% 
  # Add prior based on Chodroff & Wilson (2018). Note that this database has substantially higher 
  # VOT variance than our exposure conditions, which we based on estimates from Kronrod et al. 
  # (2016). Uses all three cues; duration and spectral noise taken from Kronrod et al.
  bind_rows(
    d.chodroff_wilson.isolated %>%
      make_MVG_ideal_observer_from_data(
        cues = c("VOT", "f0_Mel", "vowel_duration"),
        Sigma_noise = matrix(c(80, 0, 0, 0, 878, 0, 0, 0, 80), nrow = 3, dimnames = list(c("VOT", "f0_Mel", "vowel_duration"), c("VOT", "f0_Mel", "vowel_duration")))) %>%
      mutate(Condition.Exposure = "prior") %>% 
      # join in the test stimuli. All 3 cues are used here because we assume listeners' prior experience would take into account all cues
      left_join(
        bind_rows(
          # get the unique test stimuli and their 3 cues
          d.for_analysis %>%  
            filter(Phase == "test") %>%
            distinct(Condition.Exposure, Phase, Item.VOT, Item.f0_Mel, Item.vowel_duration) %>% 
            mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.vowel_duration), ~ c(..1, ..2, ..3))) %>% 
            select(Condition.Exposure, Phase, x), 
          # get the unlabeled exposure stimuli and their 3 cues
          # Since blocks differ in which VOTs are unlabeled, we include all blocks
          # (we are simulating the expected parameters across all exposure blocks)
          d.for_analysis %>%  
            filter(Phase == "exposure", Item.Labeled == F) %>%
            group_by(Condition.Exposure) %>%
            filter(ParticipantID == first(ParticipantID)) %>%
            select(Condition.Exposure, Phase, Item.VOT, Item.f0_Mel, Item.vowel_duration) %>% 
            mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.vowel_duration), ~ c(..1, ..2, ..3))) %>% 
            select(Condition.Exposure, Phase, x)) %>% 
          mutate(Condition.Exposure = "prior") %>% 
          nest(x = x)) %>% 
      nest(io = -c(Condition.Exposure, Phase, x)))  

# Get logistic parameter estimates for a simulated ideal observer listener. Do so for both 
# exposure and test. Even though this turned out to be overkill, we did it because it was 
# theoretically possible that the procedure we use to analyze *listeners'* responses (a 
# logistic model with a linear effect of VOT) introduces a bias into the estimation. Here, 
# we thus apply a variant of that same analysis approach to responses that are sampled from
# the three ideal observer fit on the exposure data (one each per exposure condition).
# 
# Unlike for the analysis of participants' responses, there is no need to use a psychometric
# model since ideal observers have not attentional lapses (i.e., lambda = 0). 
d.io.categorization <- 
  rbind(
  # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io %>% 
        filter(Phase == "test"), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "test") %>% 
      crossing(Block = c(1, 3, 5, 7, 8, 9)), 
    # Assess the ideal observers on the VOT locations that occur in the three different 
    # exposure conditions. Only use unlabeled exposure trials, since those are the trials
    # we assess participants' behavior on.
    get_logistic_parameters_from_model(
      io %>% 
        filter(Phase == "exposure"),
      model_col = "io",
      groups = "Condition.Exposure") %>%
      mutate(Phase = "exposure") %>%
      crossing(Block = c(2, 4, 6))) %>%
  select(Condition.Exposure, Phase, data, model_unscaled, Block, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>% 
  mutate(across(c(Phase, Condition.Exposure, Block), factor)) 
```
(ref:plot-IO-logistic-regression-fit) The expected categorization functions predicted by the logistic regression of responses by idealized listeners by exposure condition and phase (solid lines). The ideal listeners' proportion of "t"-responses (dashed lines) converge with the logistic regression fit along the assessed VOT region and are therefore partially obscured by the logistic regression fitted lines. 
```{r plot-IO-logistic-regression-fit}
tokens <- 
  bind_rows(
      d.for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Phase, Item.VOT) %>% 
        rename(VOT = Item.VOT),
      # get the unlabeled exposure stimuli and their 3 cues
      # Since blocks differ in which VOTs are unlabeled, we include all blocks
      # (we are simulating the expected parameters across all exposure blocks)
      d.for_analysis %>%  
        filter(Phase == "exposure", Item.Labeled == F) %>%
        group_by(Condition.Exposure) %>%
        filter(ParticipantID == first(ParticipantID)) %>%
        select(Condition.Exposure, Phase, Item.VOT) %>% 
        rename(VOT = Item.VOT)) %>% 
      select(Condition.Exposure, Phase, VOT)

arrow_df <- d.io.categorization %>% 
  filter(Condition.Exposure != "prior") %>% 
  select(Condition.Exposure, Phase, PSE) %>%
  group_by(Condition.Exposure, Phase) %>% 
  slice_head(n = 1) %>% 
  mutate(x = PSE, xend = PSE, y = .5, yend = .01)  

d.io.categorization %>% 
  filter(Condition.Exposure != "prior") %>% 
  select(Condition.Exposure, Phase, model_unscaled) %>% 
  group_by(Condition.Exposure, Phase) %>% 
  slice_head(n = 1) %>% 
  # append VOT values for models to predict
  cross_join(
    tibble(
      VOT = seq(-50, 135, 5))) %>% 
  nest(stimuli = VOT) %>% 
  mutate(prediction = map2(.x = stimuli, .y = model_unscaled, ~ predict(.y, newdata = .x))) %>% 
  unnest(c(stimuli, prediction)) %>% 
  mutate(proportion_t_responses = plogis(prediction)) %>% 
  ggplot(aes(x = VOT, y = proportion_t_responses, colour = Condition.Exposure)) +
  geom_line(size = 1) + 
  geom_line(
    data = io %>% 
  filter(Condition.Exposure != "prior") %>% 
  select(-x) %>% 
  group_by(Condition.Exposure, Phase) %>% 
  cross_join(
    tibble(x = seq(-50, 130, 5))) %>% 
  nest(x = x) %>% 
  mutate(categorisation = 
           map2(x, io, ~ 
                  get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% 
                  mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>% 
  unnest(categorisation, names_repair = "unique") %>% 
  filter(category == "/t/"),
  mapping = aes(x = VOT, y = response, colour = Condition.Exposure),
  alpha = .5,
  size = 1,
  linetype = 2,
  inherit.aes = F) +
  geom_rug(
    data = tokens %>% group_by(Condition.Exposure, Phase),
    mapping = aes(x = VOT, colour = Condition.Exposure),
    alpha = .6,
    size = .8,
    inherit.aes = F) +
  geom_segment(data = arrow_df,
          mapping = aes(x = x, xend = xend, y = y , yend = yend, group = Phase),
           colour = "grey",
          alpha = 0.8,
           arrow = arrow(type = "open" , length = unit(0.04, "npc"))) +
  geom_text(data = arrow_df, 
            mapping = aes(x = x+2.5, y = .2, label = round(PSE, 2)),
            size = 2.5) +
  scale_x_continuous(breaks = seq(-50, 135, 20)) +
  scale_y_continuous("Proportion 't'-responses") +
  facet_wrap( ~ Phase, ncol = 1) +
  scale_color_manual("Exposure condition", labels = c("baseline", "+10", "+40"), values = colours.condition) +
  theme(legend.position = "top")
rm(arrow_df, tokens)
```

## Accounting for potential biases in the estimation of intercepts, slopes, and PSEs {#sec:io-bias-correction}
The perceptual model of our psychometric mixed-effects analyses assumed linear effects of VOT on the log-odds of "t"-responses. We made this assumptions for the sake of simplicity, and in order to avoid over-fitting. The ideal observers, however, predict non-linear effects of VOT [since /d/ and /t/ do *not* have equal variances; for details, see @bicknell2018; @kleinschmidt-jaeger2015; @kronrod2016]. We thus estimated the intercepts, slopes, and PSEs of ideal observers paralleling the analysis approach for the human data. This makes sure that any biases in the estimation of human intercepts, slopes, and PSEs that are introduced by our analysis approach are also taken into account for the idealized listeners (see also SI, \@ref{sec:GAMM} for a re-analysis of the human data that relaxes the linearity assumption).

<!-- TO DO: this is where we will talk about how we estimated the dashed lines in the main text. then we will present additional, more fine-grained bias correction -->


# Additional Bayesian hypothesis tests

## Test blocks

### PSE: Differences in the rate of change between exposure conditions and block
In the main text, we refer to differences in the rate of block to block changes across exposure conditions. These differences correspond to the interactions between exposure conditions and block in the psychometric mixed-effect model. These interactions are summarized in Table \@ref(tab:hypothesis-table-interaction-condition-block)

```{r hypothesis-table-interaction-condition-block, results='asis'}
hyp_interaction.condition_block <-
  hypothesis(
    fit_test,
    c(
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1) < 0",

      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 > 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",

      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",

      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",

      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 +
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 +
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",

    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 +
   mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0"),
   robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

table.interaction.condition_block <-
  make_hyp_table(
    hyp_interaction.condition_block,
    rep(c(
  # Comparing differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{PSE}$",
  "Block 2 to 3: increased $\\Delta_{PSE}$",
  "Block 3 to 4: increased $\\Delta_{PSE}$",
  "{\\em Block 1 to 4: increased $\\Delta_{PSE}$}",
  "Block 4 to 5: decreased $\\Delta_{PSE}$",
  "Block 5 to 6: decreased $\\Delta_{PSE}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{PSE}$}"), 3),
    caption = "Did the rate of block-to-block changes differ across exposure conditions? This table summarizes the interactions between exposure condition and block---specifically, whether the differences between exposure conditions changed from test block to test block.") %>%
  pack_rows("Difference in +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in +40 vs. baseline", 15, 21)

table.interaction.condition_block
```

### Slopes: Differences across exposure conditions, across blocks within each condition, and differences in the rate of change between exposure conditions and block {#sec:slopes analyses}
Analyses of changes in slopes that compare changes between blocks within conditions and interactions of exposure conditions and blocks are summarized in Tables \@ref(tab:hypothesis-table-simple-slopes-condition), \@ref(tab:hypothesis-table-simple-slopes-block), and \@ref(tab:hypothesis-table-interaction-condition-block-VOT) 

```{r hypothesis-table-simple-slopes-condition, results='asis'}
hyp.simple_slopes_condition <- 
  hypothesis(
    fit_test.simple_effects_condition, 
    c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block1:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block3:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block5:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  hyp.simple_slopes_condition, 
  c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline", "+40 vs. +10", "+40 vs. baseline"), 5)), 
    caption = "Did exposure affect the slope of categorization responses? This table summarizes the differences in slopes between the exposure conditions for each test block.") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)

rm(fit_test.simple_effects_condition)
```

```{r hypothesis-table-simple-slopes-block, results='asis'}
hyp.simple_slopes_block <- 
  hypothesis(
    fit_test.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift0:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift0:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift0:Block_Test6vs.Test5:VOT_gs > 0",
      
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift10:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift10:Block_Test4vs.Test3:VOT_gs > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift10:Block_Test6vs.Test5:VOT_gs > 0",
      
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift40:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift40:Block_Test4vs.Test3:VOT_gs > 0",
    
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift40:Block_Test6vs.Test5:VOT_gs > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
    hyp.simple_slopes_block,
    c(rep(c(
  # Comparing differences blocks within conditions
  "Block 1 to 2: decreased slope",
  "Block 2 to 3: decreased slope",
  "Block 3 to 4: decreased slope",
  "{\\em Block 1 to 4: decreased slope}",
  "Block 4 to 5: increased slope",
  "Block 5 to 6: increased slope",
  "{\\em Block 4 to 6: increased slope}"), 2),
  "Block 1 to 2: increased slope",
  "Block 2 to 3: increased slope",
  "Block 3 to 4: increased slope",
  "{\\em Block 1 to 4: increased slope}",
  "Block 4 to 5: decreased slope",
  "Block 5 to 6: decreased slope",
  "{\\em Block 4 to 6: decreased slope}"),
    caption = "Was there incremental change in slope from test block 1 to 4? Did slopes change dissipate with repeated testing from block 4 to 6? This table summarizes the simple slopes for each exposure condition between blocks.") %>% 
  pack_rows("Difference between blocks: baseline", 1, 7) %>%
  pack_rows("Difference between blocks: +10", 8, 14) %>%
  pack_rows("Difference between blocks: +40", 15, 21) 

rm(fit_test.simple_effects_block)
```

```{r hypothesis-table-interaction-condition-block-VOT, results='asis'}
hyp_interaction.condition_block_VOT <-
  hypothesis(
    fit_test,
    c(
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) > 0",

    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 +
   mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0"),
   robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

table.interactions.condition_block_VOT <-
  make_hyp_table(
    hyp_interaction.condition_block_VOT,
    rep(c(
  # Comparing slope differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{slope}$",
  "Block 2 to 3: increased $\\Delta_{slope}$",
  "Block 3 to 4: increased $\\Delta_{slope}$",
  "{\\em Block 1 to 4: increased $\\Delta_{slope}$}",
  "Block 4 to 5: decreased $\\Delta_{slope}$",
  "Block 5 to 6: decreased $\\Delta_{slope}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{slope}$}"), 3),
    caption = "Did the slope differences between exposure conditions change from block to block? This table summarizes the interactions between exposure condition and block---specifically, whether the differences in slopes between exposure conditions changed from test block to test block.") %>%
  pack_rows("Difference in slopes: +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in slopes: +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in slopes: +40 vs. baseline", 15, 21)
table.interactions.condition_block_VOT
```

## Exposure blocks {#SI-analysis-exposure-blocks}
This section reports the hypothesis tables for exposure blocks corresponding the simple effects of exposure condition and blocks (provided in the main text for test blocks), and the interaction between condition and block.

<!-- TO DO: add overall summary here, summarizing the most important parallels and differences to the results for test blocks. -->
<!-- TO DO: follow the heading format for test blocks -->

### PSE: Simple effects of exposure condition within each block
<!-- TO DO --> 


### PSE: Simple effects of block within each exposure condition
<!-- TO DO -->

### Differences in block-to block *PSE* changes across conditions
<!-- TO DO -->

### Differences in block-to block *slope* changes across conditions
<!-- TO DO -->





## Relaxing the linearity assumption {#sec:GAMM}
The perceptual model of our psychometric mixed-effects analyses assumed linear effects of VOT on the log-odds of "t"-responses. As already mentioned in SI \@ref(sec:io-bias-correction), we made this assumptions for the sake of simplicity, and in order to avoid over-fitting. However, there are reasons to expect that the effects of VOT on listeners' categorizations are non-linear. Specifically, ideal observers with Gaussian categories along a single cue dimension (here: VOT) predict that the posterior log-odds of each category change linearly along that cue dimension if and only if the variance of both Gaussian categories is equal [for details, see @bicknell2018; @kleinschmidt-jaeger2015; @kronrod2016]. For US English /d/ and /t/, this is well-known *not* to be the case, with /t/ having larger variance than /d/ (see also Figure \@ref(fig:exposure-means-database-density) in the main text). Under the assumption of Gaussian categories, ideal observer thus predict a quadratic effect along VOT---specifically, the log-odds of "t"-responses are predicted to increase more than linearly with increasing VOT between the two category means (since the /t/ category has larger variance).

<!-- While Gaussian categories are a simplifying assumption for the sake of modeling, rather than a critical assumption of the ideal adaptor framework [cf. @kleinschmidt-jaeger2015, Appendix]^[For example, the assumption of Gaussian categories leads to implausible predictions for extreme VOT values that lie outside the range of the two category means. Specifically, for VOTs smaller than the category mean of /d/, "t"-responses are predicted to eventually increase with decreasing VOT values (as a consequence of the same quadratic trend mentioned above). Additionally, distribution of VOTs for the /d/ category tends to be noticeably positively-skewed and thus non-normal.], the predicted quadratic trend is also likely to emerge in exemplar models. -->

While the linearity assumption made in our main analysis does not *necessarily* introduce statistical bias, it is possible that it does. We thus conducted additional analyses that relaxed the linearity assumption by modeling the effect of VOT as a non-parametric smooth. Since this analysis affords additional degrees of freedom, we reduced concerns about over-fitting by combining the (unlabeled) exposure and test data into a single analysis. Instead of modeling effects block by block, we decided to model the incremental and cumulative effects of exposure by including a non-linear effect of trial and its interaction with VOT in the analysis. In addition to replicating our analysis under relaxed assumptions about linearity, this auxiliary analysis also sheds light on the causes for the 'zigzag' pattern in the intercept and slope estimates for exposure and test blocks that we reported in the main analysis.

### Substituting GAMMs for GLMMs in our psychometric mixed-effects model
Specifically, we use the same psychometric mixed-effects model as in the main analysis, except that we replaced the full factorial of VOT, block, and exposure condition in the perceptual model with a separate tensor smooth of VOT and trial for each of the three exposure conditions.^[`t2(VOT, Trial, by = Condition.Exposure, bs = "tp")`]. This makes this auxiliary analysis a mixed-effect mixture model, for which the mixture component that is the perceptual model is a generalized additive mixed-effect models (GAMM), rather than a generalized linear mixed-effects model. The analysis contained the same full random effect structure and priors as the psychometric model presented in the main text. Both VOT and trial were Gelman-scaled prior to the analysis.

```{r}
VOT.mean_test <-
    d.for_analysis %>%
    filter(Phase == "test") %>%
    ungroup() %>%
    summarise(mean = mean(Item.VOT, na.rm = T)) %>%
    pull(mean)

d.GAMM <-
  d.for_analysis %>%
  # Get Gelman-scaled trial ID across blocks
  # (do so before filtering, so that trial roughly corresponds to amount of evidence)
  arrange(ParticipantID, Block, Trial) %>%
  group_by(ParticipantID) %>%
  # This does not include catch trials anymore (that's ok since
  # those trials do not contain information about the distribution
  # of VOTs for the learner)
  mutate(Trial = 1:length(Trial)) %>%
  filter(Item.Labeled == F) %>%
  mutate(Trial_gs = (Trial - mean(Trial)) / (2 * sd(Trial))) %>%
  prepVars(test_mean = VOT.mean_test, levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type)

my_priors <-
    c(
      prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
      prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
      prior(lkj(1), class = "cor"))

# Recode condition as ordered factor if one wants difference smooths(comparing
# levels against each other), rather than separate smooths for each level of
# Condition.Exposure
m <- brm(
    formula =
      bf(
        Response.Voiceless ~ 1,
         mu1 ~ 0 + offset(0),
        # Think further about the specific basis functions for this 2D tensor smooth
        # e.g., bs = c("cc", "tp"), k=c(10, 10)
         mu2 ~ 1 + t2(VOT_gs, Trial_gs, by = Condition.Exposure, bs = "tp") +
          # Random effects (could use bs = "re" or "fs"; "fs" only works when the
          # non RE predictors are continuous)
          # Could determine order of smooth via, e.g., m = 1
          s(ParticipantID, bs = "re") +
          s(VOT_gs, ParticipantID, bs = "re") +
          s(Trial_gs, ParticipantID, bs = "re") +
          s(VOT_gs, Trial_gs, ParticipantID, bs = "re") +
          s(Item.MinimalPair, bs = "re") +
          s(VOT_gs, Item.MinimalPair, bs = "re") +
          s(Trial_gs, Item.MinimalPair, bs = "re") +
          s(Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Trial_gs, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(Trial_gs, Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Trial_gs, Condition.Exposure, Item.MinimalPair, bs = "re"),
         theta1 ~ 1),
    data = d.GAMM,
    prior = my_priors,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 2500,
    warmup = 1500,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    control = list(adapt_delta = .99),
    backend = "cmdstanr",
    threads = threading(threads = 2),
    file = paste0("../models/GAMM-test+exposure-wRE.rds"))
```

### Results
Figure \@ref(fig:GAMM-figure) shows the predicted log-odds of "t"-responses that result from the fitted GAMM. The GAMM results replicate the directional effects of exposure both within and across exposure conditions. For the baseline condition, the contour lines largely shift downwards with exposure. This means that the same VOT is more likely to be categorized as "t" with increasing exposure. For the +40 condition, the opposite trend is observed (and much more clearly), indicating that the same VOT is *less* likely to be categorized as "t" with increasing exposure. The +10 condition falls between the baseline and +40 condition.

The GAMM results also confirm that the changes introduced by exposure are undone with repeated testing. This shows in the contour lines over trials XXX-XXX, which trend towards reverting the changes introduced by the preceding exposure. The trend shows most clearly for the baseline and +40 condition (in opposite directions since the exposure effects are in opposite directions for these two conditions).

(ref:GAMM-figure) Predicted log-odds of "t"-response by trial (across all exposure and test blocks) and VOT for each of the three exposure conditions. Combinations of trial and VOT that did not occur in any of the three exposure conditions are left white (the reduced VOT range of test blocks makes them easily identifiable).

```{r GAMM-figure, fig.cap="(ref:GAMM-figure)"}
# Plot smooth over rescaled data
ms <- conditional_smooths(
  m,
  smooths = 't2(VOT_gs, Trial_gs, by = Condition.Exposure, bs = "tp")',
  too_far = .025)
ms[[1]] %<>%
  mutate(
    VOT_gs = VOT_gs * 2 * sd(d.GAMM$Item.VOT) + VOT.mean_test,
    Trial_gs = Trial_gs * 2 * sd(d.GAMM$Trial) + mean(d.GAMM$Trial))
p <- brms:::plot.brms_conditional_effects(ms, stype = "raster", plot = F)
p[[1]] +
  aes(x = Trial_gs, y = VOT_gs) +
  geom_contour(aes(z = estimate__), color = "black") +
  xlab("Trial") + ylab("VOT") +
  scale_fill_viridis_c('log-odds of "t"-response') +
  coord_cartesian(expand = F) +
  theme(legend.position = "top")
```


# Comparing predictions of different adaptive mechanisms
Use database from @chodroff-wilson2018 as a starting point, and then expose the three different adaptive mechanisms introduced in @xie2023 to the distributions from the experiment.

```{r functions-for-adaptive-changes}
# TO DO: consider making plots a bit pretty, e.g., by grouping participant responses together
# into 5msec VOT bins and by making model prediction a line plot, rather than a point plot.
plot_predicted_vs_actual_categorization_responses <- function(data, colors.group = NULL) {
  p <-
    data %>%
    mutate(ExposureGroup = gsub("_Up\\sto", "", ExposureGroup)) %>%
    ggplot(aes(x = VOT)) +
    stat_summary(fun = mean, geom = "line", aes(y = Response)) +
    stat_summary(fun = mean, geom = "line", aes(y = Predicted_posterior), color = "gray") +
    stat_summary(fun.data = mean_cl_boot, geom = "pointrange", aes(y = Response), size = 1/4) +
    facet_wrap(~ ExposureGroup, ncol = 3) 
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group)
  plot(p)

  p %+%
    (data %>%
       mutate(
         ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
         ExposureGroup = gsub("[ABC]A", "", ExposureGroup))) %>%
    plot()

  p <- 
    data %>%
    group_by(ExposureGroup, VOT) %>%
    summarise(across(c(Response, Predicted_posterior), mean)) %>%
    ungroup() %>%
    mutate(
      Test = gsub("^.*test(.*)$", "\\1", ExposureGroup),
      Test = ifelse(Test == "no exposure", 0, Test),
      ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
      ExposureGroup = gsub("[ABC]A", "", ExposureGroup),
      ExposureGroup = gsub("^.*Shift([0-9]+).*$", "+\\1", ExposureGroup)) %>%
    ggplot(aes(x = Predicted_posterior, y = Response)) +
    geom_abline(intercept = 0, slope = 1, color = "lightgray") +
    geom_text(aes(color = ExposureGroup, label = Test)) +
    geom_smooth(aes(color = ExposureGroup)) +
    annotate(
      geom = "text",
      label = paste0(
        "R^2 = ",
        round(data %>%
                with(., cor(Predicted_posterior, Response)) %>%
                . ^ 2, 3) * 100, "%"),
      x = .1, y = .9) +
    xlab('Predicted proportion "t"-responses') +
    ylab('Observed proportion "t"-responses') 
  
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group[c(3, 6, 9, 10)])
  plot(p)
}
```

## Setting prior beliefs about marginal cue means and category means and covariance matrices
<!-- TO DO: consider subsetting to /i/ as following vowel context -->

```{r predictions-exposure-conditions-by-IOs}
# We decided to use C-CuREd VOT and f0 (in Mel) as the input to the ideal observers.
# Our approach to C-CuREing removes effects of speech rate as well as talker-specific
# means of the cues.
cues <- c("VOT", "f0_Mel")

prior_marginal_VOT_f0_stats <-
  d.chodroff_wilson %>%
  group_by(Talker) %>%
  summarise(across(all_of(cues), mean)) %>%
  ungroup() %>%
  summarise(
    x_mean = list(c(VOT = mean(VOT), f0 = mean(f0_Mel))),
    x_var_VOT = var(VOT),
    x_var_f0 = var(f0_Mel),
    x_cov = list(cov(cbind(VOT, f0_Mel))))

m_MVG.VOT_f0 <-
  make_MVG_from_data(
  data = d.chodroff_wilson,
  category = "category",
  cues = cues)

m_IO.VOT_f0 <-
  m_MVG.VOT_f0 %>%
  lift_MVG_to_MVG_ideal_observer(
      Sigma_noise = matrix(c(80, 0, 0, 878), ncol = 2, dimnames = list(names(first(.$mu)), names(first(.$mu)))),
      prior = c("/d/" = .5, "/t/" = .5),
      # TO DO: set to lapse rate inferred in psychometric model fit to participants
      lapse_rate = plogis(fixef(fit_test)[[2]]),
      lapse_bias = c("/d/" = .5, "/t/" = .5))

m_IA.VOT_f0 <-
  m_IO.VOT_f0 %>%
  lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = 10, nu = 10)
```

```{r prepare-data-for-adaptive-changes}
# Make a data frame that splits the entire data in unique exposure-test combinations.
# This data frame will be used for adaptive changes in normalization and category
# representations.
d_for_ASP <-
  d.for_analysis %>%
  ungroup() %>%
  # Exclude catch trials and the final two test blocks since we expect unlearning during those blocks
  # (which is not modeled)
  filter(
    !(Phase == "exposure" & is.na(Item.ExpectedResponse.Voicing)),
    as.numeric(Block) <= 7) %>%
  # Use F0 as intended by generation script. This makes fitting more computationally feasible
  # (336 instead of 1001 unique combinations of group & test locations), and also removes the
  # potential that annotation / measurement error perturb the f0 values. It does, however,
  # make the assumption that f0 is the same across the three minimal pairs. While likely wrong,
  # this assumption better aligns with the fact that the model doesn't have a way to keep track
  # of any lexical, phonological, or phonetic context effects on VOT (and thus no way to predict
  # any potential differences between minimal pairs).
  # rename(VOT = Item.VOT, f0_Mel = Item.F0_target_for_generation_script) %>%
  # Create new condition variable that uniquely identifies what participants have seen at any of the
  # tests and a new block variable that identifies the type and order of blocks. Also create two
  # convenience variables, x (stimulus) and Response.Category (to make matching with the category
  # column of ideal observers/adaptors more straightforward).
  mutate(
    Condition.for_ASP = paste0(Condition.Exposure, List.ExposureBlockOrder, List.ExposureMaterials),
    Block.for_ASP = paste0(Phase, Block),
    x = map2(VOT, f0_Mel, ~ c(.x, .y)),
    Response.Category = factor(ifelse(Response.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/")),
    Item.ExpectedResponse.Category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/"))) %>%
  slice_into_unique_exposure_test(
    condition_var = "Condition.for_ASP",
    block_var = "Block.for_ASP",
    block_order = c("test1", "exposure2", "test3", "exposure4", "test5", "exposure6", "test7")) %>%
  mutate(ExposureGroup = factor(ExposureGroup)) %>%
  select(
    ExposureGroup, Phase, ParticipantID, Block, Trial, x,
    VOT, f0_Mel, vowel_duration, Response.Category, Item.ExpectedResponse.Category)
```

```{r set-subsample-proportion}
set.seed(1976)
# For changes in decision-making, the adaptive algorithm is order-sensitive. This makes it computationally
# demanding to find the optimal parameterization for this algorithm (as optimization requires updating the
# data and calculating the likelihood of the test responses under the updated model, but order-sensitivity
# means that the updating cannot be vectorized but must be calculated trial-by-trial). This makes it
# computationally infeasible to find the optimal learning rate (beta) for the entire data set. We thus
# select a subset of participants for each exposure-test combination, optimize the parameter for that subset,
# and then use that (approximately) 'optimal' parameter to update the *entire* data set (once) to calculate
# the likelihood, R2, etc. for that parameter.
#
# In order to penalize all adaptive models equally, we subset the same amount of data for each model,
# including the other two change models (even though they are computationally much less demanding to fit).
sample_proportion_of_participants_per_exposure_test_condition <- .5

# TO DO: IMPLEMENT SUBSAMPLING FOR NORMALIZATION AND FOR IBBU TO COMPARE LIKELIHOODS IN A MEANINGFUL WAY.
# ---> Actually since the other algorithms are not order sensitive, they should not depend on how many
# participants are included in the fitting. Perhaps a better way to make this a fair comparison is to 
# subset the test data?
```

## Changes in normalization

```{r}
# Only one participant per exposure group is needed for exposure data
# (since they all have the same exposure up to that point, and our
# approach to normalization is order-insensitive; so doing this separately
# for each participant shouldn't make a difference).
#
# -------------------------------------------------------------------------
# TO DO: results changes after limiting data to first participant in each
# exposure group. this is unexpected. One possibility is that this is simply
# an optimization problem. another possibility is that participants differ
# in their cue means even within ExposureGroup (e.g., due to missing data)
# even though that shouldn't be the case.
# -------------------------------------------------------------------------
filename.parameter <- "../models/best_performing_parameters.normalization.rds"
filename.df <- "../models/best_performing_data.normalization.rds"

if (RESET_MODELS || !file.exists(filename.parameter)) {
  # This function is intended for the optimization run right below it
  history.optimization_normalization <- tibble(.rows = 0)
  range.prior_kappa.normalization <- c(10^-5, 10^5)

  d_for_ASP.for_normalization <-
    d_for_ASP %>%
    group_by(ExposureGroup, Phase) %>%
    filter(
      ParticipantID %in%
        sample(
          x = unique(ParticipantID),
          size = round(length(unique(ParticipantID)) * sample_proportion_of_participants_per_exposure_test_condition)))

  best_performing_parameters.normalization <-
    optim(
      par = log(mean(range.prior_kappa.normalization)),
      fn = get_likelihood_from_updated_normalization,
      method = "L-BFGS-B",
      lower = log(min(range.prior_kappa.normalization)),
      upper = log(max(range.prior_kappa.normalization)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  saveRDS(best_performing_parameters.normalization, filename.parameter)

  # Get the normalized data based on the optimal parameter for normalization
  # and then prepare it for plotting
  df.normalized <-
    update_normalization_and_normalize_test_data(
      kappa = exp(best_performing_parameters.normalization$par[1]),
      data = d_for_ASP)

  df.normalized %<>%
    mutate(observationID = 1:nrow(.)) %>%
    # Get posterior of prior model for the normalized data and join it to the
    # data.
    left_join(
      df.normalized %>%
        group_map(
          .f =
            ~ get_posterior_from_model(
              model = m_IO.VOT_f0,
              x = .x$x,
              noise_treatment = "marginalize",
              lapse_treatment = "marginalize") %>%
            filter(category == "/t/")) %>%
        .[[1]],
      by = "observationID") %>%
    # Prepare the data frame for plotting by renaming variable names to those
    # expected by plot_predicted_vs_actual_categorization_responses
    mutate(
      VOT = map_dbl(x.x, ~ .x[1]),
      Response = ifelse(Response.Category == "/t/", 1, 0)) %>%
    rename(Predicted_posterior = posterior_probability)

  # TO DO: calculate likelihood for whole data set here

  saveRDS(df.normalized, filename.df)
  d_for_ASP.for_normalization <- NULL
} else {
  best_performing_parameters.normalization <- readRDS(filename.parameter)
  df.normalized <- readRDS(filename.df)
}

cat("Changes in normalization achieve maximum log-likelihood of", best_performing_parameters.normalization$value, "on", sample_proportion_of_participants_per_exposure_test_condition * 100, "percent of the data for kappa =", exp(best_performing_parameters.normalization$par))
cat("The same kappa achieved log-likelihood of", NA, "one whole data set.")
```

```{r, fig.width=7, fig.height=6}
df.normalized %>% plot_predicted_vs_actual_categorization_responses()
```

## Changes in decision-making

```{r}
filename.parameter <- "../models/best_performing_parameters.decision_making.rds"
filename.df <- "../models/best_performing_data.decision_making.rds"

if (RESET_MODELS || !file.exists(filename.parameter)) {
  history.optimization_bias <- tibble(.rows = 0)
  range.beta <- c(10^-3, 10^3)

  d_for_ASP.for_decision_changes <-
    d_for_ASP %>%
    ungroup() %>%
    # Include all test observations but only the exposure groups that contain all 3 exposure
    # blocks (which then are reformatted as part of the updating function in order to derive
    # the predicted updated decision biases after each of the three exposure blocks).
    filter(
      # We exclude the no exposure group since its likelihood does not depend on
      # beta, and thus cannot affect the optimization of beta.
      ExposureGroup != "no exposure",
      Phase == "test" | grepl(x = ExposureGroup, pattern = ".*Up to test7")) %>%
    # Sample proportion of participants from each exposure condition
    mutate(ExposureGroup_withoutBlock = gsub("_Up to test.*", "", ExposureGroup)) %>%
    group_by(ExposureGroup_withoutBlock) %>%
    filter(ParticipantID %in%
             sample(
               x = unique(ParticipantID),
               size = round(length(unique(ParticipantID)) * sample_proportion_of_participants_per_exposure_test_condition))) %>%
    arrange(ParticipantID, Block, ExposureGroup, Phase, Trial) %>%
    droplevels()

  # Calculating exposure and test data outside of the optimization loop, in order to minimize
  # computations that need to be conducted on each optimization step.
  d_for_ASP.for_decision_changes.exposure <-
    d_for_ASP.for_decision_changes %>%
    ungroup() %>%
    filter(Phase == "exposure") %>%
    group_by(ExposureGroup, ParticipantID, Phase) %>%
    droplevels() %>%
    mutate(
      (!! sym(cues[1])) := map_dbl(x, ~ .x[1]),
      (!! sym(cues[2])) := map_dbl(x, ~ .x[2])) %>%
    group_by(ExposureGroup, ParticipantID)

  d_for_ASP.for_decision_changes.test <-
    d_for_ASP.for_decision_changes %>%
    filter(Phase == "test") %>%
    ungroup() %>%
    select(ExposureGroup, ParticipantID, x, Response.Category) %>%
    group_by(ExposureGroup, ParticipantID) %>%
    nest(data = c(x, Response.Category))

  best_performing_parameters.bias <-
    optim(
      par = mean(log(range.beta)),
      fn = get_likelihood_from_updated_bias,
      method = "L-BFGS-B",
      lower = log(min(range.beta)),
      upper = log(max(range.beta)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  saveRDS(best_performing_parameters.bias, filename.parameter)

  # Use the optimal parameter to update the model's decision biases for *all* observations and then apply
  # the model to the test data to calculate the log-likelihood and predicted responses for the entire data.
  df.bias <-
    update_decision_bias_by_group(
      prior = m_IO.VOT_f0,
      beta = exp(best_performing_parameters.bias$par),
      data =  
        d_for_ASP %>%
        group_by(ExposureGroup, ParticipantID, Phase) %>%
        filter(Phase == "exposure", grepl(x = ExposureGroup, pattern = ".*Up to test7")) %>%
        arrange(ParticipantID, Phase, Block) %>%
        droplevels() %>%
        mutate(
          (!! sym(cues[1])) := map_dbl(x, ~ .x[1]),
          (!! sym(cues[2])) := map_dbl(x, ~ .x[2])) %>%
        group_by(ExposureGroup, ParticipantID)) %>%
    format_updated_bias_models_and_join_test_data(
      prior = m_IO.VOT_f0,
      # Here we do *not* exclude the "no exposure" condition since we want to calculate the likelihood for
      # the entire data set.
      data.test = d_for_ASP %>%
        filter(Phase == "test") %>%
        select(ExposureGroup, ParticipantID, x, Response.Category) %>%
        group_by(ExposureGroup, ParticipantID) %>%
        nest(data = c(x, Response.Category)))

  # TO DO: get ll for entire data here

  df.bias %<>%
    # Categorize test tokens
    mutate(
      categorization = future_map2(
        model,
        data,
        ~ get_categorization_from_MVG_ideal_observer(
          model = .x,
          x = .y$x,
          decision_rule = "proportional",
          noise_treatment = "marginalize",
          lapse_treatment = "marginalize") %>%
          filter(category == "/t/"))) %>%
    # Prepare the data frame for plotting by renaming variable names to those
    # expected by plot_predicted_vs_actual_categorization_responses
    mutate(Response = map(data, ~ ifelse(.x$Response.Category == "/t/", 1, 0))) %>%
    select(-c(model, data)) %>%
    unnest(c(categorization, Response)) %>%
    mutate(VOT = map_dbl(x, ~ .x[1])) %>%
    rename(Predicted_posterior = response)

  saveRDS(df.bias, filename.df)
  d_for_ASP.for_decision_changes <- NULL
} else {
  best_performing_parameters.bias <- readRDS(filename.parameter)
  df.bias <- readRDS(filename.df)
}

cat("Changes in decision-making achieve maximum likelihood of", best_performing_parameters.bias$value, "on", sample_proportion_of_participants_per_exposure_test_condition * 100, "percent of the data for beta =", exp(best_performing_parameters.bias$par))
cat("The same kappa achieved log-likelihood of", NA, "one whole data set.")
```

```{r, fig.width=7, fig.height=6}
df.bias %>% plot_predicted_vs_actual_categorization_responses()
```

## Changes in category representations

```{r}
# NOT YET IMPLEMENTED
filename <- "../models/best_performing_parameters.representation.rds"

# This function is intended for the optimization run right below it
history.optimization_representation <- tibble(.rows = 0)
range.prior_kappa.representation <- c(3, 10^5)
range.prior_nu.representation <- c(3, 10^5)

update_representations <- function(
  prior,
  data
) {
  cues <- get_cue_labels_from_model(prior)

  data %>%
    filter(Phase == "exposure")
    mutate(
      (!!! syms(cues))[1] := map_dbl(x, ~ .x[1]),
      (!!! syms(cues))[2] := map_dbl(x, ~ .x[2])) %>%
      group_by(ExposureGroup) %>%
      group_map(
        .f = ~ update_NIW_ideal_adaptor_incrementally(
          prior = prior,
          exposure = .x,
          exposure.category = "Item.ExpectedResponse",
          exposure.cues = cues,
          noise_treatment = "marginalize",
          lapse_treatment = "marginalize",
          method = "label-certain",
          keep.update_history = FALSE,
          keep.exposure_data = FALSE) %>%
          nest(posterior = everything())) %>%
      reduce(bind_rows)
}

get_likelihood_from_updated_representation <- function(
    par,
    prior = m_IA.VOT_f0,
    data = d_for_ASP
) {
  kappa <- exp(par[1])

  ll <-
    update_representations(
      kappa = kappa,
      data = data) %>%
    get_likelihood_from_grouped_data(model = prior)

  history.optimization_representation <<-
    bind_rows(
      history.optimization_representation,
      tibble(kappa = kappa, nu = nu, log_likelihood = ll))

  return(ll)
}

if (RESET_MODELS || !file.exists(filename)) {
  best_performing_parameters.representation <-
    optim(
      par = log(mean(range.prior_kappanu.representation)),
      fn = get_likelihood_from_updated_normalization,
      method = "L-BFGS-B",
      lower = log(min(range.prior_kappanu.representation)),
      upper = log(max(range.prior_kappanu.representation)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  best_performing_parameters.representation <- exp(best_performing_parameters.representation$par[1])
  saveRDS(best_performing_parameters.representation, filename)
} else {
  best_performing_parameters.representation <- readRDS(filename)
}
```

## Changes in category representations while inferring prior category means and covariance matrices

```{r}
summarize.NIW_ideal_adaptor_stanfit <- function(
    x,
    pars = c("kappa", "nu", "m", "S", "lapse_rate"),
    groups = c(
      "prior",
      "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
      "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
      "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7")
) {
  add_ibbu_stanfit_draws(x, groups = groups, summarize = F) %>%
    unnest_cue_information_in_model() %>%
    gather_variables(exclude = c(".chain", ".iteration", ".draw", ".row", "cue", "cue2", "group", "category")) %>%
    ungroup() %>%
    # + Remove double mentions of variables that are constant across groups, categories, and cues
    #   (currently: lapse_rate)
    # + Remove double mentions of variables that are constant across cues
    #   (currently: kappa and nu)
    # + Remove double mentions of variables that are constant across cue2
    #   (currently: m)
    mutate(
      across(
        c(group, category),
        ~ ifelse(.variable == "lapse_rate", NA_character_, as.character(.x))),
      across(
        c(cue, cue2),
        ~ ifelse(.variable %in% c("kappa", "nu", "lapse_rate"), NA_character_, as.character(.x))),
      across(
        c(cue2),
        ~ ifelse(.variable == "m", NA, .x))) %>%
    distinct() %>%
    group_by(group, category, cue, cue2, .variable) %>%
    median_hdci() %>%
    mutate(
      group = factor(group, levels = groups),
      .variable = factor(.variable, levels = pars)) %>%
    # Format and sort output
    relocate(group, category, .variable, cue, cue2, .value, everything()) %>%
    arrange(match(.$.variable, levels(.$.variable)), match(.$group, levels(.$group)), category, cue, cue2) %>%
    rename(
      parameter = .variable,
      !! sym(unique(.$.point)) := .value,
      !! sym(unique(paste0("lower ", unique(.$.width * 100), "% ", toupper(unique(.$.interval))))) := .lower,
      !! sym(unique(paste0("upper ", unique(.$.width * 100), "% ", toupper(unique(.$.interval))))) := .upper) %>%
    select(-c(.width, .point, .interval)) %>% 
    kable()
}

plot_predicted_vs_actual_categorization_responses_for_IBBU <- function(
    model,
    groups = NULL,
    untransform_cues = F,
    # target category "/d/" = 1, "/t/" = 2
    target_category = 2,
    colors.group = NULL
) {
  # Get and summarize posterior draws from fitted model
  d.pars <-
    add_ibbu_stanfit_draws(
      model,
      groups = groups,
      summarize = F,
      wide = F,
      ndraws = NULL,
      untransform_cues = untransform_cues) %>%
    filter(group %in% .env$groups)
  
  # Prepare test_data
  # --------- THIS MIGHT NEED WORK --------
  cue.labels <- get_cue_levels_from_stanfit(model)
  data.test <-
    get_test_data_from_stanfit(model) %>%
    distinct(!!! syms(cue.labels)) %>%
    { if (untransform_cues) get_untransform_function_from_stanfit(model)(.) else . } %>%
    make_vector_column(cols = cue.labels, vector_col = "x", .keep = "all") %>%
    nest(cues_joint = x, cues_separate = .env$cue.labels)  %>%
    crossing(group = levels(d.pars$group))

  # Categorize test data
  d.pars %<>%
    group_by(group, .draw) %>%
    do(f = get_categorization_function_from_grouped_ibbu_stanfit_draws(., logit = F)) %>%
    right_join(data.test, by = "group") %>%
    group_by(group, .draw) %>%
    mutate(
      Predicted_posterior =
        pmap(
          .l = list(f, cues_joint, target_category),
          .f = ~ exec(..1, x = ..2$x, target_category = target_category))) %>%
    select(-f) %>%
    unnest(c(cues_joint, cues_separate, Predicted_posterior))

  d.pars %<>% mutate(Predicted_posterior = ifelse(is.infinite(Predicted_posterior), 1, Predicted_posterior))

  # Compare predicted and actual responses
  d.pars %>%
    group_by(group, VOT) %>%
    summarise(across(Predicted_posterior, mean)) %>%
    mutate(VOT = VOT, ExposureGroup = ifelse(group == "prior", "no exposure", group)) %>%
    left_join(get_test_data_from_stanfit(model)) %>%
    mutate(
      Response = `/t/` / (`/d/` + `/t/`),
      # Simplify the plot by collapsing over all Latin-square designed lists
      # (this still keeps all individual predictions but only has one facet per unique combination
      # of exposure condition and test, rather than also making separate facets for each of the three 
      # different block orders for each of these unique combinations)
      group = gsub("[ABC]A", "", group)) %>%
    plot_predicted_vs_actual_categorization_responses(colors.group = colors.group)
}

make_all_plots <- function(
  fit,
  groups = c(
    "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
    "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
    "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7",
    "prior"),
  colors.group =  c(
    "#550000", "#AA0000", "#FF0000",
    "#005500", "#00AA00", "#00FF00",
    "#000055", "#0000AA", "#0000FF",
    "gray"),
  colors.category = c("blue", "red"),
  ncol = 3
) {
  plot_ibbu_stanfit_parameter_correlations(fit, category.colors = colors.category) %>% plot()
  if (!is.null(groups)) {
    plot_ibbu_stanfit_parameters(fit, groups = groups, group.colors = colors.group) %>% plot()
    (plot_expected_ibbu_stanfit_categories_contour2D(fit, groups = groups, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol)) %>% plot()
    (plot_ibbu_stanfit_test_categorization(fit, groups = groups, plot_in_cue_space = T, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol)) %>% plot()
    plot_predicted_vs_actual_categorization_responses_for_IBBU(
      fit, 
      groups = c(
        "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
        "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
        "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7",
        "Cond Shift0BA_Up to test3", "Cond Shift0BA_Up to test5", "Cond Shift0BA_Up to test7",
        "Cond Shift10BA_Up to test3", "Cond Shift10BA_Up to test5", "Cond Shift10BA_Up to test7",
        "Cond Shift40BA_Up to test3", "Cond Shift40BA_Up to test5", "Cond Shift40BA_Up to test7",
        "Cond Shift0CA_Up to test3", "Cond Shift0CA_Up to test5", "Cond Shift0CA_Up to test7",
        "Cond Shift10CA_Up to test3", "Cond Shift10CA_Up to test5", "Cond Shift10CA_Up to test7",
        "Cond Shift40CA_Up to test3", "Cond Shift40CA_Up to test5", "Cond Shift40CA_Up to test7",
        "no exposure"), 
      untransform_cues = F, target_category = 2, colors.group = colors.group)
  } else {
    plot_ibbu_stanfit_parameters(fit) %>% plot()
    plot_expected_ibbu_stanfit_categories_contour2D(fit, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol) %>% plot()
    plot_ibbu_stanfit_test_categorization(fit, plot_in_cue_space = T, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol) %>% plot()
  }
}
```


### Fit with uninformative (regularizing) priors
Here some initial plots. While the plots are back-transformed into the original cue space, the table is not (yet).

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis'}
m_IA_inferred <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    use_multivariate_updating = T,
    sample = T,
    file = "../models/IBBU_VOT_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred) 
make_all_plots(m_IA_inferred)
```


```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis'}
m_IA_inferred.VOT_f0 <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT+f0_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.VOT_f0) 
make_all_plots(m_IA_inferred.VOT_f0)
```

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis'}
m_IA_inferred.VOT_f0_vowelduration <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.VOT_f0_vowelduration) 
make_all_plots(m_IA_inferred.VOT_f0_vowelduration)
```

### Fit with informative priors about expected category means and covariances

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis'}
m_IA_inferred.fixed_pars <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    sample = T,
    lapse_rate = NULL,
    mu_0 = m_IO.VOT_f0$mu,
    Sigma_0 = m_IO.VOT_f0$Sigma,
    file = "../models/IBBU_VOT+f0_informative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.fixed_pars) 
make_all_plots(m_IA_inferred.fixed_pars)
```





# WHAT OF THE REST SHOULD BE KEPT?

# Predictions of existing models of adaptive speech perception {sec:proof}
We show that two of the most influential approaches to adaptive speech perception do not predict the sublinear (less than proportional) shifts in categorisation functions observed both in the present study and in in previous work [@kleinschmidt-jaeger2016; @kleinschmidt2020]. We focus on these two models since they are the only widely-used accounts of adaptive speech perception that are (1) not limited to specific types of contrasts (unlike, e.g., vowel normalization accounts) and (2) sufficiently specific to make concrete predictions [for review of existing models, see @xie2023]. We only briefly discuss why a third type of account---exemplar models of speech perception---*likely* would also fail to predict the sublinear effects.

We emphasize that our discussion deliberately focuses on *models*, not theories. The finding of sublinear effects of exposure can be accommodated in all three theories that underlie the three models discussed here (Bayesian inference in the ideal adaptor framework, similarity-based inference in exemplar theory, or normalization relative to expectations). Yet, the fact that none of the existing *models* predicts this effect highlights that this property of adaptive speech perception was only recently discovered, and is as-of-yet not well understood.

## Incremental Bayesian belief-updating [@kleinschmidt-jaeger2011, @kleinschmidt-jaeger2012, @kleinschmidt-jaeger 2015, @kleinschmidt-jaeger2016; @kleinschmidt2020]
The only distributional learning model that has been more repeatedly tested against adaptive speech perception---incremental Bayesian belief-updating [@kleinschmidt-jaeger2011]---predicts proportional, rather than sublinear, shifts. This model had previously been found to closely predict the cumulative effects of exposure in perceptual recalibration to audio-visually [@kleinschmidt2011-jaeger; @kleinschmidt-jaeger2012] or lexically labeled speech [@cummings-theodore2023], as well as the type of exposure to unlabelled minimal pair words employed by Kleinschmidt and Jaeger [@theodore-monto2019]. However, all of these studies employed comparatively small changes in cue distributions, and lacked the design necessary to detect deviation from proportionality (we return to this point below). The findings presented in @kleinschmidt-jaeger2016 would seem to reject this specific distributional learning model [though not necessarily the theory it is derived from, @kleinschmidt-jaeger2015; for discussion of the relation between theory and model, see also @kleinschmidt2020]  <!-- TO DO: make sure we do -->

```{r}
get_IBBU_updates <- function(
    prior = NULL,
    exposure = NULL,
    test = NULL,
    condition = NULL,
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72,
    updates_to_keep = c(0, 48, 96, 144),
    seed = 1234
) {
  set.seed(seed)
  Sigma_noise <- diag(c(80, 878)[1:length(cues)])
  dimnames(Sigma_noise) <- list(cues, cues)

  if (is.null(prior)) {
    prior <- d.chodroff_wilson %>%
      make_NIW_ideal_adaptor_from_data(
        cues = cues, kappa = kappa, nu = nu, Sigma_noise = Sigma_noise)
  }

  if (is.null(exposure)) {
    message("When exposure is NULL, condition is interpreted as the VOT shift.")
    exposure <-
      lapply(
        X = as.list(1:3),
        FUN = function(x) d.chodroff_wilson %>%
          # Sample exposure from VOT distribution regardless of which cues are used
          # for belief-updating, since that's what we did in the experiment.
          make_MVG_ideal_observer_from_data(
            cues = "VOT", Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
          sample_MVG_data_from_model(Ns = 24, randomize.order = T)) %>%
      reduce(rbind) %>%
      mutate(
        Trial = 1:144,
        Condition.Exposure = paste0("TrueShift", .env$condition),
        VOT = VOT + as.numeric(.env$condition),
        f0 = predict_f0(VOT, Mel = F),
        f0_Mel = predict_f0(VOT, Mel = T)) %>%
      relocate(Condition.Exposure, Trial, category, VOT, f0)
  }

  idealized_ios <-
    make_VOT_IOs_from_exposure(exposure) %>%
    get_logistic_parameters_from_model(
    model = ., x = map(test, ~ .x[1]),
    model_col = "io",
    groups = "Condition.Exposure")

  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.order = "Trial",
    noise_treatment = "no_noise",
    lapse_treatment = "no_lapses",
    method = "label-certain") %>%
    filter(observation.n %in% updates_to_keep) %>%
    nest(ia = -observation.n) %>%
    mutate(
      Condition.Exposure = unique(exposure$Condition.Exposure),
      observation.bin = as.numeric(factor(observation.n, levels = sort(observation.n))),
      observation.alpha = observation.bin / max(observation.bin),
      cf = map(ia,
               ~ get_categorization_function_from_NIW_ideal_adaptor(.x, noise_treatment = "no_noise")),
      sf = map2(cf, observation.alpha,
                ~ stat_function(
                  fun = function(x) {
                    as.numeric(.x(
                      map(x, function(y) c(y, predict_f0(y, Mel = T))),
                      target_category = 2)) },
                  alpha = .y,
                  color = if (condition %in% c("Shift0", "Shift10", "Shift40")) colours.condition[condition] else "black"))) %>%
    select(-c(observation.bin, observation.alpha)) %>%
    relocate(Condition.Exposure, observation.n, ia, cf, sf) %>%
    # Convert updated beliefs into estimated PSE and slope
    get_logistic_parameters_from_model(
      model = ., x = test, model_col = "ia",
      groups = c("Condition.Exposure", "observation.n", "cf", "sf")) %>%
    # Join in information about idealized prior and posterior
    # (the latter based on ideal observer that has fully learned the exposure distribution).
    # Then calculate for each update step the proportion of updating relative to those
    # idealized priors and posteriors.
    left_join(
      idealized_ios %>%
        select(Condition.Exposure, slope_scaled, PSE) %>%
        rename(idealized_posterior_slope_scaled = slope_scaled, idealized_posterior_PSE = PSE)) %>%
    cross_join(
      idealized_ios %>%
        ungroup() %>%
        filter(Condition.Exposure == "prior") %>%
        select(slope_scaled, PSE) %>%
        rename(idealized_prior_slope_scaled = slope_scaled, idealized_prior_PSE = PSE)) %>%
    mutate(
      proportion_of_idealized_PSE = (PSE - idealized_prior_PSE) / (idealized_posterior_PSE - idealized_prior_PSE),
      proportion_of_idealized_slope_scaled = (slope_scaled - idealized_prior_slope_scaled) / (idealized_posterior_slope_scaled - idealized_prior_slope_scaled))
}

get_IBBU_updates_for_experiment <- function(
    condition,
    test = NULL,
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72
) {
  exposure <-
    d.for_analysis %>%
    ungroup() %>%
    filter(
      Condition.Exposure == condition,
      Phase == "exposure") %>%
    filter(ParticipantID == first(ParticipantID)) %>%
    mutate(
      category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")) %>%
    rename(
      VOT = Item.VOT,
      f0_Mel = Item.f0_Mel)

  get_IBBU_updates(
    exposure = exposure,
    test = test,
    condition = condition,
    cues = cues, kappa = kappa, nu = nu)
}

x <-
  d.for_analysis %>%
  ungroup() %>%
  filter(Phase == "test") %>%
  distinct(Item.VOT, Item.f0_Mel) %>%
  arrange(Item.VOT) %>%
  mutate(x = map2(Item.VOT, Item.f0_Mel, ~ c(.x, .y))) %>%
  pull(x)

# Plot updates for our three exposure condition
d.updates <-  
  bind_rows(get_IBBU_updates_for_experiment("Shift0", test = x),
            get_IBBU_updates_for_experiment("Shift10", test = x),
            get_IBBU_updates_for_experiment("Shift40", test = x))

# d.exposure %>%
#   distinct(VOT) %>%
#   ggplot(aes(x = VOT)) +
#   d.updates$sf +
#   scale_x_continuous(limits = c(10, 90)) +
#   geom_label(
#     data = d.updates %>% filter(observation.n == 144),
#     aes(
#       x = PSE,
#       color = Condition.Exposure,
#       label = percent(proportion_of_idealized_PSE)),x

#     y = .5)
```




```{r get-updates-nu-equal-kappa}
x <-
  tibble(VOT = seq(0, 100)) %>%
  mutate(x = map(VOT, ~ c(.x, predict_f0(.x, Mel = T)))) %>%
  pull(x)

# Plot updates for true rightward shifts of various magnitudes
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x),
            get_IBBU_updates(condition = 20, test = x),
            get_IBBU_updates(condition = 30, test = x),
            get_IBBU_updates(condition = 40, test = x))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r get-updates-nu-larger-kappa}
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x, nu = 10000),
            get_IBBU_updates(condition = 20, test = x, nu = 10000),
            get_IBBU_updates(condition = 30, test = x, nu = 10000),
            get_IBBU_updates(condition = 40, test = x, nu = 10000))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```


```{r get-updates-nu-smaller-kappa}
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x, kappa = 10000),
            get_IBBU_updates(condition = 20, test = x, kappa = 10000),
            get_IBBU_updates(condition = 30, test = x, kappa = 10000),
            get_IBBU_updates(condition = 40, test = x, kappa = 10000))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r}
m <- readRDS("../models/IBBU-Tan-Jaeger2022-Experiment2-scaled.RDS")
summary(m)
```

## Centering cues relative to expecations [C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]
Similarly, existing models of perceptual normalization---an alternative, but mutually compatible, hypothesis---also predict proportional changes in PSE. <!-- TO DO: write these sections for SI. along a single continuum normalization shift of cue mean just shifts cat function. for multi-dimensional cues, the same is true under cue integration models (variance and thus weighting does not change; shift along each dimension follows same logic as for single cue). for multi-dimensional multivariate models with interacting cues viewed from the perspective of a single cue, shifts can *appear* non-proportional but the pattern observed in KJ16---non proportionality without changes in slopes---would seem to be impossible to create as long as the test stimuli form a line in the multidimensional cue space -->

## Exemplar theory
<!-- generally the most similar exemplars should dominate perception. So it's unclear why one wouldn't converge against the input -->


# Norming experiment: Estimating listener's expectations prior to informative exposure {sec:norming-experiment}
The purpose of the norming experiment was to validate our experiment methodology. We test basic assumptions about the paradigm and stimuli we employ in this study. We obtain estimates of the category boundary between /d/ and /t/ as perceived by *the type of listeners we seek to recruit for the main experiment*.

## Methods
### Participants
Participants were recruited over Amazon's Mechanical Turk platform, and paid $2.50 each (for a targeted remuneration of \$6/hour). The experiment was only visible to Mechanical Turk participants who (1) had an IP address in the United States, (2) had an approval rating of 95% based on at least 50 previous assignments, and (3) had not previously participated in any experiment on stop voicing from our lab. 24 L1 US English listeners (female = 9; mean age = 36.2 years; SD age = 9.2 years) completed the experiment. To be eligible, participants had to confirm that they (1) spent at least the first 10 years of their life in the US speaking only English, (2) were in a quiet place, and (3) wore in-ear or over-the-ears headphones that cost at least \$15.

### Materials
All stimuli are available as part of the OSF repository for this article. We created four VOT continua ranging from -100ms VOT to +130ms VOT in 5ms steps for each of the minimal pair words, "dill-till", "dip-tip", "din-tin", and "dim-Tim". The synthesis of these continua followed the procedure described in (SI:sec). We selected 24 steps ranging from (-100, -50, -10, 5  `r paste0(seq(15, 90, 5), collapse = ", ")`, `r paste0(seq(100, 130, 10), collapse = ", ")`) from each continua. VOT tokens in the lower and upper ends were distributed over larger increments because stimuli in those ranges were expected to elicit floor and ceiling effects, respectively.

In addition to the critical minimal pair continua we also recorded three words that did not did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used as catch trials. Stimulus intensity was set to 70 dB sound pressure level for all recordings.

### Procedure
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu//experiments/DLVOT/series-A/experiment-A.html?list_test=NORM-A-forward-test). The first page of the experiment informed participants of their rights and the requirements for the experiment: that they had to be native listeners of English, wear headphones for the entire duration of the experiment, and be in a quiet room without distractions. Participants had to pass a headphone test, and were asked to keep the volume unchanged throughout the experiment. Participants could only advance to the start of the experiment by acknowledging each requirement and consenting to the guidelines of the Research Subjects Review Board of the University of Rochester.

On the next page, participants were informed about the task for the remainder of the experiment. They were informed that they would hear a female talker speak a single word on each trial, and had to select which word they heard. Participants were instructed to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. This was done to encourage their full attention.

Each trial started with a dark-shaded green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen, as shown in Figure \@ref(fig:exp1-example-trial). At 1000ms from trial onset, the fixation dot would turn bright green and an audio recording from the matching minimal pair continuum started playing. Participants were required to click on the word they heard. For each participant, /d/-initial words were either always displayed on the left side or always displayed on the right side. Across participants, this ordering was counter-balanced. After participants clicked on the word, the next trial began.

Participants heard 192 target trials (four minimal pair continua, each with 24 VOT steps, each heard twice). In addition, participants heard 12 catch trials. On catch trials, participant saw two written catch stimuli on the screen (e.g., "flare" and "rare"), and heard one of them (e.g. "rare"). Since these recordings were easily distinguishable, they served as a check on participant attention throughout the experiment.

The order of trials was randomized for each participant with the only constraint that no stimulus was repeated before each stimulus had been heard at least once. Catch trials were distributed randomly throughout the experiment with the constraint that no more than two catch trials would occur in a row. Participants were given the opportunity to take breaks after every 60 trials. Participants took an average of 12 minutes (SD = 4.8) to complete the 204 trials, after which they answered a short survey about the experiment.

```{r, echo=FALSE, warning=FALSE}
# load formatted dataframe from experiment 1
d.test <- read_csv("../data/d.test.Exp1.csv", show_col_types = F)

# load f0-5ms-into-vowel measurements of stimuli
d.f0.5ms <-
  read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv", show_col_types = F) %>%
  dplyr::select(filename, VOT, f0_5ms_into_vowel) %>%
  # This is the F0 measured in the same way as Chodroff & Wilson (2018)
        # (i.e., 5ms into the vowel)
  rename(Item.VOT = VOT,
         Item.Measured_f0 = f0_5ms_into_vowel,
         Item.Filename = filename) %>%
  mutate(Item.Filename = paste0(Item.Filename, ".wav"))

d.test %<>%
ungroup() %>%
  left_join(d.f0.5ms, by = c("Item.Filename", "Item.VOT")) %>%
              mutate(Item.Measured_f0_Mel = normMel(Item.Measured_f0),
                     Item.f0_Mel = normMel(Item.F0_target_for_generation_script))

# mark catch trials rows and check correct
d.test %<>%
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == TRUE & Item.MinimalPair == Response, TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == FALSE, NA, CatchTrial.Correct)) %>%
  group_by(ParticipantID) %>%
  mutate(Excluded.due.to.CatchTrial = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 10, TRUE, FALSE))

# get the rows removed due to catch trial performance
excluded.data.due.to.catch <-
  d.test %>%
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == TRUE)

# set RT exclusion criteria and create excluded columns
d.test %<>%
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == FALSE) %>%
  mutate(Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
         Response.log_RT.scaled = scale(Response.log_RT),
         # this gives each subject's mean log_RT
         Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>%
  ungroup() %>%
  mutate(Excluded.participant.due.to.mean.RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE))
         # Get participants whose means are more than 3x sd of mean of participant means. Mean of means is the same as mean of all rows because each participant has the same number of rows.

excluded.participants.due.to.mean.RT <-
  d.test %>%
  filter(Excluded.participant.due.to.mean.RT == TRUE)

d.test %<>%
  filter(Excluded.due.to.CatchTrial == FALSE & Excluded.participant.due.to.mean.RT == FALSE) %>%
  group_by(ParticipantID) %>%
  mutate(Excluded.trial.due.to.RT = ifelse(abs(Response.log_RT.scaled) > 3, TRUE, FALSE))

# get the rows removed due to RT
excluded.data.due.to.RT <-
  d.test %>%
  filter(Excluded.participant.due.to.mean.RT == TRUE | Excluded.trial.due.to.RT == TRUE)

# get the proportion of rows excluded from analysis
proportion.excluded <- (nrow(excluded.data.due.to.RT)) / (nrow(d.test))

# make a dataframe after exclusion to be used for analysis
d.test.excluded <-
  d.test %>%
  filter(
    Is.CatchTrial == FALSE,
    Excluded.due.to.CatchTrial == FALSE,
    Excluded.trial.due.to.RT == FALSE) %>%
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0))
```


### Exclusions
We excluded from analysis participants who committed more than 2 errors out of the 12 catch trials (<83% accuracy, N = 3), participants with an average reaction time (RT) more than three standard deviations from the mean of the by-participant means (N = 0), and participants who reported not to have used headphones (N = 0) or not to be native (L1) speakers of US English (N = 0). For the remaining participants, trials that were more than three SDs from the participant's mean RT were excluded from analysis (`r proportion.excluded * 100 `%). Finally, we excluded participants (N = 0) who had less than 50% data remaining after these exclusions.

### Analysis approach
The goal of our behavioral analyses was to address three methodological questions that are of relevance to Experiment 2: (1) whether our stimuli resulted in 'reasonable' categorisation functions, (2) whether these functions differed between the four minimal pair items, and (3) whether participants' categorisation functions changed throughout the 192 test trials.

To address these questions, we fit a single Bayesian mixed-effects psychometric model to participants' categorization responses on critical trials [e.g., @prins2011]. The *lapsing model* only contained an intercept (the response bias in log-odds) and by-participant random intercepts. Similarly, the *model for the lapse rate* only had an intercept (the lapse rate) and by-participants random intercepts. No by-item random effects were included for the lapse rate nor lapsing model since these parts of the analysis---by definition---describe stimulus-*in*dependent behavior. The *perceptual model* included an intercept and VOT, as well as the full random effect structure by participants and items (the four minimal pair continua), including random intercepts and random slopes by participant and minimal pair. We did not model the random effects of trial to reduce model complexity. This potentially makes our analysis of trials in the model anti-conservative. Finally, the models included the covariance between by-participant random effects across the three linear predictors for the lapsing model, lapse rate model, and perceptual model. This allows us to capture whether participants who lapse more often have, for example, different response biases or different sensitivity to VOT (after accounting for lapsing).

We fit the model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. Following previous work from our lab [@horberg2021; @xie2021cross], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008], and used Student priors centered around zero with a scale of 2.5 units [following @gelman-prior2008] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $\hat{R}$ were close to 1.

### Expectations
Based on previous experiments, we expected a strong positive effect of VOT, with increasing proportions of "t"-responses for increasing VOTs. We did not have clear expectations for the effect of trial other than that responses should become more uniformed (i.e move towards 50-50 "d"/"t"-bias or 0-log-odds) as the experiment progressed [@liu-jaeger2018] due to the un-informativeness of the stimuli.
Previous studies with similar paradigms have typically found lapse rates of 0-10% [< -2.2 log-odds, e.g., @clayards2008; @kleinschmidt-jaeger2016].

```{r, warning=FALSE}
# set the mean and SD values for scaling/unscaling purposes
VOT.mean_exp1 <- mean(d.test.excluded$Item.VOT)
VOT.sd_exp1 <- sd(d.test.excluded$Item.VOT)
Trial.mean <- mean(d.test.excluded$Trial)
Trial.sd <- sd(d.test.excluded$Trial)
f0.mean_exp1 <- mean(d.test.excluded$Item.f0_Mel)
f0.sd_exp1 <- sd(d.test.excluded$Item.f0_Mel)

d.test.excluded %<>%
  mutate(
    sVOT = (Item.VOT - VOT.mean_exp1) / (2 * VOT.sd_exp1),
    sTrial = (Trial - Trial.mean) / (2 * Trial.sd))

# load (or run) the psychometric model with interaction
fit <- brm(
  bf(
    Response.Voicing == "voiceless" ~ 1,
    mu1 ~ 1 + (1 | g | ParticipantID),
    mu2 ~ 1 + sVOT * sTrial + (1 + sVOT | g | ParticipantID) + (1 + sVOT | h | Item.MinimalPair),
    theta1 ~ 1 + (1 | g | ParticipantID)),
  data = d.test.excluded,
  prior = my_priors,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  family = mixture(bernoulli("logit"), bernoulli("logit")),
  control = list(adapt_delta = .99),
  file = "../models/Exp-NORM-lapsing-bias-GLMM")

# get effects of psychometric fit conditioned on VOT interacting with trial
if (file.exists("../models/conditional_effects_VOT_Trial.rds")) {
  psychometric_fit_exp1 <- read_rds("../models/conditional_effects_VOT_Trial.rds")
} else {
  int_conditions <- list(sTrial = sort(unique((d.test.excluded$Trial - Trial.mean)) / (2 * Trial.sd)))

  psychometric_fit_exp1 <-
    conditional_effects(
      fit,
      effects = "sVOT:sTrial",
      int_conditions = int_conditions,
      ndraws = 1000,
      plot = F)[[1]]

  write_rds(psychometric_fit_exp1, file = "../models/conditional_effects_VOT_Trial.rds")
}

# get the PSE from the fitted categorisation function
PSE.fit <- descale(-(summary(fit)$fixed["mu2_Intercept", 1] / summary(fit)$fixed["mu2_sVOT", 1]), VOT.mean_exp1, VOT.sd_exp1)

# get posterior samples of intercept and slope, and median qi of the PSEs
post_sample_norm <- fit %>%
  spread_draws(b_mu2_Intercept, b_mu2_sVOT) %>%
  mutate(PSE = descale(-(b_mu2_Intercept/b_mu2_sVOT), VOT.mean_exp1, VOT.sd_exp1)) %>%
  median_qi(PSE)
```



```{r}
# prepare data for predicted categorisation by min pair
newdata <- expand_grid(
  sVOT = (seq(-100, 130, 1) - VOT.mean_exp1) / (2 * VOT.sd_exp1),
  Item.MinimalPair = levels(factor(d.test.excluded$Item.MinimalPair)),
  ParticipantID = levels(factor(d.test.excluded$ParticipantID)),
  sTrial = 0)

# get expectation values for categorisation by min pair through posterior predictions
if (file.exists("../models/categorisation_by_min_pair.rds")) {
  cat_minimalpair <- read_rds("../models/categorisation_by_min_pair.rds")
} else {
  cat_minimalpair <- fit %>%
    epred_draws(
      newdata = newdata,
      ndraws = 1000,
      re_formula = ~ (1 + sVOT | Item.MinimalPair))
  write_rds(cat_minimalpair, file = "../models/categorisation_by_min_pair.rds")
}
```



```{r fitted-categorisation-minimal-pair"}
remove_axes_titles <- theme(axis.title.x = element_blank(),
                            axis.title.y = element_blank())
MinPair_plot <-
  cat_minimalpair %>%
  group_by(sVOT, Item.MinimalPair) %>%
  ggplot(aes(x = descale(sVOT, VOT.mean_exp1, VOT.sd_exp1),
             y = .epred, colour = Item.MinimalPair)) +
  stat_lineribbon(alpha = .9, .width = 0.95) +
  scale_y_continuous("Fitted proportion of 't'-responses") +
  scale_color_manual("Minimal Pair", breaks = c("dilltill", "dimtim", "dintin", "diptip"),  
                     values = c("#002699", "#0040ff", "#668cff", "#b3c6ff"),
                     labels = c("dill/till", "dim/tim", "din/tin", "dip/tip")) +
  scale_fill_brewer("CI", palette = "Greys", type = "qual") +
  remove_axes_titles +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```



```{r psychometric-plot, fig.width=3.8, fig.height =2.6, message=FALSE}
p.vot <-
  psychometric_fit_exp1 %>%
  group_by(sVOT) %>%
  summarise(estimate__ = mean(estimate__),
         lower__ = mean(lower__),
         upper__ = mean(upper__)) %>%
  ggplot(aes(x = descale(sVOT, VOT.mean_exp1, VOT.sd_exp1),
             y = estimate__)) +
  scale_x_continuous("VOT (ms)", breaks = scales::pretty_breaks(n = 4), limits = c(-100, 130)) +
  scale_y_continuous("Fitted proportion of 't' responses") +
  geom_ribbon(
    aes(ymin = lower__,
        ymax = upper__), alpha = .08) +
  geom_line(linewidth = 1.5,
            colour = "#333333",
            alpha = .8) +
  geom_errorbarh(
    data = post_sample_norm %>%
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y),
    color = "#333333",
    height = 0,
    alpha = .8,
    size = 1,
    inherit.aes = F) +
  geom_label(data = post_sample_norm %>%
      mutate(y = 0.01, PSE = round(PSE)),
    mapping = aes(x = PSE, y = y, label = PSE),
    color = "#333333",
    size = 1.8,
    label.padding = unit(0.18, "lines"),
    inherit.aes = F) +
  annotate(
    geom = "text",
    x = 75,
    y = 0.01,
    label = paste(round(post_sample_norm[[2]]), "ms", "-", round(post_sample_norm[[3]]), "ms"),
    size = 2.5,
    colour = "darkgray") +
## transformation of by-participant means into empirical logits
   # stat_summary(
   #  data = d.test.excluded %>%
   #    group_by(ParticipantID, Item.VOT) %>%
   #    mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
   #    summarise(Response.EmpiricalLogitVoiceless = qlogis((sum(Response.ProportionVoiceless) + .5)/(length(Response.ProportionVoiceless) + 1))),
   #  mapping = aes(x = Item.VOT,
   #                y = Response.EmpiricalLogitVoiceless),
   #  geom = "pointrange",
   #  fun.data = function(x){mean_cl_boot(x) %>% mutate(across(c(y, ymin, ymax), ~ plogis(.x)))},
   #  size = .4,
   #  colour = "#c4b7a6",
   #  alpha = .6,
   #  inherit.aes = F)
  stat_summary(
    data = d.test.excluded %>%
      group_by(ParticipantID, Item.VOT) %>%
      mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
      summarise(Response.ProportionVoiceless = mean(Response.ProportionVoiceless)),
    mapping = aes(x = Item.VOT,
                  y = Response.ProportionVoiceless),
    geom = "pointrange",
    fun.data = "mean_cl_boot",
    size = .4,
    colour = "#c4b7a6",
    alpha = .6,
    inherit.aes = F) +
  labs(x = "VOT (ms)")
```

(ref:fitted-categorisation-plots) Categorisation functions and points of subjective equality (PSE) derived from the Bayesian mixed-effects psychometric model fit to listeners' responses in Experiment 1. The categorization functions include lapse rates and biases. The PSEs correct for lapse rates and lapse biases (i.e., they are the PSEs of the perceptual component of the psychometric model).\protect\footnote{Here and in Experiment 2, lapse biases were close to uniform (.5/.5). For such scenarios, bias-corrected PSEs will be very similar to uncorrected PSEs. This is also evident in Figure \ref{fig:fitted-categorisation-plots}.} **Left:** Effects of VOT, lapse rate, and lapse bias, while marginalizing over trial effects as well as all random effects. Vertical point ranges represent the mean proportion and 95% bootstrapped CIs of participants' "t"-responses at each VOT step. Horizontal point ranges denote the mean and 95% quantile interval of the points of subjective equality (PSE), derived from the 8000 posterior samples of the population parameters. **Right:** The same but showing the fitted categorization functions for each of the four minimal pair continua. Participants' responses are omitted to avoid clutter.   

```{r fitted-categorisation-plots, warning=FALSE, fig.width=6, fig.height = 3.3, fig.cap="(ref:fitted-categorisation-plots)"}
MinPair_plot <- MinPair_plot + labs(x = "VOT (ms)")

xlab <- p.vot$labels$x
p.vot$labels$x <- MinPair_plot$labels$x <- " "

(p.vot | MinPair_plot) +
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "top")
```



```{r by-participant-lapse-bias, warning=FALSE, fig.height=2.6, fig.width=6.5}
lapse_participant <- fit %>%
  spread_draws(r_ParticipantID__theta1[ParticipantID, term], b_theta1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_lapse = b_theta1_Intercept + r_ParticipantID__theta1,
         estimated_lapse = plogis(Participant_lapse) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__theta1, Participant_lapse, estimated_lapse) %>%
  mode_hdci(estimated_lapse)

bias_participant <- fit %>%
  spread_draws(r_ParticipantID__mu1[ParticipantID, term], b_mu1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_bias = b_mu1_Intercept + r_ParticipantID__mu1,
         estimated_bias = plogis(Participant_bias) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__mu1, Participant_bias, estimated_bias) %>%
  mode_hdci(estimated_bias)

estimate_minpair <- fit %>% spread_draws(r_Item.MinimalPair__mu2[Item.MinimalPair, term], b_mu2_Intercept, b_mu2_sVOT) %>%
  group_by(Item.MinimalPair) %>%
  mutate(predicted_eff = ifelse(term == "Intercept", r_Item.MinimalPair__mu2 + b_mu2_Intercept, r_Item.MinimalPair__mu2 + b_mu2_sVOT)) %>%
  group_by(Item.MinimalPair, term) %>%
  mode_hdci(predicted_eff) %>%
  pivot_wider(names_from = term,
              values_from = c(predicted_eff, .lower, .upper))
```

The lapse rate was estimated to be on the slightly larger side, but within the expected range
(`r print_CI(fit, "theta1_Intercept", "theta1_Intercept < 0")`). Maximum a posteriori (MAP) estimates of by-participant lapse rates ranged from XX . Very high lapse rates were estimated for four of the participants with one in particular whose CI indicated exceptionally high uncertainty. These lapse rates might reflect data quality issues with Mechanical Turk that started to emerge over recent years [see @REFS; and, specifically for experiments on speech perception, @cummings-theodore2023], and we return to this issue in Experiment 2.

The response bias were estimated to slightly favor "t"-responses (`r print_CI(fit, "mu1_Intercept", "mu1_Intercept > 0")`), as also visible in Figure \@ref(fig:fitted-categorisation-plots) (left).
Unsurprisingly, the psychometric model suggests high uncertainty about the participant-specific response biases, as it is difficult to reliably estimate participant-specific biases while also accounting for trial and VOT effects (range of by-participant MAP estimates: XX). For all but four participants, the 95% CI includes the hypothesis that responses were unbiased. Of the remaining four participants, three were biased towards "t"-responses and one was biased toward "d"-responses.

There was no convincing evidence of a main effect of trial ($\hat{\beta} =$ `r get_CI(fit, "mu2_sTrial", "mu2_sTrial < 0")`). Given the slight overall bias towards "t"-responses, the direction of this effect indicates that participants converged towards a 50/50 bias as the test phase proceeded. This is also evident in Figure \@ref(fig:fitted-categorisation-plots) (right). In contrast, there was clear evidence for a positive main effect of VOT on the proportion of "t"-responses ($\hat{\beta} =$ `r get_CI(fit, "mu2_sVOT", "mu2_sVOT > 0")`). The effect of VOT was consistent across all minimal pair words as evident from the slopes of the fitted lines by minimal pair \@ref(fig:fitted-categorisation-plots) (left). MAP estimates of by minimal pair slopes ranged from . The by minimal-pair intercepts were more varied (MAP estimates: ) with one of the pairs, dim/tim having a slightly lower intercept resulting in fewer 't'-responses on average. In all, this justifies our assumptions that word pair would not have a substantial effect on categorisation behaviour. From the parameter estimates of the overall fit we obtained the category boundary from the point of subjective equality (PSE) `r( descale(-(summary(fit)$fixed["mu2_Intercept", 1] / summary(fit)$fixed["mu2_sVOT", 1]), VOT.mean_exp1, VOT.sd_exp1) ms)` which we use for the design of Experiment 2.

Finally to accomplish the first goal of experiment 1, we look at the interaction between VOT and trial. There was weak evidence that the effect of VOT decreased across trials ($\hat{\beta} =$ `r get_CI(fit, "mu2_sVOT:sTrial", "mu2_sVOT:sTrial < 0")`). The direction of this change---towards more shallow VOT slopes as the experiment progressed---makes sense since the test stimuli were not informative about the talker's pronunciation. Similar changes throughout prolonged testing have been reported in previous work. [@liu-jaeger2018; @liu-jaeger2019; @REFS].

Overall, there was little evidence that participants substantially changed their categorisation behaviour as the experiment progressed. Still, to err on the cautious side, Experiment 2 employs shorter test phases.



# Session Info

```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
