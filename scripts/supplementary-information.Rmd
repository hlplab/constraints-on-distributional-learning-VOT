```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")

# load required objects that were removed for memory efficiency
fit_test.simple_effects_condition <- readRDS("../models/test-nested-condition.rds")
fit_test.simple_effects_block <- readRDS("../models/test-nested-block.rds")
fit_test <- readRDS("../models/test-standard-priorSD15-0.995.rds")
fit_exposure <- readRDS("../models/exposure-standard-priorSD15-0.999.rds")
```

<!-- Do NOT knit this document. It is part of a larger document. Instead knit the main document (my-apa-formatted-article)
     If you want to separate the SI from the rest of the paper, we recommend you do so AFTER knitting them into a single PDF.
     This will make sure that all references to sections, figures, tables, etc. are working as intended. You can easily separate
     the PDF into two parts, using e.g., Acrobat PDF viewer. //-->

# Supplementary information {-}
\setcounter{section}{0}
\setcounter{footnote}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}
\renewcommand{\thefootnote}{S\arabic{footnote}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\theequation}{S\arabic{equation}}

\changelocaltocdepth{3}
\tableofcontents


# Required software {#sec:software}
Both the main text and these supplementary information (SI) are derived from the same R markdown document available via [https://osf.io/hxcy4/](OSF). It is best viewed using Acrobat Reader. The document was compiled using \texttt{knitr} in RStudio with R:

```{r}
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}).

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", omit = T, pkgs = c("MVBeliefUpdatr", "RJ-2021-048"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. The full session information is provided at the end of this document.

## Interested in using R markdown do create APA formatted documents that integrate your code with your writing?
A project template, including R markdown files that result in APA-formatted PDFs, is available at [https://github.com/hlplab/template-R-project](https://github.com/hlplab/template-R-project). Feedback welcome. We aim to help others avoid the mistakes and detours we made when first deciding to embrace literal coding to increase transparency in our projects.


# Additional information about Materials {#sec:stimulus-generation}
All stimuli are available as part of the OSF repository for this article.

## Recordings
An L1-US English female talker originally from New Hampshire was recruited for recording of the stimuli. She was recorded at the Human Language Processing lab at the Brain & Cognitive Sciences Department, University of Rochester with the help of research assistant (also an L1-US English speaker). She was 23 years old at the time of recording and was judged by the research assistant to have a generic US American accent known as "general American".

Four /d-t/ minimal pairs (*dill*-*till*, *din*-*tin*, *dim*-*tim*, *dip*-*tip*) were recorded together with 20 filler words. These fillers were made up of 10 minimal or near minimal pairs with different sounds at onset. The word pairs were separated into two lists so that they would appear in separate blocks during recording. Each critical pair was repeated 8 times while the filler pairs were repeated 5 times. Word presentation was delivered with PsychoPy [@peirce2007] and the presentation was controlled by the researcher from a computer located outside the recording room. The order of each block was randomized such that target words never appeared consecutively. The talker was instructed to speak clearly and confidently, and to maintain a consistent distance from the microphone.

## Resynthesis
Separate procedures were used to generate stimuli with positive and negative VOTs. We first describe the annotation of the original recordings required by both procedures. Then we describe the two procedures, followed by the annotation approach used for the resynthesized stimuli.

### Annotation of original recordings
All critical pairs of the talker's recordings were annotated for the four cues that are known to affect the perception of word-initial stop-voicing in L1-US English. Durational measurements of pre-voicing, VOT, and vowel duration were taken in addition to the mean F0 of the first 25% of the vowel duration. Annotations were done on Praat and  based on both listening and inspection of the waveform and spectrogram. The annotation boundaries were made following approaches reported in prior studies [e.g. @francis2003; @kirby-ladd2016; @dmitrieva2015; @clayards2017] and through personal communication with trained phoneticians. 

  * pre-voicing (voicing during closure)
      * **start:** the first sign of periodicity in the waveform before closure release.
      * **End:** the point of closure release

  * VOT
      + **start:** the point of closure release.
      + **End:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram.

  * Vowel
      + **start:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram.
      + **End:** when periodicity terminates or at closure onset; if before a lateral, when formant transition approaches steady state; if before a nasal, at the point where formants show a step-wise shift and when intensity shows a steep decline.

### Tokens with positive VOTs
Stimuli with positive VOTs were created using the "progressive cutback and replacement method" (version 31) [@winn2020] implemented in Praat [@boersma2022]. The process takes a voiced token and progressively deletes its vowel onset and replaces it with an approximately equivalent amount of onset taken from the word's voiceless minimal-pair counterpart. Winn's Praat script provides a GUI that greatly simplifies the generation of highly natural sounding stimuli. 

For each minimal pair a continuum of 31 tokens was generated between 0ms and 130ms with a step-size of 5ms. A token of the voiced category from each pair was selected to be the base sound file to make the continuum. All four minimal pair continua were created using the same aspiration sound which was excised from one of the voiceless tokens produced by the talker.  

We set the fundamental frequency (F0) to covary with VOT according to the natural correlation found in the measurements of our test talker's recorded minimal pair tokens. We first ran a linear regression that predicted the talker's F0 values from the measured VOT values. This gave the expected F0 value at every 5 ms interval of VOT. To produce the expected F0 values as the VOT tokens were generated, the start-point F0 value (where $VOT = 0ms$ and $F0 = 246 Hz$)  and the end-point F0 value (where $VOT = 150ms$ and $F0 = 252 Hz$) were entered into the Praat script. The resulting F0 values for each token were not identical across the minimal pair words as shown in Figure \@ref(fig:stimuli-cue-measurements) (the f0 measurements obtained over the first 5ms from vowel onset) but were sufficient for our aim to keep F0 positively correlated with VOT. 

The vowel cut-back ratio was set at 0.33 which translates into 0.33 ms vowel reduction for every 1ms of additional VOT. This ratio followed the estimated vowel duration-VOT trade-off for dip-tip minimal pair tokens reported in @allen-miller1999. The maximum vowel cut-back allowed was 0.5ms to avoid the short vowel in "dip" becoming too short. 

Aspiration intensity was allowed to covary positively with VOT according to the script's default setting. The script manipulates intensity by attenuating the original aspiration sound by a  user-specified value. For a continuum with $n$ steps with the value set to 6 dB (the default) the aspiration sound will be lowered by 6 dB for step 1 (the voiced end of the continuum) and then gradually interpolated to increase in intensity across the continuum so that there will be no attenuation for the $n$th step. Attenuation of the aspiration continua in this experiment was made over 21 steps (0-100ms) which resulted in a range of 55.5 dB to 60.5 dB. Aspiration for stimuli with over 100 ms VOT was maintained at 60.5 dB.

### Tokens with pre-voicing ('negative VOTs')
Stimuli with pre-voicing required a separate approach because Winn's Praat script does not yet support the creation of tokens with pre-voicing that are natural sounding.^[It can however, produce pre-voicing sufficiently well for demonstration purposes (see video demo at https://www.youtube.com/watch?v=-QaQCsyKQyo).] Pre-voicing stimuli were created by prepending pre-voicing generated from naturally produced tokens (described below) that were edited with a separate process.

Pre-voicing in 5ms increments were generated from a a clear pre-voicing waveform excised from a pre-voiced token produced by the talker. To achieve the desired duration a duration factor was first computed and then converted with the "lengthen (overlap-add)" function in Praat based on the PSOLA algorithm. For example, if the desired amount of pre-voicing was 50ms then the duration factor would be 50ms/length of the original pre-voicing sample. Each pre-voicing step was then pre-pended to a token with 0ms VOT. Each of these 0ms tokens was generated with the same aforementioned progressive-cutback-and-replacement Praat script individually. Each 0 ms VOT token would have an F0 value that corresponded to the amount of pre-voicing duration based on the predictions of the linear model. No vowel-cut back was implemented for pre-voiced tokens.

### Annotation of resynthesized stimuli (used in visualizations and for ideal observer/adaptor analyses)
All resynthesized stimuli were annotated for VOT, pre-voicing, vowel duration, and f0 following the same procedure as in the original recordings. We follow previous work on speech perception and combined the pre-voicing and VOT data, treating pre-voicing as negative VOT. This simplifies plotting and analysis. It does, however, make assumptions that we revisit in the general discussion.

For reasons we describe next, we then corrected and simplified the measurements derived from these annotations. We used these corrected measurements for all visualizations of the stimuli (e.g., in Figure \@ref(fig:exposure-means-database-matrix-plot) in the main text) and in our ideal observer and adaptor analyses. In the data shared on OSF, the corrected cue values are stored as Item.CUE_NAME (e.g., Item.VOT) and the measured values are stored as Item.CUE_NAME.measured (e.g., Item.VOT.measured). 

We used the *corrected* rather than *measured* f0 and vowel duration values. This removes measurement noise that might be introduced by the annotation procedure, and makes f0 and vowel duration values more akin to our VOT values. Specifically, the f0 values were linearly interpolated between the f0 values of the voiced and voiceless tokens of the minimal pair. For vowel durations, we used the vowel durations of the *dip*-*tip* pair, which sidesteps the difficulty of annotating the boundary between the vowel and the coda for *din*-*tin* and *dill*-*till* word pairs. This assumes that listeners perceive the same vowel duration for the different words that have the same VOT duration. This assumption is likely a reasonable approximation given that the stimuli were generated from word recordings that were made in the same context and spoken by the same individual in a similar speech style and rate. Figure \@ref(fig:plot-exposure-stimuli-cues) shows uncorrected and corrected measurements of all stimuli while Table \@ref(tab:exposure-stimuli-cue-correlations) shows their correlation values after correction.

(ref:plot-exposure-stimuli-cues) Correlations of VOT, f0 and vowel duration of the synthesized stimuli **A)** before and **B)** after measurement corrections.

```{r plot-exposure-stimuli-cues, fig.height=5.5, fig.width=10, fig.cap="(ref:plot-exposure-stimuli-cues)"}
cue_measurements <- 
  read_csv("../data/exposure_stimuli_cue_measurements.csv", show_col_types = F) %>% 
  rename(f0.target = Item.f0,
         f0.measured = Item.f0.measured,
         VowelDuration.target = Item.VowelDuration,
         VowelDuration.measured = Item.VowelDuration.measured,
         VOT = Item.VOT) %>% 
  select(-Item.VOT.measured, Item.Filename) %>% 
  pivot_longer(
    cols = 4:7,
    names_to = c(".value", "measurement"),
    names_pattern = "(.*)\\.(.*)")

(cue_measurements %>% 
  plot_exposure_stim_cues(measurement = "measured")) +
(cue_measurements %>% 
  plot_exposure_stim_cues(measurement = "target")) + 
  plot_annotation(tag_levels = 'A', tag_suffix = ")") +
  plot_layout(guides = "collect") &
  theme(legend.position = "top",
        legend.justification = "right",
        legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold"))
```

```{r exposure-stimuli-cue-correlations, fig.cap="cue correlations of synthesised stimuli", fig.cap="(ref:exposure-stimuli-cue-correlations)"}
round(cor(d.for_analysis %>% 
      distinct(VOT, f0_Mel, vowel_duration) %>% 
        rename(f0 = f0_Mel, `Vowel duration` = vowel_duration)), 3) %>% 
  as.data.frame() %>% 
  kable(
    format = "simple",
    col.names = c("Cues", "VOT", "f0", "Vowel duration"),
    align = "c",
    caption = "Cue correlations after measurement corrections")
```

# Additional information on participant exclusions {#sec:exclusions}
Beyond the participants mentioned in the main text, an additional 115 participants previewed the front page of the experiment but did not start or complete it. Our technical setup did not allow us to distinguish between participants who started the experiment and did not complete it, and participants who only previewed the experiment. We suspect though that the clear majority of the 115 participants falls into the latter category: crowdsourcing participants often preview multiple experiments and then select one to complete. Unlike in lab-based experiments, for which participants' right to stop the experiment at any point can be costly---both in terms of effort and perceived social cost---exercising this right in web-based experiments is essentially cost free. One appealing feature of the Prolific platform is that it records these numbers (unlike some other platforms). We report them here in the hope that this will become standard. Viewed across different studies, these data will have the potential to identify unintended biases in participant recruiting.

Next, we provide additional information on participants' performance during catch trials and on labelled trials (for which there is a clearly correct response). Both of these measures were used to exclude participants.

## Performance on catch and labelled trials
Of the 122 participants that completed the experiment, 1 committed more than 3 errors (< 84% accuracy). Labelled trials provided another measure to check participant attention during the experiment. Performance on labelled trials was good across all exposure conditions with participants in the +40 condition committing the most errors. This is not unexpected as the +40 condition had the furthest shifted with the most unexpected cue-category mappings. 

(ref:plot-catch-labeled-trial-performance) Total errors committed on catch trials and labelled trials. Percentage values indicate the proportion of participants in the respective groups.

```{r plot-catch-labeled-trial-performance, fig.height=4, fig.width=8, fig.cap="(ref:plot-catch-labeled-trial-performance)", message=F}

prop.catch.errors <- 
  d %>%
  group_by(ParticipantID) %>%
  summarise(Catch.errors = 18 - sum(CatchTrial.Correct, na.rm = T)) %>%
  group_by(Catch.errors) %>% 
  summarise(total = n()) %>% 
  bind_cols(
    d %>%
  group_by(ParticipantID) %>% 
  summarise() %>% 
  count() %>% 
    mutate(rows = 3) %>% 
    uncount(rows)) %>% 
  mutate(proportion = percent(total/n))


p.catch <- 
  d %>%
  group_by(ParticipantID) %>%
  summarise(Catch.errors = 18 - sum(CatchTrial.Correct, na.rm = T)) %>%
  ggplot(aes(x = Catch.errors)) +
  geom_bar(width = .5) +
  geom_text(
    prop.catch.errors,
    mapping = aes(x = Catch.errors, y = total, label = proportion), 
    position = position_dodge(.5), 
    size = 5.5,
    label.size = .01,
    size.unit = "pt",
    vjust = -.5,
    inherit.aes = F) +
  scale_x_continuous("Total catch trial errors", breaks = seq(0, 6, 1)) +
  scale_y_continuous("Number of participants", breaks = seq(0, 120, 10))

prop.labelled.errors <- 
  d %>%
  filter(Phase == "exposure", 
         Item.Labeled == T) %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  summarise(LabeledTrial.Errors = 24 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>% 
  group_by(Condition.Exposure, LabeledTrial.Errors, Block) %>% 
  count() %>% 
  left_join(
    d %>% 
      group_by(Condition.Exposure, ParticipantID) %>% 
      summarise() %>% 
      count() %>% 
      rename(n_subjects = n)) %>% 
  mutate(proportion = percent(n/n_subjects),
         Block = factor(Block))

p.labelled.group <- 
  d %>%
  filter(Phase == "exposure" & Item.Labeled == T) %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  summarise(LabeledTrial.Errors = 24 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>% 
  ggplot(aes(x = LabeledTrial.Errors, fill = Condition.Exposure)) +
  geom_bar(position = position_dodge(width = .5, preserve = "single"), width = .5) +
  geom_text(
    data = prop.labelled.errors,
    mapping = aes(x = LabeledTrial.Errors, y = n, label = proportion, group = Condition.Exposure), 
    position = position_dodge(.5), 
    size = 5.5,
    label.size = .01,
    size.unit = "pt",
    vjust = -.5,
    inherit.aes = F) +
  scale_colour_manual("Condition",
                      aesthetics = c("colour", "fill"),
                      labels = c("baseline", "+10", "+40"),
                      values = colours.condition) +
  scale_x_continuous("Total labelled trial errors") +
  coord_cartesian(ylim = c(0, 45)) +
  theme(legend.position = "top") +
  guides(fill = "none") +
  remove_y_title +
  facet_grid(Condition.Exposure ~ Block, 
             labeller = labeller(
    .cols = c("2" = "Exposure 1", "4" = "Exposure 2", "6" = "Exposure 3"), 
    .rows = c(Shift0 = "baseline", Shift10 = "+10", Shift40 = "+40")))   

p.catch | p.labelled.group
```

# Obtaining predictions of idealized listeners
In the main text, we employ ideal observers [specifically, the ASP framework described in @xie2023] to relate our results to those expected from idealized listeners. This includes predictions based on a *'typical' listener's prior experience* and predictions based on *idealized learners for each exposure condition* that have fully learned the exposure distributions. The former provides a baseline against which to compare listeners' behavior at the start of the experiment. The latter provides a baseline against which to compare changes in listeners' behavior as a function of exposure. In this section, we describe how we derived those predictions.

## Idealized pre-exposure listeners {#sec:idealized-prior-listeners}
Estimates of listeners' prior expectations about the realization of phonetic categories require at least three ingredients: (1) a database that can be reasonably assumed to approximate the types of previous experiences that listeners in the experiment would draw on to process the stimuli in the experiment; (2) a model that is used to approximate the computational consequences of listeners' representations derived from previously experienced speech stimuli; and (3) a procedure to estimate / parameterize that model based on the database [see also @feldman2009; @kronrod2016; @norris-mcqueen2008; @persson-jaeger2023; @tan2021; for a helpful review, see @schertz-clare2020]. In this section, we describe the decisions we made regarding this three components.

### A database of L1-US English word-initial /d/ and /t/ productions
We considered two databases to approximate listeners' prior expectations about the phonetic realization of /d/ and /t/ by a female L1 talkers of US English (as the one employed in our experiment). The two databases contain productions of word-initial /d/ and /t/ in isolated word productions and in connected read speech, respectively [@chodroff-wilson2017, available on OSF at https://osf.io/k6djr/].^[We thank Eleanor Chodroff for adding the isolated speech data to OSF, and for prompt and helpful responses to our questions.] The connected speech database includes word-initial stop production by 180 adult L1-US English talkers (102 female),  while the isolated speech database was made up of recordings of isolated utterances of stop-initial CVC syllables by 24 L1-US English talkers [13 female, see @chodroff-wilson2017 pp. 33, 37- 39 for details]. 

We filtered both databases to only the /d/ and /t/ tokens. For both databases we kept only female talkers to match the gender of our test talker. We kept talkers that had at least 15 tokens of each category. We removed tokens with anomalous f0 measurements, including tokens that were likely due to pitch-halving (identified by examining individual pitch plots) as well as tokens with f0 measurements below 150 Hz (as these are likely due to measurement error; Eleanor Chodroff, p.c.). Of the remaining talkers we ensured that an equal number of /d/ and /t/ tokens within each talker were sampled. The sample size was determined by first counting the number of tokens available for each talker and category, and then taking the lower of the two. 

This left a total of `r nrow(d.chodroff_wilson)` tokens from `r length(unique(d.chodroff_wilson$Talker))` female talkers: `r nrow(d.chodroff_wilson.connected)` tokens by `r length(unique(d.chodroff_wilson.connected$Talker))` female talkers from the connected speech database and `r nrow(d.chodroff_wilson.isolated)` tokens by `r length(unique(d.chodroff_wilson.isolated$Talker))` female talkers from the isolated speech database. This is the data shown in Figure \@ref(fig:exposure-means-database-matrix-plot). 

For the predictions for idealized pre-exposure listeners, we decided to focus on the isolated speech data. While this database is substantially smaller (`r length(unique(d.chodroff_wilson.isolated$Talker))` female talkers), the conditions under which the recordings were elicited more closely resemble the conditions used to record our stimuli from which the test stimuli were created. Both recordings were sampled from a similar population pool, i.e. undergraduate native L1-US English speakers and both were recordings of isolated utterances. Nonetheless there were differences in the way talkers were instructed to produce the tokens that may have contributed to differences we see between our stimuli and the database's. The isolated database tokens were CV*t* syllables (e.g., *dot*, *tot*) produced in the context of a carrier phrase "*Say ______ again.*". The participants were told to speak at a normal rate and to make a slight pause after "Say" and before "again" [@chodroff-wilson2014]. The recordings of our talker were made without a carrier phrase.<!-- TO DO: I think it would be good here to perhaps show a condensed version of the distributional plot from the main text + our test & exposure stimuli overlaid (corrected measurements), in order to see how our stimuli compare to the two databases, thereby justifying our choice. -->

### A model of listeners representations of /d/ and /t/ categories
We used the adaptive speech perception (ASP) framework described in @xie2023 as a convenient way to approximate how listeners map the acoustic-phonetic properties of the speech input onto phonetic categories like /d/ and /t/. Following Xie and colleagues, we assume that category representations can be approximated as multivariate Gaussian distributions in the phonetic space. Specifically, we describe /d/ and /t/ as multivariate Gaussian categories of VOT (in ms), f0 (in Mel), and vowel duration (in ms). As mentioned in the main text, we follow previous work and treat pre-voicing as negative VOT, and revisit the consequences of that decision in the general discussion. The choice of multivariate Gaussian categories strikes a compromise between substantially less parsimonious models (e.g., exemplar models, neural networks, LDA) and even more parsimonious models [e.g., independent Gaussians for each phonetic cue, as discussed in more detail in @xie2023]. Also following Xie and colleagues, we included perceptual noise in the ideal observer models. Perceptual noise was assumed to be independent for each phonetic cue. Noise estimates for VOT and f0 were obtained from previous work [@kronrod2016, $sigma_{noise} = 80ms^2$ and $878 Mel^2$, respectively], the perceptual noise for vowel duration was set to the same value as for VOT given that both are durational cues ($sigma_{noise} = 80ms^2$).

### A procedure to fit the parameters of the model
To train ideal observers (or other models of phonetic representations), it is necessary to make assumptions about listeners' representations of phonetic categories. Previous work has often derived estimates under the implicit assumption that listener learn and maintain a single, *talker-independent*, representation for each phonetic category across talkers. This assumption typically has taken one of two forms. One approach is to entirely ignore talker identity during the estimation of phonetic categories. This is illustrated in Figure \@ref(fig:plot-database-pairwise-cue-correlations) which shows the distribution of /d/ and /t/ over VOT, f0, and vowel duration in isolated (top left) and connected (bottom left) speech. Ellipses indicate the phonetic category representations that would be estimated from these data. Another approach is to first normalize each cue within each talker---e.g., by subtracting the talker's cue mean from each token [e.g., @mcmurray2011; @mcmurray-jongman2011; for review, see @apfelbaum-mcmurray2015; @weatherholtz-jaeger2016]. Phonetic categories are then estimated over these normalized cues. Under this approach, cues are interpreted in a talker-dependent way but the phonetic category representations remain talker-independent. The resulting representations are shown in Figure \@ref(fig:plot-database-pairwise-cue-correlations)C and \@ref(fig:plot-database-pairwise-cue-correlations)D. 

```{r}
p.database <- 
  d.chodroff_wilson %>% 
  group_by(category) %>%
  plot_phoneticdb_cues()

p.matrix_isolated <- 
  p.database %+% 
  subset(d.chodroff_wilson %>% filter(speechstyle == "isolated")) +  
  point_overlay(d.for_analysis, d.chodroff_wilson.isolated)

p.matrix_connected <- 
  p.database %+% 
  subset(d.chodroff_wilson %>% filter(speechstyle == "connected")) +  
  point_overlay(d.for_analysis, d.chodroff_wilson.connected)
```

```{r}
# make a normalised version of the plots 
d.chodroff_wilson.norm <-
   bind_rows(d.chodroff_wilson %>%
  filter(speechstyle == "connected") %>% 
  mutate(
    across(
      c("VOT", "f0_Mel", "vowel_duration"),
      function(x) apply_ccure(data = ., cue = substitute(x)))) %>%
  ungroup(),
d.chodroff_wilson %>%
  filter(speechstyle == "isolated") %>% 
  mutate(
    across(
      c("VOT", "f0_Mel", "vowel_duration"),
      function(x) apply_ccure(data = ., cue = substitute(x)))) %>%
  ungroup())

p.database.norm <- 
  d.chodroff_wilson.norm %>% 
  group_by(category) %>%
  plot_phoneticdb_cues()

p.matrix_isolated.norm <- 
  p.database.norm %+% 
  subset(d.chodroff_wilson.norm %>% filter(speechstyle == "isolated")) +  
  point_overlay(d.for_analysis, d.chodroff_wilson.isolated, center = T)

p.matrix_connected.norm <- 
  p.database.norm %+% 
  subset(d.chodroff_wilson.norm %>% filter(speechstyle == "connected")) +  
  point_overlay(d.for_analysis, d.chodroff_wilson.connected, center = T)
```

(ref:plot-database-pairwise-cue-correlations) Placement of test tokens of our experiment relative to distribution of three important cues to word-initial stop-voicing in L1-US English. **Panel A:** Isolated speech of initial /d/ and /t/ syllables by `r length(unique(d.chodroff_wilson.isolated$Talker)) ` female talkers. **Panel B:** Same as **A** but talker-normalized. **Panel C:** Connected speech of word-initial /d/ and /t/ by `r length(unique(d.chodroff_wilson.connected$Talker)) ` female talkers. **Panel D:** Same as **C** but talker-normalized. Following @xie2023, we added the overall cue mean to all talker-normalized tokens in order to show normalized tokens in the same space as unnormalized tokens (thus keeping the phonetic space constant across panels). <!-- TO DO: I actually liked the color version of this plot, but up to you -->

```{r plot-database-pairwise-cue-correlations, fig.height=8.5, fig.width=10, fig.cap="(ref:plot-database-pairwise-cue-correlations)"}
(p.matrix_isolated + p.matrix_isolated.norm) / (p.matrix_connected + p.matrix_connected.norm) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") + 
  plot_layout(guides = "collect") &
  theme(legend.position = "top",
        legend.justification = "right",
        legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold"))
```

The two approaches to talker-independent category representations have different trade-offs. When cues are not talker-normalized, the resulting category representations inherit not only within-category variability but also variability across talker, over-estimating the actual category covariances for any given talker. Normalizing cues addresses this problem. It does, however, remove potentially useful information about the correlation between cues from the signal: compared to unnormalized cues in Panel A, the talker-normalized cues in Panel B contain less information about the covariance of VOT, f0, and vowel duration.

The two approaches also share potentially important shortcomings. Specifically, neither approach is well-suited if the correlation between cues *within* talkers differs from the correlation between talker means of those cues. The /d/ and /t/ categories in Panel A conflate the two source of covariance; the categories in Panel B completely removed any information about covariance in talker means. This is problematic, as there is strong evidence that such correlations exists [@chodroff-wilson2017; @chodroff-wilson2018; @theodore2009; @sonderegger2020], and that listeners have strong expectations about, at least some types of, correlations between talker means [@idemaru-holt2011; @idemaru-holt2020; @schertz2016; @OTHERs].^[Additionally, the category means for /d/ and /t/ (solid points) for unnormalized cues are simply the weighted average of the talkers available in the data. If talkers contribute different amounts of data to the database, this means that the category means might disproportionately depend on a subset of the data.] This issue is illustrated in Figure XXX, which shows that correlation between phonetic cues *within* talkers (Panel A) and the correlations of talker means of the phonetic cues *across* talkers (Panel B).<!-- TO DO: add figure. think about visualization -->  XXX <!-- TO DO: add description . NOTE that even when those two types of correlations are similar, one would have to somehow USE that knowledge when generalizing prior expectations to a new talker. --> For the same reasons, neither of the two approaches considered above captures correlations between talkers' category means and category (co)variances. Both approaches would, for example, miss if category variance generally increases for larger category means (or vice versa).

An alternative to the two approaches described so far is to derive multiple *talker-dependent category representations* [as assumed in e.g., @kleinschmidt-jaeger2015]. Under this approach, expectations for a 'typical' talker are then derived by averaging over the talker-dependent category representations. And expectations for a typical talker of a certain type (e.g., a female talker of a certain age) are derived by averaging over the talker-dependent category representations learned from previous talkers of that type. In the only study we are aware of that has directly compared talker-independent and talker-dependent models of listeners' prior expectations, talker-dependent representations seemed to better describe human behavior [@xie2021cognition], though it is important to keep in mind that this study focused on a single supra-segmental contrast (question vs. statement prosody). 

Given the uncertainty about the most adequate approach, we implemented two of the three alternatives for the present study. For most of the main text, we rely on talker-independent category representations over the *un*normalized distribution of VOT, f0, and vowel duration (Panel A of Figure \@ref(fig:plot-database-pairwise-cue-correlations)). To this end, we obtained the mean and covariance matrices for /d/ and /t/ across all talkers in that unnormalized cue space. This talker-independent model is used to create the predictions for an idealized pre-exposure listeners in, for example, Figure \@ref(fig:plot-fit-PSE)C. Given the small number of talkers in the isolated speech database (`r length(unique(d.chodroff_wilson.isolated$Talker))`), its restriction to University students from a single campus, and the stimuli being limited to selected phonetic contexts this result is unlikely to be representative of a typical listener's exposure to everyday speech. To safeguard against providing a false sense of strength given by this point estimate, we divided the database into five folds that were randomly sampled within all `r nrow(d.chodroff_wilson.isolated %>% distinct(Talker, category))` unique combinations of Talker and category. We then trained five separate ideal observers---one for each of the five folds. This gives us the 95% CIs shown in Figure \@ref(fig:plot-fit-PSE)C.^[An alternative would have been to randomly distribute talkers into the folds. With few talkers in the database this would have meant that each prior PSE estimate would have been based on data from only 2-3 talkers. Since the aim is to simulate an adult L1 US English listener's knowledge the prior should ideally be estimated on as many talkers as possible. The first option strikes the best compromise given the small database we had at hand.]

In the general discussion, however, we present a talker-dependent model to the data from all five folds. For this, we estimated the category means and covariances separately for each talker. This gave us the range of talker-specific PSE estimates shown on the righthand-side of Figure \@ref(fig:plot-IA-human-PSE).


## Idealized learners that have fully learned the exposure distribution {#sec:idealized-learners}
To construct idealized learners that fully learned the exposure distribution, we followed a similar approach as for the idealized listeners. We again used ideal observers to approximate such a learner's category representations. Unlike for the idealized listeners described above, the idealized learners only consider the only cue that was manipulated in the experiment (VOT). This decision was made since we constructed our stimuli such that the other two cues (f0 and vowel duration) were perfectly correlated with VOT, not providing any additional information. For each exposure condition we constructed ideal observers to simulate a learner that fully learned the exposure distribution (i.e. the cumulative distribution after having experienced all 144 trials). Such an idealized learner would therefore categorize the stimuli at test based purely on the statistics of exposure input.

```{r IO-predicted-PSE-bias-correction}
io.bias_correct <-    
  io %>%
  crossing(Phase = c("test", "exposure")) %>% 
  # Include 
  left_join(
    bind_rows(
      d.for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Phase, Item.VOT, Item.f0_Mel, Item.VowelDuration),
      # Since blocks differ in which VOTs are unlabeled, we include all blocks
      # (we are simulating the expected parameters across all exposure blocks)
      d.for_analysis %>%  
        filter(Phase == "exposure", Item.Labeled == F) %>%
        group_by(Condition.Exposure) %>%
        filter(ParticipantID == first(ParticipantID)) %>%
        select(Condition.Exposure, Phase, Item.VOT, Item.f0_Mel, Item.VowelDuration)) %>% 
      mutate(
        x.VOT = Item.VOT,
        x.all = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(..1, ..2, ..3))) %>%
      select(Condition.Exposure, Phase, x.VOT, x.all) %>%
      rename(data_from = Condition.Exposure),
    by = "Phase") %>%
  mutate(x = ifelse(str_starts(Condition.Exposure, "prior"), x.all, x.VOT)) %>%
  select(-c(x.all, x.VOT)) %>%
  nest(x = x)

# Get logistic parameter estimates for a simulated ideal observer listener. Do so for both 
# exposure and test. Even though this turned out to be overkill, we did it because it was 
# theoretically possible that the procedure we use to analyze *listeners'* responses (a 
# logistic model with a linear effect of VOT) introduces a bias into the estimation. Here, 
# we thus apply a variant of that same analysis approach to responses that are sampled from
# the three ideal observer fit on the exposure data (one each per exposure condition).
# 
# Unlike for the analysis of participants' responses, there is no need to use a psychometric
# model since ideal observers have not attentional lapses (i.e., lambda = 0). 
d.IO_intercept.slope.PSE <- 
  rbind(
  # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io.bias_correct %>% 
        filter(Phase == "test"), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "test") %>% 
      crossing(Block = c(1, 3, 5, 7, 8, 9)), 
    # Assess the ideal observers on the VOT locations that occur in the three different 
    # exposure conditions. Only use unlabeled exposure trials, since those are the trials
    # we assess participants' behavior on.
    get_logistic_parameters_from_model(
      io.bias_correct %>% 
        filter(Phase == "exposure"),
      model_col = "io",
      groups = "Condition.Exposure") %>%
      mutate(Phase = "exposure") %>%
      crossing(Block = c(2, 4, 6))) %>%
  select(Condition.Exposure, Phase, data, model_unscaled, Block, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>% 
  mutate(across(c(Phase, Condition.Exposure, Block), factor)) %>% 
  add_block_labels()
```

(ref:plot-IO-logistic-regression-fit) The expected categorization functions predicted by a logistic regression fit to the responses of an idealized learner by exposure condition and phase (solid lines). The idealizes learner's proportion of "t"-responses (dashed lines) converge with the logistic regression fit along the assessed VOT region and are therefore partially obscured by the logistic regression fitted lines. Grey arrows point to expected PSEs of the logistic regression fit. 

```{r plot-IO-logistic-regression-fit, fig.height=base.height*3+1/2, fig.width=base.width*4, fig.cap="(ref:plot-IO-logistic-regression-fit)"}
tokens <- 
  bind_rows(
    d.for_analysis %>%  
      filter(Phase == "test") %>%
      distinct(Condition.Exposure, Phase, Item.VOT) %>% 
      rename(VOT = Item.VOT),
    # get the unlabeled exposure stimuli and their 3 cues
    # Since blocks differ in which VOTs are unlabeled, we include all blocks
    # (we are simulating the expected parameters across all exposure blocks)
    d.for_analysis %>%  
      filter(Phase == "exposure", Item.Labeled == F) %>%
      group_by(Condition.Exposure) %>%
      filter(ParticipantID == first(ParticipantID)) %>%
      select(Condition.Exposure, Phase, Item.VOT) %>% 
      rename(VOT = Item.VOT)) %>% 
  select(Condition.Exposure, Phase, VOT)

arrow_df <- 
  d.IO_intercept.slope.PSE %>% 
  filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")) %>% 
  select(Condition.Exposure, Phase, PSE) %>%
  group_by(Condition.Exposure, Phase) %>% 
  slice_head(n = 1) %>% 
  mutate(x = PSE, xend = PSE, y = .5, yend = .01)  

d.IO_intercept.slope.PSE %>% 
  filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")) %>% 
  select(Condition.Exposure, Phase, model_unscaled) %>% 
  group_by(Condition.Exposure, Phase) %>% 
  slice_head(n = 1) %>% 
  # append VOT values for logistic regression models to predict
  cross_join(
    tibble(
      VOT = seq(-50, 150, 5))) %>% 
  nest(stimuli = VOT) %>% 
  mutate(prediction = map2(.x = stimuli, .y = model_unscaled, ~ predict(.y, newdata = .x))) %>% 
  unnest(c(stimuli, prediction)) %>% 
  mutate(proportion_t_responses = plogis(prediction)) %>% 
  ggplot(aes(x = VOT, y = proportion_t_responses, colour = Condition.Exposure)) +
  geom_line(linewidth = 1, alpha = .3) + 
  geom_line(
    data = io.bias_correct %>% 
      filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")) %>% 
      select(-x) %>% 
      # append VOT values for IO to categorise
      group_by(Condition.Exposure, Phase) %>% 
      cross_join(
        tibble(x = seq(-50, 150, 5))) %>% 
      nest(x = x) %>% 
      mutate(categorisation = 
               map2(x, io, ~ 
                      get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% 
                      mutate(VOT = map_dbl(x, ~ .x[1])))) %>% 
      unnest(categorisation, names_repair = "unique") %>% 
      filter(category == "/t/"),
    mapping = aes(x = VOT, y = response, colour = Condition.Exposure),
    alpha = .5,
    linewidth = 1,
    linetype = 2,
    inherit.aes = F) +
  geom_rug(
    data = tokens %>% group_by(Condition.Exposure, Phase),
    mapping = aes(x = VOT, colour = Condition.Exposure),
    alpha = .4,
    linewidth = .8,
    inherit.aes = F) +
  geom_segment(data = arrow_df,
          mapping = aes(x = x, xend = xend, y = y , yend = yend, group = Phase),
           colour = "grey",
          alpha = 0.8,
           arrow = arrow(type = "open" , length = unit(0.04, "npc"))) +
  geom_text(data = arrow_df, 
            mapping = aes(x = x+2.5, y = .2, label = round(PSE, 2)),
            size = 2.5) +
  scale_x_continuous(breaks = seq(-50, 130, 20)) +
  scale_y_continuous('Proportion "t"-responses') +
  facet_wrap( ~ Phase, ncol = 1) +
  scale_color_manual("Exposure condition", labels = c("baseline", "+10", "+40"), values = colours.condition) +
  theme(legend.position = "top")

rm(arrow_df, tokens)
```

## Putting models and listeners on the same scale: equating potential biases in the estimation of intercepts, slopes, and PSEs {#sec:io-bias-correction}
Figure \@ref(fig:plot-fit-PSE)C in the main text shows PSE estimates for the participants in our experiments. These estimates are based on the perceptual model contained in the Bayesian psychometric mixed-effects model we used to analyze participants' responses, and thus reflect PSEs that are corrected for the rate of attentional lapses. Of note, this perceptual model assumes linear effects of VOT on the log-odds of "t"-responses. We made this assumptions for the sake of simplicity, and in order to avoid over-fitting. On the one hand, auxiliary analyses presented in section \@ref(sec:GAMM) replicate all tendencies reported in the main text while relaxing the linearity assumption. On the other hand, it is possible that the linearity assumption introduced biases into the estimation of participants' intercepts, slopes, and PSEs. Critically, the ideal observers that we use to describe idealized listeners before and after exposure predict non-linear effects of VOT [since /d/ and /t/ do *not* have equal variances; for details, see [@bicknell2024; @kleinschmidt-jaeger2015; @kronrod2016]. Thus, instead of directly calculating predicted intercepts, slopes, and PSEs from the ideal observers, we estimated their intercepts, slopes, and PSEs paralleling the analysis approach for the human data. This makes sure that any biases in the estimation of human intercepts, slopes, and PSEs that are introduced by our analysis approach are also taken into account for the idealized listeners and learners.

Specifically, we applied the following procedure to each of the five idealized observers representing idealized listeners prior to exposure (one for each cross-validation fold), and each of the three ideal observers representing idealized learners (one for the three exposure condition). We first sampled `r 10^12 `posterior responses from the ideal observer at each of the VOT steps. For example, for test blocks, we sampled `r 10^12 ` responses from the model at each of the 12 VOT steps. These responses were sampled while assuming a lapse rate of 0 and a uniform response bias. We then fit a logistic regression model---predicting "t"-responses---to the sampled responses, with VOT as the predictor and the sampled responses as the outcome. By emulating a zero lapse rate and using an ordinary logistic regression, we derive estimates of intercepts, slopes, and PSEs for each ideal observer that are directly comparable to the estimates for human listeners (which reflect the *lapse-corrected* PSE of the perceptual model component of the psychometric mixed-effects model fit to listeners' responses). 

Figure \@ref(fig:plot-fit-PSE)C in the main text presents prediction lines that were estimated based on the test blocks. This decision was made since procedure described here produced very similar bias-corrected predictions for exposure and test blocks, and we felt that introducing all complexity described here in the main text would distract from the main message of the paper. Figure \@ref(fig:plot-fit-pars) shows a version of Figure \@ref(fig:plot-fit-PSE), in which the prediction lines and ribbons have been adjusted separately for each block. In this variant of the figure, we also include an additional panel that shows the block-by-block changes in the *intercepts* and *slopes* of listeners' categorization functions, as well as the ideal observers' predictions for these parameters (discussed further in sections \@ref(sec:slopes-analyses-test) and \@ref(sec:slopes-analyses-exposure)). 

(ref:plot-fit-pars) Same as Figure \@ref(fig:plot-fit-PSE) but also showing block-to-block changes in the *intercepts* and *slopes* of listeners' categorization function (PSE = -intercept / slope). Additionally, the prediction lines for idealized learners are adjusted on a block-by-block basis for potential biases in the estimation of intercepts, slopes, and PSEs that might be introduced by the linearity assumption of the psychometric mixed-effects model (that effects of VOT on listeners' responses are linear in log-odds of responding "t").

```{r prepare-plot-intercepts-slopes-SI}
p.intercept_1to9 <-
  p.across_blocks +  
  geom_rect(
    data = 
      d.IO_intercept.slope.PSE %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      add_block_labels() %>% 
      group_by(Block.plot_label) %>%
      summarise(
        intercept_unscaled.lower = mean(intercept_unscaled) - sd(intercept_unscaled) / sqrt(length(intercept_unscaled)) * 1.96, 
        intercept_unscaled.upper = mean(intercept_unscaled) + sd(intercept_unscaled) / sqrt(length(intercept_unscaled)) * 1.96,
        intercept_scaled.lower = mean(intercept_scaled) - sd(intercept_scaled) / sqrt(length(intercept_scaled)) * 1.96, 
        intercept_scaled.upper = mean(intercept_scaled) + sd(intercept_scaled) / sqrt(length(intercept_scaled)) * 1.96) %>% filter(Block.plot_label %in% c("Test 1", "Exposure 1")),
    mapping = aes(xmin = -Inf, xmax= Inf, ymin = intercept_unscaled.lower, ymax = intercept_unscaled.upper),
    fill = "#d2d4dc",
    alpha = .3,
    inherit.aes = F) +
  geom_step(
    data = d.IO_intercept.slope.PSE %>% filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")), 
    mapping = aes(x = Block.plot_label, y = intercept_scaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.5, 
    inherit.aes = F) +
  scale_y_continuous("Intercept (log-odds)")

p.slope_1to9 <-
  # Plotting participants' and ideal observers' slopes. We're transforming both types of slopes back
  # into the unit ms VOT. This makes sure that the different slopes are actually comparable across
  # exposure and test blocks (participants) and ideal observers, each of which (might) have different
  # SDs that were used during the calculation of the scaled slopes.
  #
  # (scaled slopes are good for *effect size* comparison but---perhaps confusingly---that's not the
  # same as making sure that the effects are actually expressed in the same units.)
  p.across_blocks +  
  # Using UNscaled slope because slopes are comparable across scales anyway and this is more intuitive
  # (remove _unscaled to switch to slopes over Gelman-scaled VOT)
  aes(y = slope_unscaled, ymin = slope_unscaled.lower, ymax = slope_unscaled.upper) +
  # TO DO: figure out why these ribbons aren't working. Perhaps x needs to be continuous?
  # Alternatively, make a sequence of rectangles, one for each block
  geom_ribbon(
    data = 
      d.IO_intercept.slope.PSE %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      group_by(Block.plot_label) %>%
      summarise(
        slope_unscaled.lower = mean(slope_unscaled) - sd(slope_unscaled) / sqrt(length(slope_unscaled)) * 1.96, 
        slope_unscaled.upper = mean(slope_unscaled) + sd(slope_unscaled) / sqrt(length(slope_unscaled)) * 1.96) %>% filter(Block.plot_label %in% c("Test 1", "Exposure 1")),
    mapping = aes(x = Block.plot_label, ymin = slope_unscaled.lower, ymax = slope_unscaled.upper),
    fill = "#d2d4dc",
    alpha = .3,
    inherit.aes = F) +
  geom_step(
    data = d.IO_intercept.slope.PSE %>% 
      filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")), 
    # Using UNscaled slope because slopes are comparable across scales anyway and this is more intuitive
    # (substitute _unscaled to _scaled to switch to slopes over Gelman-scaled VOT)
    mapping = aes(x = Block.plot_label, y = slope_unscaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.5,
    position = position_nudge(x = -.5),
    inherit.aes = F) +
  geom_segment(
    data = d.IO_intercept.slope.PSE %>% 
  filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")) %>% 
  select(c(slope_unscaled, Condition.Exposure, Block, Block.plot_label)) %>% 
    filter(Block == 9),
  mapping = aes(x = 8.5, xend = 9.5, y = slope_unscaled, yend = slope_unscaled),
  linetype = 2,
  linewidth = .8,
  alpha = .5,
  colour = colours.condition,
  inherit.aes = F) +
  scale_y_continuous("slope (log-odds/ms VOT)")


p.estimates_1to7 <-  
  (p.intercept_1to9 | p.slope_1to9) +
  plot_layout(
    guides = "collect", 
    design = "
DDDDEEEE
") &
  theme(legend.position = "none", axis.text = element_text(size = 8))
```

```{r plot-fit-pars, fig.width=12.5, fig.height=9, fig.cap="(ref:plot-fit-pars)"}
# TO DO: something is of with the background of the Test panels in panel A
p.fit_1to7 + p.fit_7to9 + p.PSE_1to9 + p.estimates_1to7 +
  plot_layout(
    design = "
AAAAAAAAA#
BBB##CCCCC
#####CCCCC
DDDDDDDDDD
DDDDDDDDDD
DDDDDDDDDD
",
heights = c(1, 1, 2)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

# The Bayesian psychometric mixed-effects model: additional information and hypothesis tests
We first present some additional information about the psychometric mixed-effects model used for the analysis of participants' categorization responses. Then we present detailed summary tables of the Bayesian hypothesis tests about listeners' *PSE* for the test and exposure blocks, followed by parallel summary tables for the *slope* of listeners' categorization function. Then, we present an auxiliary analysis that assess changes in lapse rates across the exposure and test blocks (the main analyses assumed constant lapse rates within each type of block). These analyses validate our decision to assume constant lapse rates within each type of block. Finally, we present analyses that combined the exposure and test data, while relaxing the linearity assumption for the effects of VOT. All analyses presented in this section are variants of the same psychometric mixed-effects model, which we introduce next.

## Additional information about the Bayesian psychometric mixed-effects model {#sec:analysis-approach}
We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric model [@kuss2005; @prins2011; @prins2019; @schutt2015]. The psychometric model is an extension of mixed-effects logistic regression that also takes into account attentional lapses. The mixed-effects psychometric model describes the probability of "t"-responses as a weighted mixture of a perceptual and a lapsing model. The perceptual model predicts responses on trials where participants pay attention and respond based on the stimulus. We implemented the perceptual model as a mixed-effects logistic regression, predicting "t"-responses from exposure condition (backward difference coded, comparing the +10ms against the baseline condition, and the +40ms against the +10ms shift condition), test block (backward difference coded from the first to last test block), VOT (Gelman scaled), and their full factorial interaction. The model included by-participant random intercepts and slopes for all within-participant manipulations (block and VOT) and by-item random intercepts and slopes for all within-participant manipulations (exposure condition, block, VOT).

We fit the two psychometric models for exposure and test blocks using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio].^[Here and throughout the text, we refer to one model for the test blocks and one model for the exposure blocks. However, more specifically, we fit several variants of each model that only differed in the way that predictors were nested within the perceptual model. In addition to the standard formulation of the perceptual model, which expressed the effect the effects of exposure condition, block, and VOT as the full factorial interactions (\texttt{response $\sim$ condition * block * VOT}), we also fit variants that yielded the simple effects of block within each condition (\texttt{response $\sim$ condition / (block * VOT)}), the simple effects of condition within each block (\texttt{response $\sim$ block / (condition * VOT)}), or separate intercept and VOT slope estimates for each exposure condition and block (\texttt{response $\sim$ 0 + (condition * block) / VOT}). All of these variants are prediction-equivalent, but each of them facilitate the formulation of different hypothesis tests of relevance to our questions. For details, we refer to the R markdown document that this SI is generated from. The R markdown contains the R code used to specify the different models and hypothesis tests.] Predictor coding and priors were identical across both models. To facilitate comparison of effect sizes across predictors, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008]. We also centered VOT based on the mean of the *test* blocks. This makes sure that all other effects---e.g., the effects of exposure condition and block---are analyzed at the same VOT across the two separate models. Following previous work from our lab [@horberg-jaeger2021; @xie2021jep] we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we used Student priors centered around zero with a scale of 2.5 units [following @gelman-prior2008] and 3 degrees of freedom, with the exception of VOT which had a wider scale of 15 units because of convergence issues (see \@ref(sec:SI-exposure-block-analysis) for elaboration). For the lapse rate we used the \texttt{brms} default logistic prior centered around 0 with a scale of 1 unit. This assumes uniformity for $0 < lapse rates < 1$ while down-weighting the extreme values. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $1 < \hat{R} < 1.01$.

## PSE results for test blocks
The main text contains summary tables for the simple effects of exposure condition within each test block (Table \@ref(hypothesis-table-simple-effects-condition)) and the simple effects of block within each exposure condition (Table \@ref(hypothesis-table-simple-effects-block)). The main text also refers to differences in the rate of block to block changes across exposure conditions. These differences correspond to the interactions between exposure conditions and block in the psychometric mixed-effect model. These interactions are summarized in Table \@ref(tab:hypothesis-table-interaction-condition-block).

### PSE: Differences in the rate of change between exposure conditions and block

```{r hypothesis-table-interaction-condition-block, results='asis'}
hyp_interaction.condition_block <-
  hypothesis(
    fit_test,
    c(
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1) < 0",

      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 > 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",

      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",

      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",

      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
      "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 +
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 +
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",

    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
    "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 +
   mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0"),
   robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

table.interaction.condition_block <-
  make_hyp_table(
    fit_test,
    hyp_interaction.condition_block,
    rep(c(
  # Comparing differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{PSE}$",
  "Block 2 to 3: increased $\\Delta_{PSE}$",
  "Block 3 to 4: increased $\\Delta_{PSE}$",
  "{\\em Block 1 to 4: increased $\\Delta_{PSE}$}",
  "Block 4 to 5: decreased $\\Delta_{PSE}$",
  "Block 5 to 6: decreased $\\Delta_{PSE}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{PSE}$}"), 3),
    caption = "Did the rate of block-to-block changes in PSEs differ across exposure conditions? This table summarizes the interactions between exposure condition and block---specifically, whether the differences between exposure conditions changed from test block to test block.") %>%
  pack_rows("Difference in +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in +40 vs. baseline", 15, 21)

table.interaction.condition_block
```


## PSE results for exposure blocks {#sec:SI-exposure-block-analysis}
While the standard formulation of the psychometric mixed-effects model converged (Tables \@ref(tab:hypothesis-table-exposure-interaction-condition-block) and \@ref(tab:hypothesis-table-exposure-interaction-condition-block-VOT)) the nested formulations from which we obtain simple effects, returned several divergent transitions during sampling. Divergent transitions signal the need for caution in model interpretation because it indicates unreliability of the posterior estimate (see https://mc-stan.org/misc/warnings.html#runtime-warnings for explanation of warning types). 

A possible solution to eliminating divergent transitions in this model would be to relax the variance of the prior distribution assumed for the slope parameter (*VOT_gs* in the model formula). Indeed, in earlier fits of the test phase data we faced convergence issues when we used the recommended weakly regularising prior (student-t distribution with SD = 2.5; @gelman-prior2008) for the slope parameter. Diagnostic plots revealed a slight bi-modality in the posterior distribution which may have been an impediment to smooth sampling; this was solved by increasing the prior distribution's variance to 15 units which allowed the inclusion of data points that were further from the mean.^[A discussion on the theoretical and practical consequences of prior-setting approaches can be found in [@gelman2017].] However, fitting the nested exposure models here with yet more accommodating priors would detract from having an identical set of priors across all models fitted for test and exposure. Given that both the simple effects models returned only a few divergent transitions (3 in the nested block and 9 in the nested condition model) and showed fairly good $\hat{R}$ values ($1< \hat{R} < 1.1$) and ESS values we opted not to explore further solutions for them at this stage. <!--from the stan site: "But if you get only few divergences and you get good Rhat and ESS values, the resulting posterior is often good enough to move forward."  -->

```{r fit-exposure-simple-effects-condition, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block2:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block4:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block6:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_exposure.simple_effects_condition <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
        (0 + Block / VOT_gs | ParticipantID) +
        (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
      theta1 ~ 1),
    data = d.test_exposure_for_analysis %>%
      filter(Phase == "exposure") %>%
      prepVars(test_mean = test_mean, levels.Condition = levels_Condition.Exposure),
    prior = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .999),
    file = "../models/exposure-nested-condition.rds")
```


```{r fit-exposure-simple-effects-block, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Block2:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Block4:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Block6:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_exposure.simple_effects_block <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Condition.Exposure/(Block * VOT_gs) + 
      (0 + Block * VOT_gs | ParticipantID) + 
      (0 + Condition.Exposure/(Block * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "exposure") %>%
      prepVars(levels.Condition = levels_Condition.Exposure),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .999),
    file ="../models/exposure-nested-block.rds")
```

Consistent with the trend observed in test blocks, categorization boundaries were clearly separated between the conditions in all exposure blocks and grew progressively larger with more exposure (Table \@ref(tab:hypothesis-table-exposure-simple-effects-condition)). Evidence for this effect was stronger in the +40 vs. +10 comparison 
(`r get_bf(fit_exposure.simple_effects_condition, "mu2_Block6:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)` $\leq$ BFs $\leq$ `r get_bf(fit_exposure.simple_effects_condition, "mu2_Block4:Condition.Exposure_Shift40vs.Shift10 + mu2_Block4:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`) with the difference in PSE reaching its maximum in the final exposure block (`r get_bf(fit_exposure.simple_effects_condition, "mu2_Block6:Condition.Exposure_Shift40vs.Shift10 < 0")`). The difference between +10 and baseline widened in exposure block 2 (`r get_bf(fit_exposure.simple_effects_condition, "mu2_Block4:Condition.Exposure_Shift10vs.Shift0 < 0")`) but narrowed in the final block (`r get_bf(fit_exposure.simple_effects_condition, "mu2_Block6:Condition.Exposure_Shift10vs.Shift0 < 0")`). The smaller difference in PSE between the +10 and baseline conditions reflected the pattern in test blocks. 

Analysis of boundary shifts between exposure blocks show a similar incremental pattern albeit with weaker evidential support (`r get_bf(fit_exposure.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Exposure3vs.Exposure2 > 0", bf = T)` $\leq$ BFs $\leq$ `r get_bf(fit_exposure.simple_effects_block, "mu2_Condition.ExposureShift10:Block_Exposure3vs.Exposure2 > 0", bf = T)` Table \@ref(tab:hypothesis-table-exposure-simple-effects-block)). In the +10 condition, categorization functions shifted leftwards with greater exposure while in the +40 condition categorization moved further rightwards. Categorization in the baseline condition showed an overall smaller shift between exposure blocks 1 to 3.

The effects of VOT (slope) did not change significantly between conditions within each exposure block (Table \@ref(tab:hypothesis-table-exposure-simple-slopes-condition)) and between blocks within exposure condition (Table \@ref(tab:hypothesis-table-exposure-simple-slopes-block)). This conforms to the trend found in slope analyses for test blocks as well as the predictions of the ideal observers. Table \@ref(tab:hypothesis-table-exposure-interaction-condition-block) and Table \@ref(tab:hypothesis-table-exposure-interaction-condition-block-VOT) report the effects of interactions between exposure condition and block (PSE); and exposure, condition, block and VOT respectively (slope). With respect to PSE changes, the difference between +10 and baseline held constant from exposure block 1 to exposure block 2 and increased marginally from exposure block 2 to exposure block 3 (`r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 < 0", est = T)` $\leq$ estimates $\leq$ `r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0", est = T)`; `r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 < 0", bf = T)` $\leq$ BFs $\leq$ `r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0", est = T)`). Between the +40 and +10 conditions, PSE differences increased  with more exposure as did PSE differences between +40 and the baseline condition. This trend reflects that of test blocks but evidential support was weaker than that found in test block comparisons (`r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 < 0", bf = T)` $\leq$ BFs $\leq$ `r get_bf(fit_exposure, "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 < 0", bf = T)`).  

### PSE: Simple effects of condition within each exposure block

```{r hypothesis-table-exposure-simple-effects-condition, results='asis'}
hyp.exposure_simple_effects_condition <- 
  hypothesis(
    fit_exposure.simple_effects_condition, 
    c(
      "mu2_Block2:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block2:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block2:Condition.Exposure_Shift10vs.Shift0 + mu2_Block2:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block4:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block4:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block4:Condition.Exposure_Shift40vs.Shift10 + mu2_Block4:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block6:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block6:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block6:Condition.Exposure_Shift40vs.Shift10 + mu2_Block6:Condition.Exposure_Shift10vs.Shift0 < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  fit_exposure.simple_effects_condition, 
  hyp.exposure_simple_effects_condition, 
  rep(c("+10 vs. baseline < 0", "+40 vs. +10 < 0", "+40 vs. baseline < 0"), 3), 
    caption = "This table summarizes the simple effects of the exposure conditions for each exposure block. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses).") %>% 
  pack_rows("Exposure block 1", 1, 3) %>%
  pack_rows("Exposure block 2", 4, 6) %>% 
  pack_rows("Exposure block 3", 7, 9) 
```

### PSE: Simple effects of block within each exposure condition



```{r hypothesis-table-exposure-simple-effects-block, results='asis'}
hyp.exposure_simple_effects_block <- 
  hypothesis(
    fit_exposure.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Exposure2vs.Exposure1 > 0",
      "mu2_Condition.ExposureShift0:Block_Exposure3vs.Exposure2 > 0",
      "mu2_Condition.ExposureShift0:Block_Exposure2vs.Exposure1 + mu2_Condition.ExposureShift0:Block_Exposure3vs.Exposure2 > 0",

      "mu2_Condition.ExposureShift10:Block_Exposure2vs.Exposure1 > 0",
      "mu2_Condition.ExposureShift10:Block_Exposure3vs.Exposure2 > 0",
      "mu2_Condition.ExposureShift10:Block_Exposure2vs.Exposure1 + mu2_Condition.ExposureShift10:Block_Exposure3vs.Exposure2 > 0",

      "mu2_Condition.ExposureShift40:Block_Exposure2vs.Exposure1 < 0",
      "mu2_Condition.ExposureShift40:Block_Exposure3vs.Exposure2 < 0",
      "mu2_Condition.ExposureShift40:Block_Exposure2vs.Exposure1 + mu2_Condition.ExposureShift40:Block_Exposure3vs.Exposure2 < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

  make_hyp_table(
    fit_exposure.simple_effects_block,
    hyp.exposure_simple_effects_block,
    c(rep(c(
      # Comparing differences blocks within conditions
      "Block 1 to 2: decreased PSE",
      "Block 2 to 3: decreased PSE",
      "{\\em Block 1 to 3: decreased PSE}"), 2),
      "Block 1 to 2: increased PSE",
      "Block 2 to 3: increased PSE",
      "{\\em Block 1 to 3: increased PSE}"),
    caption = "Was there incremental change from exposure block 1 to 3? This table summarizes the simple effects of block for each exposure condition. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses).") %>% 
  pack_rows("Difference between blocks: baseline", 1, 3) %>%
  pack_rows("Difference between blocks: +10", 4, 6) %>%
  pack_rows("Difference between blocks: +40", 7, 9) 
```
\pagebreak

### PSE: Differences in the rate of change between exposure conditions and block


```{r hypothesis-table-exposure-interaction-condition-block, results='asis'}
hyp.interaction.exposure.condition_block <- 
  hypothesis(
    fit_exposure, 
    c(
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0",
      
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

  make_hyp_table(
    fit_exposure,
    hyp.interaction.exposure.condition_block,
    rep(c(
  # Comparing differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{PSE}$",
  "Block 2 to 3: increased $\\Delta_{PSE}$",
  "{\\em Block 1 to 3: increased $\\Delta_{PSE}$}"), 3),
    caption = "Did the rate of block-to-block changes in PSEs differ between exposure conditions? This table summarizes the interactions between exposure condition and block---specifically, whether the differences between exposure conditions changed from exposure block to exposure block.") %>%
  pack_rows("Difference in +10 vs. baseline", 1, 3) %>%
  pack_rows("Difference in +40 vs. +10", 4, 6) %>% 
  pack_rows("Difference in +40 vs. baseline", 7, 9)
```
\pagebreak

## Slope results for test blocks {#sec:slopes-analyses-test}
In the main text, we focused on 'shifts' in listeners' categorization function---i.e., changes in listeners' PSEs. Changes in the *slope* of the categorization function, or lack thereof, have received comparatively little attention in previous work [though see @clayards2008; @theodore-monto2019]. They are, however, an important part of the empirical facts that theories of speech perception need to account for [see also @kleinschmidt2020]. While the present experiment was not primarily intended to test hypotheses about slope changes, we make a couple of post-hoc observations based on Bayesian hypothesis tests that parallel those for PSEs. Tables \@ref(tab:hypothesis-table-simple-slopes-condition)-\@ref(tab:hypothesis-table-interaction-condition-block-VOT) summarize Bayesian hypothesis tests about slopes that compare differences between conditions within blocks (Table \@ref(tab:hypothesis-table-simple-slopes-condition)), changes between blocks within conditions (Table \@ref(tab:hypothesis-table-simple-slopes-block)), and interactions of exposure conditions and blocks (Table \@ref(tab:hypothesis-table-interaction-condition-block-VOT)). 

Our exposure conditions only manipulated the *absolute location* of category means, while holding constant both the relative distance between category means and the relative distance of each exposure token from those means (and thus also the category variances). Any model of adaptive speech perception that relies on distributional learning would thus predict little to no effect of exposure condition on the slope of listeners' categorization functions.^[If our experiment only contained exposure blocks, no effect of exposure condition would be predicted. However, our experiment also contained test blocks, which did *not* differ between exposure conditions. To the extent that listeners attributed test trials to category /d/ or /t/, this can introduce differences in the category variances across exposure conditions. Overall, we would expect these differences to be small for two reasons: (1) test trials were unlabeled, leaving listeners with uncertainty as to *which* category to attribute them to, and (2) test blocks were shorter than exposure blocks (by the start of Test 4, listeners had heard 36 test trials and 144 exposure trials).] This includes, for example, exemplar models [@johnson1997] and Bayesian ideal adaptors [@kleinschmidt-jaeger2015].

This prediction was confirmed both prior to exposure and following exposure. Paralleling PSEs, there was evidence that slopes did not differ prior to exposure (top of Table \@ref(tab:hypothesis-table-simple-slopes-condition)), though the strength of this evidence was at best anecdotal, and thus weaker than for PSEs. The weak support for the null is not particularly surprising given the fact that we used regularizing priors. Such priors have their highest density over the null, weakening the ability to detect support for the null through the Savage-Dickey method (which compare the in/decrease of the posterior, compared to the prior, density over the null). Additionally, there are at least two *a priori* reasons to expect that our paradigm would lead to high estimation uncertainty for the slopes (estimation of which the paradigm was not intended to prioritize). First, the VOT steps during test were at least 5ms apart. For subjects that have highly categorical perception---as was the case for many of our subjects---this makes it difficult to precisely estimate the actual slope. Second, our analysis approach, which assumed *linear* effects of VOT. Contrary to this simplifying assumption, there are reasons to expect quadratic effects of VOT [e.g., @bicknell2024; @bushong-jaeger2019; @bushong-jaeger2024]. It's possible that our simplifying assumption introduced additional estimation uncertainty. 

In contrast to PSEs, there was also little evidence that slopes differed between exposure conditions at any point after exposure (remainder of Table \@ref(tab:hypothesis-table-simple-slopes-condition); see also Figure \@ref(fig:plot-fit-PSE)). Compared to the changes in PSEs (Figure \@ref(fig:plot-fit-slope-PSE)D), the *slopes* of listeners' categorization functions were similar across exposure conditions: the 95% CIs of the differences across conditions included 0 for all comparisons and all test blocks. BFs favored the null in all but two comparisons, though support for the null was again never more than anecdotal.


<!-- THIS NEEDS TO STILL BE EDITED AN INTEGRATED. Also consider consolidates slope section for test and exposure



First, the slopes of listeners' categorization functions in Panel C approximate those predicted by the idealized learner models: many of the 95% CIs overlap with the dashed lines. At the same time, listeners' slopes do not seem to fully converge against those expected from an idealized learner.^[Without the inclusion of perceptual noise, ideal observers predict much steeper categorization functions [see also @feldman2009; @kronrod2016]. This offers a potential explanation for the mismatch between the ideal observer predictions and human categorization responses when perceptual noise is not considered [@clayards2008].] 



This is expected under distributional learning models of adaptive speech perception [@kleinschmidt-jaeger2015], given that the exposure variances of /d/ and /t/---unlike their means---were held constant across all three exposure conditions. Slopes also changed comparatively little relative to listeners' categorization responses in Test 1 ($0.6 \leq$ BFs $\leq 4$; see SI, \@ref(sec:SI-slope-tests)). This finding, too, is broadly compatible with distributional learning models of adaptive speech perception, given that the exposure variances of /d/ and /t/ were designed to approximate the variance of /d/ and /t/ in typical speech input (see Methods). Credible intervals of the pre-exposure slopes in Test 1 overlap with the predictions of the ideal observer model introduced in the preceding paragraph (gray dashed lines in  Figure \@ref(fig:plot-fit-slope-PSE)C; we discuss the colored dashed lines in the next section). It is, however, worth noting that listeners' slopes in all three exposure conditions were consistently larger than those predicted by the ideal observer. 

while prior and posterior predictions are not a great fit, relative ordering of prior and posterior predictions is actually in line with listeners behavior. One possible explanation is that the perceptual noise estimate derived by Kronrod et al is an over-estimate. Lowering the perceptual noise should increase the slope making for a better fit with human responses (it should also shift the PSE towards /d/, I think).

 -->
 
<!-- TO DO: discuss reasons for slope discrepancies? 1) slope changes with perceptual noise. our estimates of noise might be wrong. in particular, we assume the both temporal cues (VOT and vowel duration) are subject to identical and independent noise. 2) our estimates of category variance are based on recordings that contain a variety of phonetic contexts, whereas listeners in our stimuli only experienced one type of context /i/. if listeners normalize cues based on phonetic contexts (as is often assumed) estimates based on unnormalized cues might over-estimate category variance. Both 1) and 2) would explain under-estimation of slopes since more variability -> smaller slope. But  -->
 
### Slopes: difference between conditions within each test block

```{r hypothesis-table-simple-slopes-condition, results='asis'}
hyp.simple_slopes_condition <- 
  hypothesis(
    fit_test.simple_effects_condition, 
    c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block1:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block3:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block5:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10:VOT_gs = 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block7:Condition.Exposure_Shift10vs.Shift0:VOT_gs = 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  fit_test.simple_effects_condition,
  hyp.simple_slopes_condition, 
  c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0"), 5)), 
    caption = "Did exposure condition affect the slope of categorization responses? This table summarizes the differences in slopes between the exposure conditions within each test block. Since there was little reason to expect differences across conditions, all hypothesis tests contained in this table test the null (using the Savage-Dickey density ratio). We note that such tests depend on the prior.") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)

rm(fit_test.simple_effects_condition)
```

With regard to how categorization slopes (across all conditions) would change relative to the *pre*-exposure test, our expectations were less clear until we constructed the idealized learner reference lines in Figures \@ref(fig:plot-fit-slope-PSE)C (after the experiment). On the one hand, we had designed the category variances of /d/ and /t/ to be somewhat plausible given the distribution VOT in American English. We did, for example, make sure that /t/ had larger VOT variance than /d/. So, to the extent that we created category variances that successfully resembled those that participants would expected based on their prior experience, exposure should not result in changes in participants' categorization slopes. On the other hand, methodological limitations (see Procedure in main text) kept us from most closely mimicking the distribution of VOTs in the database of American English that guided our design [@chodroff-wilson2018, as described in \@ref(sec:idealized-prior-listeners)]. 

### Slopes: difference between blocks within exposure condition

```{r hypothesis-table-simple-slopes-block, results='asis'}
hyp.simple_slopes_block <- 
  hypothesis(
    fit_test.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift0:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift0:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift0:Block_Test6vs.Test5:VOT_gs > 0",
      
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift10:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift10:Block_Test4vs.Test3:VOT_gs > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift10:Block_Test6vs.Test5:VOT_gs > 0",
      
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1:VOT_gs + mu2_Condition.ExposureShift40:Block_Test3vs.Test2:VOT_gs + mu2_Condition.ExposureShift40:Block_Test4vs.Test3:VOT_gs > 0",
    
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4:VOT_gs + mu2_Condition.ExposureShift40:Block_Test6vs.Test5:VOT_gs > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

  make_hyp_table(
    fit_test.simple_effects_block,
    hyp.simple_slopes_block,
    c(rep(c(
      # Comparing differences blocks within conditions
      "Block 1 to 2: increased slope",
      "Block 2 to 3: increased slope",
      "Block 3 to 4: increased slope",
      "{\\em Block 1 to 4: increased slope}",
      "Block 4 to 5: increased slope",
      "Block 5 to 6: increased slope",
      "{\\em Block 4 to 6: increased slope}"), 2),
      "Block 1 to 2: increased slope",
      "Block 2 to 3: increased slope",
      "Block 3 to 4: increased slope",
      "{\\em Block 1 to 4: increased slope}",
      "Block 4 to 5: increased slope",
      "Block 5 to 6: increased slope",
      "{\\em Block 4 to 6: increased slope}"),
    caption = "Did participants' categorization slopes change between test blocks? This table summarizes the interactions between exposure condition and block---specifically whether the differences in slopes between exposure condition changed between blocks.") %>%
  pack_rows("Difference between blocks: baseline", 1, 7) %>%
  pack_rows("Difference between blocks: +10", 8, 14) %>%
  pack_rows("Difference between blocks: +40", 15, 21) 
```

### Slopes: difference in the rate of change between exposure conditions and test block

```{r hypothesis-table-interaction-condition-block-VOT, results='asis'}
hyp_interaction.condition_block_VOT <-
  hypothesis(
    fit_test,
    c(
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) > 0",

    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 +
   mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0"),
   robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

  make_hyp_table(
    fit_test,
    hyp_interaction.condition_block_VOT,
    rep(c(
  # Comparing slope differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{slope}$",
  "Block 2 to 3: increased $\\Delta_{slope}$",
  "Block 3 to 4: increased $\\Delta_{slope}$",
  "{\\em Block 1 to 4: increased $\\Delta_{slope}$}",
  "Block 4 to 5: decreased $\\Delta_{slope}$",
  "Block 5 to 6: decreased $\\Delta_{slope}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{slope}$}"), 3),
    caption = "Did the slope differences between exposure conditions change from block to block? This table summarizes the interactions between exposure condition, block, and VOT---specifically, whether the differences in slopes between exposure conditions changed from test block to test block.") %>%
  pack_rows("Difference in slopes: +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in slopes: +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in slopes: +40 vs. baseline", 15, 21)
```

\pagebreak

## Slope results for exposure blocks {#sec:slopes-analyses-exposure}


### Slopes: difference between conditions within each exposure block

```{r hypothesis-table-exposure-simple-slopes-condition, results='asis'}
hyp.exposure_simple_slopes_condition <- 
  hypothesis(
    fit_exposure.simple_effects_condition, 
    c(
      "mu2_Block2:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block2:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block2:Condition.Exposure_Shift10vs.Shift0:VOT_gs + mu2_Block2:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block4:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block4:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block4:Condition.Exposure_Shift40vs.Shift10:VOT_gs + mu2_Block4:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block6:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0",
      "mu2_Block6:Condition.Exposure_Shift40vs.Shift10:VOT_gs > 0",
      "mu2_Block6:Condition.Exposure_Shift40vs.Shift10 + mu2_Block6:Condition.Exposure_Shift10vs.Shift0:VOT_gs > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  fit_exposure.simple_effects_condition,
  hyp.exposure_simple_slopes_condition, 
  rep(c("+10 vs. baseline < 0", "+40 vs. +10 < 0", "+40 vs. baseline < 0"), 3), 
    caption = "Did exposure condition affect the slope of categorization responses? This table summarizes the differences in slopes between the exposure conditions within each exposure block") %>% 
  pack_rows("Exposure block 1", 1, 3) %>%
  pack_rows("Exposure block 2", 4, 6) %>% 
  pack_rows("Exposure block 3", 7, 9) 

rm(fit_exposure.simple_effects_condition)
```

### Slopes: difference between blocks within exposure condition

```{r hypothesis-table-exposure-simple-slopes-block, results='asis'}
hyp.exposure_simple_slopes_block <- 
  hypothesis(
    fit_exposure.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Exposure2vs.Exposure1:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Exposure3vs.Exposure2:VOT_gs > 0",
      "mu2_Condition.ExposureShift0:Block_Exposure2vs.Exposure1:VOT_gs + mu2_Condition.ExposureShift0:Block_Exposure3vs.Exposure2:VOT_gs > 0",

      "mu2_Condition.ExposureShift10:Block_Exposure2vs.Exposure1:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Exposure3vs.Exposure2:VOT_gs > 0",
      "mu2_Condition.ExposureShift10:Block_Exposure2vs.Exposure1:VOT_gs + mu2_Condition.ExposureShift10:Block_Exposure3vs.Exposure2:VOT_gs > 0",

      "mu2_Condition.ExposureShift40:Block_Exposure2vs.Exposure1:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Exposure3vs.Exposure2:VOT_gs > 0",
      "mu2_Condition.ExposureShift40:Block_Exposure2vs.Exposure1:VOT_gs + mu2_Condition.ExposureShift40:Block_Exposure3vs.Exposure2:VOT_gs > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  fit_exposure.simple_effects_block,
  hyp.exposure_simple_slopes_block, 
  rep(c("Block 1 to 2: increased slope", "Block 2 to 3: increased slope", "Block 1 to 3: increased slope"), 3), 
    caption = "Did participants’ categorization slopes change between exposure blocks? This table summarizes the interactions between exposure condition and block --- specifically whether the differences in slopes between exposure condition changed between exposure blocks") %>% 
  pack_rows("Difference between blocks: baseline", 1, 3) %>%
  pack_rows("Difference between blocks: +10", 4, 6) %>% 
  pack_rows("Difference between blocks: +40", 7, 9) 

rm(fit_exposure.simple_effects_block)
```
\pagebreak

### Slopes: differences in the rate of change between exposure conditions and block

```{r hypothesis-table-exposure-interaction-condition-block-VOT, results='asis'}
hyp.interaction.exposure.condition_block_VOT <- 
  hypothesis(
    fit_exposure, 
    c(
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2  < 0",

      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Exposure3vs.Exposure2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure2vs.Exposure1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Exposure3vs.Exposure2  < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

  make_hyp_table(
    fit_exposure,
    hyp.interaction.exposure.condition_block_VOT,
    rep(c(
  # Comparing differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{slope}$",
  "Block 2 to 3: increased $\\Delta_{slope}$",
  "{\\em Block 1 to 3: increased $\\Delta_{slope}$}"), 3),
    caption = "Did the rate of block-to-block changes differ across exposure conditions? This table summarizes the interactions between between VOT, exposure condition, and block ---specifically, whether the differences between exposure conditions changed from exposure block to exposure block.") %>%
  pack_rows("Difference in +10 vs. baseline", 1, 3) %>%
  pack_rows("Difference in +40 vs. +10", 4, 6) %>% 
  pack_rows("Difference in +40 vs. baseline", 7, 9)

```

\pagebreak


## Lapse rates by exposure and test blocks {#sec:analysis-lapse}

```{r fit-lapse-by-block, message=FALSE}
fit_lapse_by_block <- 
  fit_model(
    data = d.for_analysis,
    phase = "all",
    formulation = "lapse_block",
    priorSD = 15, 
    adapt_delta = .999)
```


All analyses presented in the main text and in the preceding SI sections assume a constant lapse rate across exposure and test blocks. These analyses found lapse rates of `r logit_to_prob(fit_test, "theta1_Intercept")` (95%-CI: `r logit_to_prob(fit_test, "theta1_Intercept, 3")`- `r logit_to_prob(fit_test, "theta1_Intercept, 4")`) during test blocks and `r logit_to_prob(fit_exposure, "theta1_Intercept")` (95%-CI: `r logit_to_prob(fit_exposure, "theta1_Intercept", 3)`-`r logit_to_prob(fit_exposure, "theta1_Intercept", 4)`) during exposure blocks, much smaller than in previous work [@clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020]. The decision to assume constant lapse rates seems to be supported by Figures \@ref(fig:plot-fit-PSE)A-B, which show that participants' responses (the point ranges) in all exposure and test blocks approach 0 and 100% "t"-responses for small and large VOTs, respectively. Still, to explore changes in lapse rates we refitted the same psychometric mixed-effects model with a lapse model that included block as a predictor (sliding-difference coded from the first to the ninth block). Priors in this model were the same as that in the constant-lapse model including the prior for lapse rates which was the \texttt{brms} [@R-brms_a] default of a logistic prior centered around 0 with a scale of 1 unit. This prior assigns fairly equal weighting over values of $0 < p < 1$ and lower weighting of the extreme values. 

The estimated lapse rate for each block is shown in Table \@ref(tab:hypothesis-table-lapse-by-block). Lapse rates were at their highest in the first block ($1.5\%$) but fell sharply up to the fifth block ($\leq.0011\%$) before rising in the sixth ($0.07\%$) and seventh block ($0.04\%$). Where estimates were especially low, we note the accompanying wide confidence intervals partly resulting from our use of a uniform prior. Overall, these results show some evidence of familiarization with the task. However, unlike previous work, lapse rates were very low even during the first 12 trials (the first test block). For comparison, a post-hoc analysis reported in @kleinschmidt2020 estimated lapse rates as high as 12% during the first 37 trials of his experiment. Lapse rates reduced to about 5% over the remaining 185 trials. 

```{r hypothesis-table-lapse-by-block, results='asis'}
hyp.lapse <- 
  hypothesis(fit_lapse_by_block, map_chr(.x = 1:9, ~ c(get_lapse_hypothesis(fit_lapse_by_block, .x)))) %>% 
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  fit_lapse_by_block, 
  hyp.lapse, 
  paste(rep("Block", 9), 1:9, ": $\\lambda$ > 0"), 
  caption = "This table summarizes the estimated proportion of lapses for each block of the experiment.",
  digits = 3) 

rm(fit_lapse_by_block, fit_exposure)
```

One possible explanation for the consistently small lapse rate in the present experiment is that we recruited participants from the Prolific crowdsourcing platform rather than Mechanical Turk [@kleinschmidt-jaeger2016; @kleinschmidt2020]. Prolific has been found to deliver higher data quality than the Amazon's Mechanical Turk crowd-sourcing platform [@peer2017; @adams2020; @albert-smilek2023]. Another possibility is that the use of natural-, rather than robotic-sounding, stimuli engaged participants attention in the present experiment.

\pagebreak

## Comparing exposure effects against idealized learner models (all blocks) {#sec:shrinkage-test-all}


```{r hypothesis-table-shrinkage-all, results='asis'}
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs  - " , predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs  - " , predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x3/mu2_IpasteCondition.ExposureBlocksepEQxShift0x3:VOT_gs - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x3/mu2_IpasteCondition.ExposureBlocksepEQxShift10x3:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x3/mu2_IpasteCondition.ExposureBlocksepEQxShift40x3:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x5/mu2_IpasteCondition.ExposureBlocksepEQxShift0x5:VOT_gs - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x5/mu2_IpasteCondition.ExposureBlocksepEQxShift10x5:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x5/mu2_IpasteCondition.ExposureBlocksepEQxShift40x5:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x3/mu2_IpasteCondition.ExposureBlocksepEQxShift10x3:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x3/mu2_IpasteCondition.ExposureBlocksepEQxShift0x3:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x3/mu2_IpasteCondition.ExposureBlocksepEQxShift40x3:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x3/mu2_IpasteCondition.ExposureBlocksepEQxShift0x3:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x5/mu2_IpasteCondition.ExposureBlocksepEQxShift10x5:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x3/mu2_IpasteCondition.ExposureBlocksepEQxShift0x3:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x5/mu2_IpasteCondition.ExposureBlocksepEQxShift40x5:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x5/mu2_IpasteCondition.ExposureBlocksepEQxShift0x5:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")")), 
    robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    # test block 1
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    # test block 2
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    # test block 3
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    
    # test block 4
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table complements Table \\ref{tab:hypothesis-table-shrinkage-test4}.",
  col1_width = "27em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 9) %>% 
  pack_rows("Test block 1", 1, 3) %>% 
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 13, 20) %>% 
  pack_rows("Test block 1", 13, 14) %>% 
  pack_rows("Test block 2", 15, 16) %>% 
  pack_rows("Test block 3", 17, 18) %>% 
  pack_rows("Test block 4", 19, 20)
```

\pagebreak

## Evaluating accuracy of psychometric model, idealised learners against human accuracy

(ref:plot-IO-psychometric-model-human-accuracy) Same as Figure \@ref(fig:IO-human-accuracy) but also showing listeners' empirical recognition accuracy, as estimated from the unlabeled trials of exposure blocks (triangles). For the empirical accuracy, lineranges show 95% bootstrapped CIs.

```{r plot-IO-psychometric-model-human-accuracy, fig.width=7.5, fig.height=base.height*2 + .75, fig.cap="(ref:plot-IO-psychometric-model-human-accuracy)"}
p.human_psych_accuracy
```

## Relaxing the linearity assumption for VOT {#sec:GAMM}
The perceptual model of our psychometric mixed-effects analyses assumed linear effects of VOT on the log-odds of "t"-responses. As already mentioned in SI \@ref(sec:io-bias-correction), we made this assumptions for the sake of simplicity, and in order to avoid over-fitting. However, there are reasons to expect that the effects of VOT on listeners' categorizations are non-linear. Specifically, ideal observers with Gaussian categories along a single cue dimension (here: VOT) predict that the posterior log-odds of each category change linearly along that cue dimension if and only if the variance of both Gaussian categories is equal [for details, see @bicknell2024; @kleinschmidt-jaeger2015; @kronrod2016]. For US English /d/ and /t/, this is well-known *not* to be the case, with /t/ having larger variance than /d/ (see also Figure \@ref(fig:exposure-means-database-matrix-plot) in the main text). Under the assumption of Gaussian categories, ideal observer thus predict a quadratic effect along VOT---specifically, the log-odds of "t"-responses are predicted to increase more than linearly with increasing VOT between the two category means (since the /t/ category has larger variance).

<!-- While Gaussian categories are a simplifying assumption for the sake of modeling, rather than a critical assumption of the ideal adaptor framework [cf. @kleinschmidt-jaeger2015, Appendix]^[For example, the assumption of Gaussian categories leads to implausible predictions for extreme VOT values that lie outside the range of the two category means. Specifically, for VOTs smaller than the category mean of /d/, "t"-responses are predicted to eventually increase with decreasing VOT values (as a consequence of the same quadratic trend mentioned above). Additionally, distribution of VOTs for the /d/ category tends to be noticeably positively-skewed and thus non-normal.], the predicted quadratic trend is also likely to emerge in exemplar models. -->

While the linearity assumption made in our main analysis does not *necessarily* introduce statistical bias, it is possible that it does. We thus conducted additional analyses that relaxed the linearity assumption by modeling the effect of VOT as a non-parametric smooth. Since this analysis affords additional degrees of freedom, we reduced concerns about over-fitting by combining the (unlabeled) exposure and test data into a single analysis. Instead of modeling effects block by block, we decided to model the incremental and cumulative effects of exposure by including a non-linear effect of trial and its interaction with VOT in the analysis. In addition to replicating our analysis under relaxed assumptions about linearity, this auxiliary analysis also sheds light on the causes for the 'zigzag' pattern in the intercept and slope estimates for exposure and test blocks that we reported in the main analysis.

### Substituting GAMMs for GLMMs in our psychometric mixed-effects model
Specifically, we use the same psychometric mixed-effects model as in the main analysis, except that we replaced the full factorial of VOT, block, and exposure condition in the perceptual model with a separate tensor smooth of VOT and trial for each of the three exposure conditions.^[`t2(VOT, Trial, by = Condition.Exposure, bs = "tp")`]. This makes this auxiliary analysis a mixed-effect mixture model, for which the mixture component that is the perceptual model is a generalized additive mixed-effect models (GAMM), rather than a generalized linear mixed-effects model. The analysis contained the same full random effect structure and priors as the psychometric model presented in the main text. Both VOT and trial were Gelman-scaled prior to the analysis.

```{r message=FALSE}
VOT.mean_test <-
    d.for_analysis %>%
    filter(Phase == "test") %>%
    ungroup() %>%
    summarise(mean = mean(Item.VOT, na.rm = T)) %>%
    pull(mean)

d.GAMM <-
  d.for_analysis %>%
  # Get Gelman-scaled trial ID across blocks
  # (do so before filtering, so that trial roughly corresponds to amount of evidence)
  arrange(ParticipantID, Block, Trial) %>%
  group_by(ParticipantID) %>%
  # This does not include catch trials anymore (that's ok since
  # those trials do not contain information about the distribution
  # of VOTs for the learner)
  mutate(Trial = 1:length(Trial)) %>%
  filter(Item.Labeled == F) %>%
  mutate(Trial_gs = (Trial - mean(Trial)) / (2 * sd(Trial))) %>%
  prepVars(test_mean = VOT.mean_test, levels.Condition = levels_Condition.Exposure)

my_priors <-
  c(
    prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
    prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
    prior(lkj(1), class = "cor"))

# Recode condition as ordered factor if one wants difference smooths(comparing
# levels against each other), rather than separate smooths for each level of
# Condition.Exposure
m <- brm(
    formula =
      bf(
        Response.Voiceless ~ 1,
         mu1 ~ 0 + offset(0),
        # Think further about the specific basis functions for this 2D tensor smooth
        # e.g., bs = c("cc", "tp"), k=c(10, 10)
         mu2 ~ 1 + t2(VOT_gs, Trial_gs, by = Condition.Exposure, bs = "tp") +
          # Random effects (could use bs = "re" or "fs"; "fs" only works when the
          # non RE predictors are continuous)
          # Could determine order of smooth via, e.g., m = 1
          s(ParticipantID, bs = "re") +
          s(VOT_gs, ParticipantID, bs = "re") +
          s(Trial_gs, ParticipantID, bs = "re") +
          s(VOT_gs, Trial_gs, ParticipantID, bs = "re") +
          s(Item.MinimalPair, bs = "re") +
          s(VOT_gs, Item.MinimalPair, bs = "re") +
          s(Trial_gs, Item.MinimalPair, bs = "re") +
          s(Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Trial_gs, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(Trial_gs, Condition.Exposure, Item.MinimalPair, bs = "re") +
          s(VOT_gs, Trial_gs, Condition.Exposure, Item.MinimalPair, bs = "re"),
         theta1 ~ 1),
    data = d.GAMM,
    prior = my_priors,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 2500,
    warmup = 1500,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    control = list(adapt_delta = .99),
    backend = "cmdstanr",
    threads = threading(threads = 2),
    file = paste0("../models/GAMM-test+exposure-wRE.rds"))
```

### Results
Figure \@ref(fig:GAMM-figure) shows the predicted log-odds of "t"-responses that result from the fitted GAMM. The GAMM results replicate the directional effects of exposure both within and across exposure conditions. For the baseline condition, the contour lines largely shift downwards with exposure. This means that the same VOT is more likely to be categorized as "t" with increasing exposure. For the +40 condition, the opposite trend is observed (and much more clearly), indicating that the same VOT is *less* likely to be categorized as "t" with increasing exposure. The +10 condition falls between the baseline and +40 condition.

The GAMM results also confirm that the changes introduced by exposure are undone with repeated testing. This shows in the contour lines over trials 160-175, which trend towards reverting the changes introduced by the preceding exposure. The trend shows most clearly for the baseline and +40 condition (in opposite directions since the exposure effects are in opposite directions for these two conditions). 

(ref:GAMM-figure) Predicted log-odds of "t"-response by trial (across all exposure and test blocks) and VOT for each of the three exposure conditions. Combinations of trial and VOT that did not occur in any of the three exposure conditions are left white (the reduced VOT range of test blocks makes them easily identifiable).

```{r GAMM-figure, fig.height=base.height*3+1/2, fig.width=base.width*4, warning=FALSE, fig.cap="(ref:GAMM-figure)"}
# Plot smooth over rescaled data
ms <- conditional_smooths(
  m,
  smooths = 't2(VOT_gs, Trial_gs, by = Condition.Exposure, bs = "tp")',
  too_far = .025)
ms[[1]] %<>%
  mutate(
    VOT_gs = VOT_gs * 2 * sd(d.GAMM$Item.VOT) + VOT.mean_test,
    Trial_gs = Trial_gs * 2 * sd(d.GAMM$Trial) + mean(d.GAMM$Trial))
p <- brms:::plot.brms_conditional_effects(ms, stype = "raster", plot = F)
p[[1]] +
  aes(x = Trial_gs, y = VOT_gs) +
  geom_contour(aes(z = estimate__), color = "black") +
  xlab("Trial") + ylab("VOT") +
  scale_fill_viridis_c('log-odds of "t"-response') +
  coord_cartesian(expand = F) + 
  facet_wrap(~ Condition.Exposure, 
             labeller = labeller(Condition.Exposure = c("Shift0" = "baseline", "Shift10" = "+10", "Shift40" = "+40"))) + 
  theme(legend.position = "top") 
```


# Visual analysis of reaction times

```{r fig,width=8, fig.height=5, fig.width=6}
p.RT_scatter <- 
  d %>% 
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  filter(Is.CatchTrial == FALSE) %>%
  mutate(Response.log_RT = log10(Response.RT)) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>%
  ggplot(aes(x = mean, y = sd, label = ParticipantID)) +
  geom_text(aes(colour = Condition.Exposure), alpha = .5, size = 3) +
  geom_rug(aes(colour = Condition.Exposure), alpha = .5) +
  scale_x_continuous("mean log10-RT (ms)") +
  scale_y_continuous("SD log10-RT (ms)") +
  scale_color_manual(
    "Exposure condition",
    aesthetics = c("colour", "fill"),
    labels = c("baseline", "+10", "+40"),
    values = colours.condition) +
  guides(colour = "none", text = "none") +
  facet_wrap(
    ~ Block, 
    labeller = labeller(.cols = c("1" = "Test 1", "2" = "Exposure 1", "3" = "Test 2", "4" = "Exposure 2", "5" = "Test 3", "6" = "Exposure 3", "7" = "Test 4", "8" = "Test 5", "9" = "Test 6")))
```

(ref:plot-RT-all-data) Summary of participant reaction times by block. **Top:** Scatter plot of means and SDs of RTs in $log_{10}$ms by participant and block. **Bottom:** Progression of RTs by trial (including catch trials) and block. Lines indicate mean RTs, ribbons indicate region of two times SD from the mean.

```{r, plot-RT-all-data, fig.height=7.5, fig.width=7, fig.cap="(ref:plot-RT-all-data)"}
p.RT_line <- 
  d %>%
  group_by(Trial, Condition.Exposure, Block) %>%
  mutate(Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT))) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>%
  ggplot(aes(x = Trial, y = mean)) +
  geom_ribbon(aes(ymin = mean - 2*sd, ymax = mean + 2*sd), colour = "lightgrey", alpha = .1) +
  geom_line(aes(group = Condition.Exposure, color = Condition.Exposure), linewidth = .9) +
  scale_x_continuous("Trial", breaks = c(12, 24, 36, 48)) +
  scale_y_continuous("mean log10-RT (ms)") +
  coord_trans(y = "log10") +
  scale_color_manual(
    "Exposure condition",
    aesthetics = c("colour", "fill"),
    labels = c("baseline", "+10", "+40"),
    values = colours.condition) +
  facet_grid(
    Condition.Exposure ~ Block, 
    scales = "free_x", space = "free_x",
    labeller = labeller(.rows = c("Shift0" = "baseline", "Shift10" = "+10", "Shift40" = "+40"),
                        .cols = c("1" = "Test\n 1", "2" = "Exposure\n 1", "3" = "Test\n 2", "4" = "Exposure\n 2", "5" = "Test\n 3", "6" = "Exposure\n 3", "7" = "Test\n 4", "8" = "Test\n 5", "9" = "Test\n 6"))) +
  theme(legend.position = "top")

p.RT_scatter/p.RT_line +
  plot_layout(guides = "collect", heights = c(1.9, 1)) &
  theme(legend.position = "top")
```


# Comparing predictions of ideal adaptor against participants' responses {#sec:ideal-adaptor}

```{r functions-for-adaptive-changes}
summarize.NIW_ideal_adaptor_stanfit <- function(
    x,
    pars = c("kappa", "nu", "m", "S", "lapse_rate"),
    groups = c(
      "prior",
      "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
      "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
      "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7")
) {
  add_ibbu_stanfit_draws(x, groups = groups, summarize = T) %>%
    unnest_cue_information_in_model() %>%
    gather_variables(exclude = c(".chain", ".iteration", ".draw", ".row", "cue", "cue2", "group", "category")) %>%
    ungroup() %>%
    # + Remove double mentions of variables that are constant across groups, categories, and cues
    #   (currently: lapse_rate)
    # + Remove double mentions of variables that are constant across cues
    #   (currently: kappa and nu)
    # + Remove double mentions of variables that are constant across cue2
    #   (currently: m)
    mutate(
      across(
        c(group, category),
        ~ ifelse(.variable == "lapse_rate", NA_character_, as.character(.x))),
      across(
        c(cue, cue2),
        ~ ifelse(.variable %in% c("kappa", "nu", "lapse_rate"), NA_character_, as.character(.x))),
      across(
        c(cue2),
        ~ ifelse(.variable == "m", NA, .x))) %>%
    distinct() %>%
    group_by(group, category, cue, cue2, .variable) %>%
    median_hdci() %>%
    mutate(
      group = factor(group, levels = groups),
      .variable = factor(.variable, levels = pars)) %>%
    # Format and sort output
    relocate(group, category, .variable, cue, cue2, .value, everything()) %>%
    arrange(match(.$.variable, levels(.$.variable)), match(.$group, levels(.$group)), category, cue, cue2) %>%
    rename(
      parameter = .variable,
      !! sym(unique(.$.point)) := .value,
      !! sym(unique(paste0("lower ", unique(.$.width * 100), "% ", toupper(unique(.$.interval))))) := .lower,
      !! sym(unique(paste0("upper ", unique(.$.width * 100), "% ", toupper(unique(.$.interval))))) := .upper) %>%
    select(-c(.width, .point, .interval)) %>% 
    kable()
}

# TO DO: consider making plots a bit pretty, e.g., by grouping participant responses together
# into 5msec VOT bins and by making model prediction a line plot, rather than a point plot.
plot_predicted_vs_actual_categorization_responses <- function(data, colors.group = NULL) {
  p <-
    data %>%
    mutate(ExposureGroup = gsub("_Up\\sto", "", ExposureGroup)) %>%
    ggplot(aes(x = VOT)) +
    stat_summary(fun = mean, geom = "line", aes(y = Response)) +
    stat_summary(fun = mean, geom = "line", aes(y = Predicted_posterior), color = "gray") +
    stat_summary(fun.data = mean_cl_boot, geom = "pointrange", aes(y = Response), size = 1/4) +
    facet_wrap(~ ExposureGroup, ncol = 3) 
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group)
  plot(p)

  p %+%
    (data %>%
       mutate(
         ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
         ExposureGroup = gsub("[ABC]A", "", ExposureGroup))) %>%
    plot()

  p <- 
    data %>%
    group_by(ExposureGroup, VOT) %>%
    summarise(across(c(Response, Predicted_posterior), mean)) %>%
    ungroup() %>%
    mutate(
      Test = gsub("^.*test(.*)$", "\\1", ExposureGroup),
      Test = ifelse(Test == "no exposure", 0, Test),
      ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
      ExposureGroup = gsub("[ABC]A", "", ExposureGroup),
      ExposureGroup = gsub("^.*Shift([0-9]+).*$", "+\\1", ExposureGroup)) %>%
    ggplot(aes(x = Predicted_posterior, y = Response)) +
    geom_abline(intercept = 0, slope = 1, color = "lightgray") +
    geom_text(aes(color = ExposureGroup, label = Test)) +
    geom_smooth(aes(color = ExposureGroup)) +
    annotate(
      geom = "text",
      label = paste0(
        "R^2 = ",
        round(data %>%
                with(., cor(Predicted_posterior, Response)) %>%
                . ^ 2, 3) * 100, "%"),
      x = .1, y = .9) +
    xlab('Predicted proportion "t"-responses') +
    ylab('Observed proportion "t"-responses') 
  
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group[c(3, 6, 9, 10)])
  plot(p)
}

plot_predicted_vs_actual_categorization_responses_for_IBBU <- function(
    model,
    groups = NULL,
    untransform_cues = F,
    # target category "/d/" = 1, "/t/" = 2
    target_category = 2,
    colors.group = NULL
) {
  # Get and summarize posterior draws from fitted model
  d.pars <-
    add_ibbu_stanfit_draws(
      model,
      groups = groups,
      summarize = F,
      wide = F,
      ndraws = NULL,
      untransform_cues = untransform_cues) %>%
    filter(group %in% .env$groups)
  
  # Prepare test_data
  # --------- TO DO: THIS MIGHT NEED WORK --------
  cue.labels <- get_cue_levels_from_stanfit(model)
  data.test <-
    get_test_data_from_stanfit(model) %>%
    distinct(!!! syms(cue.labels)) %>%
    { if (untransform_cues) get_untransform_function_from_stanfit(model)(.) else . } %>%
    make_vector_column(cols = cue.labels, vector_col = "x", .keep = "all") %>%
    nest(cues_joint = x, cues_separate = .env$cue.labels)  %>%
    crossing(group = levels(d.pars$group))

  # Categorize test data
  d.pars %<>%
    group_by(group, .draw) %>%
    do(f = get_categorization_function_from_grouped_ibbu_stanfit_draws(., logit = F)) %>%
    right_join(data.test, by = "group") %>%
    group_by(group, .draw) %>%
    mutate(
      Predicted_posterior =
        pmap(
          .l = list(f, cues_joint, target_category),
          .f = ~ exec(..1, x = ..2$x, target_category = target_category))) %>%
    select(-f) %>%
    unnest(c(cues_joint, cues_separate, Predicted_posterior)) %>%
    # Repair estimates that yield infinite posteriors
    mutate(
      Predicted_posterior = case_when(
        is.infinite(Predicted_posterior) & sign(Predicted_posterior) == 1 ~ 1, 
        is.infinite(Predicted_posterior) & sign(Predicted_posterior) == -1 ~ 0,
        T ~ Predicted_posterior))
  
  # Compare predicted and actual responses
  d.pars %>%
    group_by(group, VOT) %>%
    summarise(across(Predicted_posterior, mean)) %>%
    mutate(VOT = VOT, ExposureGroup = ifelse(group == "prior", "no exposure", group)) %>%
    left_join(get_test_data_from_stanfit(model)) %>%
    mutate(
      Response = `/t/` / (`/d/` + `/t/`),
      # Simplify the plot by collapsing over all Latin-square designed lists
      # (this still keeps all individual predictions but only has one facet per unique combination
      # of exposure condition and test, rather than also making separate facets for each of the three 
      # different block orders for each of these unique combinations)
      group = gsub("[ABC]A", "", group)) %>%
    plot_predicted_vs_actual_categorization_responses(colors.group = colors.group)
  
  # Could feed d.pars into parts of get_logistic_parameters_from_model 
  # (only the part that samples responses and fits the logistic to it).
  # That would return a posterior distribution of PSEs (one PSE for each posterior draw of parameters)
  # that could be compared against listeners' PSEs.
  resolution <- 10000
  d.pars %>%
    # Prepare data frame for logistic regression
    mutate(
      n_d = round((if (target_category == 1) Predicted_posterior else (1 - Predicted_posterior)) * .env$resolution), 
      n_t = .env$resolution - n_d) %>%
    group_by(group, .draw) %>%
    nest() %>%
    # Fit logistic regression and extract relevant information
    # (the regression only uses VOT regardless of what cues are used for the categorization
    # so that this matches the analysis of the human responses)
    mutate(
      model_unscaled = map(data, ~ glm(
        # unscaling the VOT here because the VOT from fit is already scaled
        cbind(n_t, n_d) ~ 1 + I(((VOT* 2 * VOT.sd_test) + VOT.mean_test)),
        family = binomial,
        data = .x)),
      intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
      slope_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
      model_scaled = map(data, ~ glm(
        # using VOT values as is because the model was fit using scaled VOT
        cbind(n_t, n_d) ~ 1 + VOT,
        family = binomial,
        data = .x)),
      intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
      slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
      PSE = -intercept_unscaled/slope_unscaled)
}

make_all_plots <- function(
  fit,
  groups = c(
    "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
    "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
    "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7",
    "prior"),
  colors.group =  c(
    "#550000", "#AA0000", "#FF0000",
    "#005500", "#00AA00", "#00FF00",
    "#000055", "#0000AA", "#0000FF",
    "gray"),
  colors.category = c("blue", "red"),
  ncol = 3
) {
  plot_ibbu_stanfit_parameter_correlations(fit, category.colors = colors.category) %>% plot()
  if (!is.null(groups)) {
    plot_ibbu_stanfit_parameters(fit, groups = groups, group.colors = colors.group) %>% plot()
    (plot_expected_ibbu_stanfit_categories_contour2D(fit, groups = groups, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol)) %>% plot()
    (plot_ibbu_stanfit_test_categorization(fit, groups = groups, plot_in_cue_space = T, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol)) %>% plot()
    plot_predicted_vs_actual_categorization_responses_for_IBBU(
      fit, 
      groups = c(
        "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
        "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
        "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7",
        "Cond Shift0BA_Up to test3", "Cond Shift0BA_Up to test5", "Cond Shift0BA_Up to test7",
        "Cond Shift10BA_Up to test3", "Cond Shift10BA_Up to test5", "Cond Shift10BA_Up to test7",
        "Cond Shift40BA_Up to test3", "Cond Shift40BA_Up to test5", "Cond Shift40BA_Up to test7",
        "Cond Shift0CA_Up to test3", "Cond Shift0CA_Up to test5", "Cond Shift0CA_Up to test7",
        "Cond Shift10CA_Up to test3", "Cond Shift10CA_Up to test5", "Cond Shift10CA_Up to test7",
        "Cond Shift40CA_Up to test3", "Cond Shift40CA_Up to test5", "Cond Shift40CA_Up to test7",
        "no exposure"), 
      untransform_cues = F, target_category = 2, colors.group = colors.group)
  } else {
    plot_ibbu_stanfit_parameters(fit) %>% plot()
    plot_expected_ibbu_stanfit_categories_contour2D(fit, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol) %>% plot()
    plot_ibbu_stanfit_test_categorization(fit, plot_in_cue_space = T, category.colors = colors.category) + facet_wrap(~ group, ncol = ncol) %>% plot()
  }
}
```

## Fit with uninformative (regularizing) priors
Here some initial plots. While the plots are back-transformed into the original cue space, the table is not (yet).

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis', eval=FALSE}
m_IA_inferred.VOT <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.VOT) 
make_all_plots(m_IA_inferred.VOT)
```

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis', eval=FALSE}
m_IA_inferred.VOT_f0 <-
  infer_prior_beliefs(
    exposure = d_for_ASP%>%
    filter(group %in% .env$groups) %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT+f0_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.VOT_f0) 
make_all_plots(m_IA_inferred.VOT_f0)
```

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis', eval=FALSE}
m_IA_inferred.VOT_f0_vowelduration <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.VOT_f0_vowelduration) 
make_all_plots(m_IA_inferred.VOT_f0_vowelduration)
```

## Fit with informative priors about expected category means and covariances based on phonetic data from @chodroff-wilson2018

```{r, size='tiny', fig.width=12, fig.height=12, warning=FALSE, results='asis', eval=FALSE}
m_IA_inferred.fixed_pars <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    sample = T,
    lapse_rate = NULL,
    mu_0 = m_IO.VOT_f0$mu,
    Sigma_0 = m_IO.VOT_f0$Sigma,
    file = "../models/IBBU_VOT+f0_informative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

summarize.NIW_ideal_adaptor_stanfit(m_IA_inferred.fixed_pars) 
make_all_plots(m_IA_inferred.fixed_pars)
```


# Comparing shrinkage hypothesis with different priors


```{r hypothesis-table-shrinkage-test4-actual-mean, results='asis', echo=FALSE, message=FALSE}
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")")),
    robust = F) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using the mean as point estimate.",
  col1_width = "27em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```

```{r hypothesis-table-shrinkage-test4-actual-median, results='asis', echo=FALSE, message=FALSE}
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ")/(", predictedPSE_scaled_10, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ")/(", predictedPSE_scaled_40, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")/(", predictedPSE_scaled_0, "-", "(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs)", ")")),
    robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using the median as point estimate.",
  col1_width = "27em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```




```{r hypothesis-table-shrinkage-test4-prior-mean, results='asis', echo=FALSE, message=FALSE}
predictedPSE_scaled_prior.mean <- get_IO_predicted_PSE(condition = "prior", mean = T)
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", predictedPSE_scaled_prior.mean, ")/(", predictedPSE_scaled_10, "-", predictedPSE_scaled_prior.mean, ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", predictedPSE_scaled_prior.mean, ")/(", predictedPSE_scaled_0, "-", predictedPSE_scaled_prior.mean, ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", predictedPSE_scaled_prior.mean, ")/(", predictedPSE_scaled_40, "-", predictedPSE_scaled_prior.mean, ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", predictedPSE_scaled_prior.mean, ")/(", predictedPSE_scaled_0, "-", predictedPSE_scaled_prior.mean, ")")),
    robust = F) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{predicted_{prior}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{predicted_{prior}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{predicted_{prior}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{predicted_{prior}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using mean as point estimate and the mean of cross-validated priors obtained from idealized learners.",
  col1_width = "29em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```


```{r hypothesis-table-shrinkage-test4-prior-median, results='asis', echo=FALSE, message=FALSE}

hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", predictedPSE_scaled_prior.median, ")/(", predictedPSE_scaled_10, "-", predictedPSE_scaled_prior.median, ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", predictedPSE_scaled_prior.median, ")/(", predictedPSE_scaled_0, "-", predictedPSE_scaled_prior.median, ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", predictedPSE_scaled_prior.median, ")/(", predictedPSE_scaled_40, "-", predictedPSE_scaled_prior.median, ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", predictedPSE_scaled_prior.median, ")/(", predictedPSE_scaled_0, "-", predictedPSE_scaled_prior.median, ")")),
    robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{predicted_{prior}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{predicted_{prior}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{predicted_{prior}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{predicted_{prior}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{predicted_{prior}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using median as point estimate and the median of cross-validated prior obtained from idealized learners.",
  col1_width = "29em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```


```{r hypothesis-table-shrinkage-test4-actual-meanblock1-mean, results='asis', echo=FALSE, message=FALSE}
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_10, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_0, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_40, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_0, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")")),
    robust = F) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using mean as point estimate and average mean Block 1 PSEs as the prior.",
  col1_width = "27em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```



```{r hypothesis-table-shrinkage-test4-actual-meanblock1-median, results='asis', echo=FALSE, message=FALSE}
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" ),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_10, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_0, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")"),
      
      str_c("((-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_40, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ") >", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs)  -", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")/(", predictedPSE_scaled_0, "-", "((-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) + (-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs))/3", ")")),
    robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{baseline}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{+10}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{+40}})| > 0$",
    "$\\frac{\\Delta(PSE_{actual_{+10}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+10}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$",
    "$\\frac{\\Delta(PSE_{actual_{+40}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{+40}}, PSE_{actual_{pre}})} > \\frac{\\Delta(PSE_{actual_{baseline}}, PSE_{actual_{pre}})}{\\Delta(PSE_{ideal_{baseline}}, PSE_{actual_{pre}})}$"),
  caption = "Comparison of actual changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). Using median as point estimate and average median Block 1 PSEs as the prior.",
  col1_width = "27em") %>%
  pack_rows("Have participants converged against the PSE expected from idealized learner?", 1, 3) %>% 
  pack_rows("Does convergence against ideal PSE differ across conditions (asymmetric shrinkage)?", 4, 5)
```


# Session Info

```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```
