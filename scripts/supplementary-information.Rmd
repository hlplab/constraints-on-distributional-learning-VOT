<!-- Do NOT knit this document. It is part of a larger document. Instead knit the main document (my-apa-formatted-article) 
     If you want to separate the SI from the rest of the paper, we recommend you do so AFTER knitting them into a single PDF. 
     This will make sure that all references to sections, figures, tables, etc. are working as intended. You can easily separate 
     the PDF into two parts, using e.g., Acrobat PDF viewer. //-->
     
# Supplementary information {-}
\setcounter{section}{0}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via [OSF](). It is best viewed using Acrobat Reader. Some links and animations might not work in other PDF viewers. 

# Required software {#sec:software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r} 
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}). 

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. The full session information is provided at the end of this document.

# Predictions of existing models of adaptive speech perception {sec:proof}
We show that two of the most influential approaches to adaptive speech perception do not predict the sublinear (less than proportional) shifts in categorisation functions observed both in the present study and in in previous work [@kleinschmidt-jaeger2016; @kleinschmidt2020]. We focus on these two models since they are the only widely-used accounts of adaptive speech perception that are (1) not limited to specific types of contrasts (unlike, e.g., vowel normalization accounts) and (2) sufficiently specific to make concrete predictions [for review of existing models, see @xie2023]. We only briefly discuss why a third type of account---exemplar models of speech perception---*likely* would also fail to predict the sublinear effects.

We emphasize that our discussion deliberately focuses on *models*, not theories. The finding of sublinear effects of exposure can be accommodated in all three theories that underlie the three models discussed here (Bayesian inference in the ideal adaptor framework, similarity-based inference in exemplar theory, or normalization relative to expectations). Yet, the fact that none of the existing *models* predicts this effect highlights that this property of adaptive speech perception was only recently discovered, and is as-of-yet not well understood.

## Incremental Bayesian belief-updating [@kleinschmidt-jaeger2011, @kleinschmidt-jaeger2012, @kleinschmidt-jaeger 2015, @kleinschmidt-jaeger2016; @kleinschmidt2020]
The only distributional learning model that has been more repeatedly tested against adaptive speech perception---incremental Bayesian belief-updating [@kleinschmidt-jaeger2011]---predicts proportional, rather than sublinear, shifts. This model had previously been found to closely predict the cumulative effects of exposure in perceptual recalibration to audio-visually [@kleinschmidt2011-jaeger; @kleinschmidt-jaeger2012] or lexically labeled speech [@cummings-theodore2023], as well as the type of exposure to unlabelled minimal pair words employed by Kleinschmidt and Jaeger [@theodore-monto2019]. However, all of these studies employed comparatively small changes in cue distributions, and lacked the design necessary to detect deviation from proportionality (we return to this point below). The findings presented in @kleinschmidt-jaeger2016 would seem to reject this specific distributional learning model [though not necessarily the theory it is derived from, @kleinschmidt-jaeger2015; for discussion of the relation between theory and model, see also @kleinschmidt2020]  <!-- TO DO: make sure we do --> 

```{r}
get_IBBU_updates <- function(
    prior = NULL,
    exposure = NULL,
    test = NULL,
    condition = NULL, 
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72,
    updates_to_keep = c(0, 48, 96, 144), 
    seed = 1234
) {
  set.seed(seed)
  Sigma_noise <- diag(c(80, 878)[1:length(cues)])
  dimnames(Sigma_noise) <- list(cues, cues)
  
  if (is.null(prior)) {
    prior <- d.chodroff_wilson %>%
      make_NIW_ideal_adaptor_from_data(
        cues = cues, kappa = kappa, nu = nu, Sigma_noise = Sigma_noise)
  }
  
  if (is.null(exposure)) {
    message("When exposure is NULL, condition is interpreted as the VOT shift.")
    exposure <- 
      lapply(
        X = as.list(1:3), 
        FUN = function(x) d.chodroff_wilson %>%
          # Sample exposure from VOT distribution regardless of which cues are used 
          # for belief-updating, since that's what we did in the experiment.
          make_MVG_ideal_observer_from_data(
            cues = "VOT", Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>% 
          sample_MVG_data_from_model(Ns = 24, randomize.order = T)) %>%
      reduce(rbind) %>% 
      mutate(
        Trial = 1:144,
        Condition.Exposure = paste0("TrueShift", .env$condition),
        VOT = VOT + as.numeric(.env$condition),
        f0 = predict_f0(VOT, Mel = F),
        f0_Mel = predict_f0(VOT, Mel = T)) %>%
      relocate(Condition.Exposure, Trial, category, VOT, f0)
  }

  idealized_ios <- 
    make_VOT_IOs_from_exposure(exposure) %>%
    get_logistic_parameters_from_model(
    model = ., x = map(test, ~ .x[1]), 
    model_col = "io", 
    groups = "Condition.Exposure")
    
  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.order = "Trial",
    noise_treatment = "no_noise",
    lapse_treatment = "no_lapses",
    method = "label-certain") %>%
    filter(observation.n %in% updates_to_keep) %>%
    nest(ia = -observation.n) %>%
    mutate(
      Condition.Exposure = unique(exposure$Condition.Exposure),
      observation.bin = as.numeric(factor(observation.n, levels = sort(observation.n))),
      observation.alpha = observation.bin / max(observation.bin),
      cf = map(ia, 
               ~ get_categorization_function_from_NIW_ideal_adaptor(.x, noise_treatment = "no_noise")),
      sf = map2(cf, observation.alpha, 
                ~ stat_function(
                  fun = function(x) {
                    as.numeric(.x(
                      map(x, function(y) c(y, predict_f0(y, Mel = T))), 
                      target_category = 2)) }, 
                  alpha = .y, 
                  color = if (condition %in% c("Shift0", "Shift10", "Shift40")) colours.condition[condition] else "black"))) %>%
    select(-c(observation.bin, observation.alpha)) %>%
    relocate(Condition.Exposure, observation.n, ia, cf, sf) %>%
    # Convert updated beliefs into estimated PSE and slope
    get_logistic_parameters_from_model(
      model = ., x = test, model_col = "ia", 
      groups = c("Condition.Exposure", "observation.n", "cf", "sf")) %>%
    # Join in information about idealized prior and posterior 
    # (the latter based on ideal observer that has fully learned the exposure distribution).
    # Then calculate for each update step the proportion of updating relative to those 
    # idealized priors and posteriors.
    left_join(
      idealized_ios %>%
        select(Condition.Exposure, slope_scaled, PSE) %>%
        rename(idealized_posterior_slope_scaled = slope_scaled, idealized_posterior_PSE = PSE)) %>%
    cross_join(
      idealized_ios %>%
        ungroup() %>%
        filter(Condition.Exposure == "prior") %>%
        select(slope_scaled, PSE) %>%
        rename(idealized_prior_slope_scaled = slope_scaled, idealized_prior_PSE = PSE)) %>%
    mutate(
      proportion_of_idealized_PSE = (PSE - idealized_prior_PSE) / (idealized_posterior_PSE - idealized_prior_PSE),
      proportion_of_idealized_slope_scaled = (slope_scaled - idealized_prior_slope_scaled) / (idealized_posterior_slope_scaled - idealized_prior_slope_scaled))
}

get_IBBU_updates_for_experiment <- function(
    condition, 
    test = NULL,
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72
) {
  exposure <- 
    d_for_analysis %>% 
    ungroup() %>%
    filter(
      Condition.Exposure == condition,
      Phase == "exposure") %>%
    filter(ParticipantID == first(ParticipantID)) %>%
    mutate(
      category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")) %>%
    rename(
      VOT = Item.VOT,
      f0_Mel = Item.F0_Mel)
  
  get_IBBU_updates(
    exposure = exposure, 
    test = test,
    condition = condition, 
    cues = cues, kappa = kappa, nu = nu)
}

x <-
  d_for_analysis %>%
  ungroup() %>%
  filter(Phase == "test") %>%
  distinct(Item.VOT, Item.F0_Mel) %>%
  arrange(Item.VOT) %>%
  mutate(x = map2(Item.VOT, Item.F0_Mel, ~ c(.x, .y))) %>%
  pull(x)

# Plot updates for our three exposure condition
d.updates <-  
  bind_rows(get_IBBU_updates_for_experiment("Shift0", test = x), 
            get_IBBU_updates_for_experiment("Shift10", test = x), 
            get_IBBU_updates_for_experiment("Shift40", test = x)) 

d.exposure %>% 
  distinct(VOT) %>%
  ggplot(aes(x = VOT)) +
  d.updates$sf +
  scale_x_continuous(limits = c(10, 90)) +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r get-updates-nu-equal-kappa}
x <-
  tibble(VOT = seq(0, 100)) %>%
  mutate(x = map(VOT, ~ c(.x, predict_f0(.x, Mel = T)))) %>%
  pull(x)

# Plot updates for true rightward shifts of various magnitudes
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x), 
            get_IBBU_updates(condition = 20, test = x), 
            get_IBBU_updates(condition = 30, test = x), 
            get_IBBU_updates(condition = 40, test = x)) 

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r get-updates-nu-larger-kappa}
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x, nu = 10000), 
            get_IBBU_updates(condition = 20, test = x, nu = 10000), 
            get_IBBU_updates(condition = 30, test = x, nu = 10000), 
            get_IBBU_updates(condition = 40, test = x, nu = 10000)) 

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```
```{r get-updates-nu-smaller-kappa}
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x, kappa = 10000), 
            get_IBBU_updates(condition = 20, test = x, kappa = 10000), 
            get_IBBU_updates(condition = 30, test = x, kappa = 10000), 
            get_IBBU_updates(condition = 40, test = x, kappa = 10000)) 

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r}
m <- readRDS("../models/IBBU-Tan-Jaeger2022-Experiment2-scaled.RDS")
summary(m)
```

## Centering cues relative to expecations [C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]
Similarly, existing models of perceptual normalization---an alternative, but mutually compatible, hypothesis---also predict proportional changes in PSE. <!-- TO DO: write these sections for SI. along a single continuum normalization shift of cue mean just shifts cat function. for multi-dimensional cues, the same is true under cue integration models (variance and thus weighting does not change; shift along each dimension follows same logic as for single cue). for multi-dimensional multivariate models with interacting cues viewed from the perspective of a single cue, shifts can *appear* non-proportional but the pattern observed in KJ16---non proportionality without changes in slopes---would seem to be impossible to create as long as the test stimuli form a line in the multidimensional cue space -->

## Exemplar theory
<!-- generally the most similar exemplars should dominate perception. So it's unclear why one wouldn't converge against the input -->

# Materials {sec:stimulus-generation}

## Recordings
An L1-US English female talker originally from New Hampshire was recruited for recording of the stimuli. She was recorded at the Human Language Processing lab at the Brain & Cognitive Sciences Department, University of Rochester with the help of research assistant (also an L1-US English speaker). She was 23 years old at the time of recording and was judged by the research assistant to have a generic US American accent known as "general American". 

Four /d-t/ minimal pairs (dill-till, din-tin, dim-tim, dip-tip) were recorded together with 20 filler words. These fillers were made up of 10 minimal or near minimal pairs with different sounds at onset. The word pairs were separated into two lists so that they would appear in separate blocks during recording. Each critical pair was repeated 8 times while the filler pairs were repeated 5 times. Word presentation was delivered with PsychoPy [@Peirce2019] and the presentation was controlled by the researcher from a computer located outside the recording room. The order of each block was randomised such that target words never appeared consecutively. The talker was instructed to speak clearly and confidently, and to maintain a consistent distance from the microphone. 

## Annotation
All critical pairs of the talker's recordings were annotated.
Durational, measurements of voicing lead, VOT, and vowel were taken in addition to the mean F0 of the first 25% of the vowel duration. Annotations were made with a combination of listening to the audio file and inspection of the waveform and spectrogram. The annotation boundaries were made according to the following principles:

- pre-voicing (voicing during closure) 
  -**start:** the first sign of periodicity in the waveform before closure release. 
  -**End:** the point of closure release

- VOT 
  -**start:** the point of closure release. 
  -**End:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram.

- Vowel 
  -**start:** the beginning of clearly defined periodicity in the waveform and at the appearance of low frequency energy in the spectrogram. 
  -**End:** if before a stop, when periodicity becomes irregular or at closure onset; if before a lateral, when formant transition approaches steady state; if before a nasal, at point where formants show a step-wise shift and when intensity shows a steep decline. 

- F0 at vowel onset 
  -the average pitch measurement estimated over the first 25% of the total vowel duration.

[INSERT EXAMPLE IMAGES]

## Resynthesis
The stimuli was created using the "progressive cutback and replacement method" by [@winn2020] implemented in Praat [@boersma2022praat]. This automates and greatly simplifies the process for generating highly natural sounding stimuli. Users of the script need only specify certain parameters to produce desired stimuli. Stimuli with pre-voicing were created separately from stimuli with positive VOT. This was because the script was not coded to automate the creation of tokens with pre-voicing that are natural sounding ^[it can however, produce pre-voicing sufficiently well for demonstration purposes, see video demo at https://www.youtube.com/watch?v=-QaQCsyKQyo]. As such, the pre-voicing stimuli were created by prepending pre-voicing generated from naturally produced tokens (described below) that were edited with a separate process.

### Tokens with positive VOTs
For each minimal pair a continuum of 31 tokens was generated between 0ms and 150ms with a step-size of 5ms. A token of the voiced category from each pair was selected to be the base sound file to make the continuum. All four minimal pair continua had an identical aspiration sound which was excised from one of the voiceless tokens produced by the talker.  
While the main manipulation of the recordings was done on VOT we set the fundamental frequency (F0) to covary with VOT according to the natural correlation exhibited by our talker. The F0 values were predicted by regressing the talker's F0 measurements on VOT. Target F0 values for each token were then generated by setting the predicted F0 values of the end-point VOT tokens (0ms and 150ms) in the Praat script.   
The vowel cut-back ratio was set at 0.33 which translates into a third of a ms vowel reduction for every 1ms of VOT. This ratio followed the estimated vowel duration-VOT trade-off for dip-tip minimal pair tokens reported in @allen-miller1999. The maximum allowed vowel cut-back was 0.5ms to avoid the short vowel in **dip** becoming too short. 
Lastly, the rate of increase for aspiration intensity was kept at the default settings of the script.

### Token with pre-voicing ('negative VOTs')
Pre-voicing in 5ms increments were generated from a a clear pre-voicing waveform excised from a voiced token produced by the talker. To achieve a desired duration a duration factor is first computed and then converted with the "lengthen (overlap-add)" function in Praat. For example, if the desired amount of prevoicing was 50ms then the duration factor would be 50ms/length of the original pre-voicing sample. Each pre-voicing step is then prepended to a token with 0ms VOT. Each of these 0ms tokens was generated with @winn2020 Praat script by manually entering the expected F0 value for a given pre-voicing duration based on the predictions of the linear model. No vowel-cut back was implemented for pre-voiced tokens.

All the synthesised stimuli were subsequently annotated for pre-voicing, VOT, vowel duration and F0 at the first 5ms from vowel onset. This F0 measurement was made in order to align the data with the production database that we use for ideal observer analysis. Each item's F0 in relation to VOT is plotted in figure X. 

```{r}
# annotation information of synthesised stimuli
files <- list.files(path = "../materials/stimuli_AE/annotation_files/", pattern = "^d.*_stimuli_f0_measured.csv")

file_list <- seq(1:length(files))

d <- map(file_list, ~ read_csv(file = str_c("../materials/stimuli_AE/annotation_files/", files[[.x]]))) 
d %<>% bind_rows() %>% 
  mutate(temp = filename) %>% 
  separate(temp, c("word", "VOT", "target_f0"), sep = "_") %>% 
  mutate(VOT = as.numeric(gsub("VOT(.*)$", "\\1", VOT))) %>% 
  arrange(filename, word, vowel, VOT, target_f0, f0_5ms_into_vowel, f0_10ms_into_vowel)

#write_csv(d, "../../materials/stimuli_AE/annotation_files/AE_stimuli_synthesised_actualf0.csv")

# using lm() to obtain general linear function
lm(f0_5ms_into_vowel ~ 1 + VOT, data = d)

d %>% 
  ggplot(aes(x = VOT, y = f0_5ms_into_vowel, colour = word)) +
  geom_point(alpha = .4, size = 2) + 
  scale_x_continuous(breaks = seq(-100, 150, 10)) +
  scale_y_continuous(breaks = seq(240, 252, 0.5)) +
  geom_abline(intercept = 245.46968, slope = 0.03827, alpha = .5) +
  scale_colour_discrete("Minimal pair")

d %>% 
  ggplot(aes(x = VOT, y = vowel, colour = word)) +
  geom_point(alpha = .4, size = 2) + 
  scale_x_continuous(breaks = seq(-100, 150, 10)) +
  scale_y_continuous("vowel length") +
  geom_abline(intercept = 245.46968, slope = 0.03827, alpha = .5) +
  scale_colour_discrete("Minimal pair")
```


## List creation

## Exclusions
We provide additional information on participants' performance during catch trials and on labelled trials (for which there is a clearly correct response). Both of these measures were used to exclude participants.

### Performance on catch trials

(ref:plot-catch-trial-performance) Of the 122 participants, 1 committed more than 3 errors (<84% accuracy)

```{r plot-catch-trial-performance, fig.height=4, fig.width=6, fig.cap="ref:plot-catch-trial-performance"}
d %>% 
  group_by(ParticipantID) %>% 
  summarise(Total_errors = 18 -sum(CatchTrial.Correct, na.rm = T)) %>%
  ggplot(aes(x = Total_errors)) +
  geom_bar(width = .5) +
  scale_x_continuous("Total catch trial errors", breaks = seq(0, 6, 1)) +
  scale_y_continuous("Number of participants", breaks = seq(0, 120, 5)) 
```

### Performance on labelled exposure trials

```{r labeled-trial-performance-group, fig.width=6, fig.height=4, message=F}
p.labelled <- d %>% 
  filter(Phase == "exposure") %>% 
  group_by(ParticipantID, Condition.Exposure) %>% 
  summarise(LabeledTrial.Correct = 72 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>% 
  group_by(Condition.Exposure, LabeledTrial.Correct) %>% 
  summarise(freq = n()) %>% 
  ggplot(aes(x = LabeledTrial.Correct, y = freq, fill = Condition.Exposure)) +
  geom_bar(stat = "identity", position = position_dodge2(width = .5, preserve = "single"), width = .5) +
  geom_text(aes(label = freq), position = position_dodge(width = .5), vjust = -.5) +
  scale_colour_manual("Exposure condition", 
                      aesthetics = c("colour", "fill"), 
                      breaks = c("Shift0", "Shift10", "Shift40"), 
                      values = c("#cc0000", "#12D432","#0481F3")) +
  scale_x_continuous("Total labelled trial errors") +
  scale_y_continuous("No. of participants", breaks  = seq(0, 115, 5)) +
  theme(legend.position = "none") 

p.labelled.group <- d %>% 
  filter(Phase == "exposure" & Item.Labeled == T) %>% 
  group_by(ParticipantID, Condition.Exposure, Block) %>% 
  summarise(LabeledTrial.Errors = 24 - sum(LabeledTrial.Correct, na.rm = TRUE)) %>% 
  ggplot(aes(x = LabeledTrial.Errors, fill = Condition.Exposure)) +
  geom_bar(position = position_dodge(width = .5, preserve = "single"), width = .5) +
  scale_colour_manual("Exposure condition", 
                      aesthetics = c("colour", "fill"), 
                      breaks = c("Shift0", "Shift10", "Shift40"), 
                      values = c("#cc0000", "#12D432","#0481F3")) +
  scale_x_continuous("Total labelled trial errors") +
  theme(legend.position = "top") +
  facet_grid(Condition.Exposure ~ Block) 

p.labelled | p.labelled.group + 
  plot_layout(guides = "collect") & theme(legend.position = "top")
```


```{r, message=FALSE, fig.width=6, fig.height=5}
d_for_analysis %>% 
  filter(Is.CatchTrial == FALSE & Phase == "exposure") %>% 
  mutate(Item.Labeled = factor(ifelse(Item.Labeled == TRUE, "labeled", "unlabeled"))) %>% 
  summarise(Condition.Exposure, Block, Item.Labeled, Response.Correct) %>% 
  ggplot(aes(x = Response.Correct, fill = Item.Labeled)) +
  geom_bar(position = position_dodge2(width = .5, preserve = "single"), width = .5) +
  scale_x_discrete("Response", labels = c("incorrect", "correct")) +
  scale_colour_manual("Labelling", aesthetics = c("colour", "fill"), breaks = c("labeled", "unlabeled"), values = c("#f6546a", "#00ced1")) +
  theme(legend.position = "top") +
  facet_grid(Condition.Exposure ~ Block) 
```


```{r fig,width=8, fig.height=5, fig.width=6}
d %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  filter(Is.CatchTrial == FALSE) %>%
  mutate(Response.log_RT = log10(Response.RT)) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>%
  ggplot(aes(x = mean, y = sd, label = ParticipantID)) +
  geom_text(aes(colour = Condition.Exposure), alpha = .5) +
  geom_rug(aes(colour = Condition.Exposure), alpha = .5) +
  scale_x_continuous("mean log-RT (log10 of msec)") +
  scale_y_continuous("SD of log-RT") +
  scale_color_manual("Exposure condition", 
                      aesthetics = c("colour", "fill"), 
                      breaks = c("Shift0", "Shift10", "Shift40"), 
                      values = c("#cc0000", "#12D432","#0481F3")) +
  theme(legend.position = "top") +
  facet_wrap( ~ Block )
```


```{r, fig.width=8, fig.height=3}
d %>%
  filter(
    Is.CatchTrial == FALSE &
    Exclude_participant.due_to_catch_trials == FALSE &
    Exclude_participant.due_to_labeled_trials == FALSE &
    Exclude_participant.due_to_VOT_slope == FALSE) %>% 
  group_by(ParticipantID, Block) %>%
  mutate(
    Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
    Response.log_RT.scaled = scale(Response.log_RT), # deducts each value with subject's own mean, divide by own SD
    Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(across(c(ParticipantID, Condition.Exposure, Block), factor)) %>%
  group_by(ParticipantID, Condition.Exposure,Block, Trial) %>% 
  summarise(Response.log_RT) %>% 
  ggplot(aes(x = Trial, y = Response.log_RT, colour = ParticipantID, group = Condition.Exposure)) +
  geom_path(aes(group = ParticipantID), alpha = .3) +
  scale_x_continuous("Trial", breaks = c(12, 24, 36, 48)) +
  scale_y_continuous("mean log-RT (log10 of msec)") +
  facet_grid(Condition.Exposure ~ Block, scales = "free_x", space = "free_x") +
  theme(legend.position = "none")
```

```{r, fig.width=8, fig.height=3}
d %>% 
  group_by(Trial, Condition.Exposure, Block) %>%
  mutate(Response.log_RT = log10(Response.RT)) %>%
  summarise_at("Response.log_RT", .funs = list("mean" = mean, "sd" = sd)) %>% 
  ggplot(aes(x = Trial, y = mean)) +
  geom_ribbon(aes(ymin = mean - 2*sd, ymax = mean + 2*sd), colour = "lightgrey", alpha = .1) +
  geom_line(aes(group = Condition.Exposure, color = Condition.Exposure), size = 1) +
  scale_x_continuous("Trial", breaks = c(12, 24, 36, 48)) +
  scale_y_continuous("mean log-RT (log10 of msec)") +
  coord_trans(y = "log10") +
      scale_color_manual("Exposure condition", 
                      aesthetics = c("colour", "fill"), 
                      breaks = c("Shift0", "Shift10", "Shift40"), 
                      values = c("#cc0000", "#12D432","#0481F3")) +
  facet_grid(Condition.Exposure~ Block, scales = "free_x", space = "free_x") +
  theme(legend.position = "top")
```


## Analysis approach {sec:analysis-approach}
We analyzed participants' categorisation responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric model [@prins2012psychometric]. The psychometric model is an extension of mixed-effects logistic regression that also takes into account attentional lapses. 

The mixed-effects psychometric model describes the probability of "t"-responses as a weighted mixture of a perceptual and a lapsing model. The perceptual model predicts responses on trials where participants pay attention and respond based on the stimulus. We implemented the perceptual model as used mixed-effects logistic regression, predicting "t"-responses from exposure condition (backward difference coded, comparing the +10ms against the +0ms shift condition, and the +40ms against the +10ms shift condition), test block (backward difference coded from the first to last test block), VOT (Gelman scaled), and their full factorial interaction. The model included by-participant random intercepts and slopes for all within-participant maniplations (block and VOT) and by-item random intercepts and slopes for all within-participant manipulations (exposure condition, block, VOT).

The lapsing model predicts participant responses that are made independent of the stimulus---for example, responses that result from attentional lapses. These responses depend only on participants' response bias. We used mixed-effects logistic regression with only a population-level intercept, allowing non-uniform responses bias but assuming that response biases did not vary across participants. Finally, the relative weight of the perceptual and lapsing model is determined by the lapse rate. We again used mixed-effects logistic regression with only a population-level intercept, inferring lapse rates from that data while assuming that lapse rates did not vary across participants or blocks. This assumption is validated by Figures \@ref(fig:plot-fit-intercept-slope-PSE)A-B, which shows that participants' responses (the point ranges) in all exposure and test blocks approach 0 and 100% "t"-responses for small and large VOTs, respectively [unlike in @kleinschmidt2020]. One possible explanation for these consistently small lapse rate [also compared to @clayards2008] is that the present experiment recruited participants from the Prolific platform, which has been found [@REFS] to deliver higher data quality than the Amazon's Mechanical Turk crowdsourcing platform used by @kleinschmidt-jaeger2016 and @kleinschmidt2020.

We fit the psychometric model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. To faciltiate comparison of effect sizes across predictors, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008scaling]. Following previous work from our lab [@horberg2021rational; @xie2021cross], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we used Student priors centered around zero with a scale of 2.5 units [following @gelman2008weakly] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $1 < \hat{R} < 1.01$.

# Obtaining estimates of listeners prior expectations

The database contains 92 talkers (female = XXX, male = XXX; for details, see SI, \@ref(sec:VOT-production)).

# Norming experiment: Estimating listener's expectations prior to informative exposure {sec:norming-experiment}
The norming experiment investigates native (L1) US English listeners' categorization of word-initial stop voicing by an unfamiliar female L1 US English talker, prior to more informative exposure. Specifically, listeners heard isolated recordings from a /d/-/t/ continuum, and had to respond which word they  heard (e.g., "din" or "tin"). The recordings varied in voice onset time (VOT), the primary phonetic cue to word-initial stop voicing in L1 US English, as well as correlated secondary cues (f0 and rhyme duration). Critically, exposure was relatively uninformative about the talker's use of the phonetic cues in that all phonetic realizations occurred equally often. 

The primary goal of norming was methodological. We used the norming experiment to test basic assumptions about the paradigm and stimuli we employ in this study. We obtain estimates of the category boundary between /d/ and /t/ *for the specific stimuli used in Experiment 2*, as perceived by *the type of listeners we seek to recruit for the main experiment*. We also test whether prolonged testing across the phonetic continuum changes listeners' categorization behavior. Previous work has found that prolonged testing on uniform distributions can reduce the effects of previous exposure [e.g., @mitterer2011; @liu-jaeger2018], at least in listeners of the age group we recruit from [@scharenborg2013comparing]. However, these studies employed only a small number of 5-7 perceptually highly ambiguous stimuli, each repeated many times. In the norming experiment, we employ a much larger set of stimuli that span the entire continuum from very clear /d/s to very clear /t/s, each presented only twice. If prolonged testing changes listeners' responses, this has to be taken into account in the design of the main.


## Methods 
### Participants
Participants were recruited over Amazon's Mechanical Turk platform, and paid $2.50 each (for a targeted remuneration of \$6/hour). The experiment was only visible to Mechanical Turk participants who (1) had an IP address in the United States, (2) had an approval rating of 95% based on at least 50 previous assignments, and (3) had not previously participated in any experiment on stop voicing from our lab. 

24 L1 US English listeners (female = 9; mean age = 36.2 years; SD age = 9.2 years) completed the experiment. To be eligible, participants had to confirm that they (1) spent at least the first 10 years of their life in the US speaking only English, (2) were in a quiet place, and (3) wore in-ear or over-the-ears headphones that cost at least \$15. 

### Materials 
The VOT continua ranged from -100ms VOT to +130ms VOT in 5ms steps. Experiment 1 employs 24 of these steps (-100, -50, -10, 5  `r paste0(seq(15, 90, 5), collapse = ", ")`, `r paste0(seq(100, 130, 10), collapse = ", ")`). VOT tokens in the lower and upper ends were distributed over larger increments because stimuli in those ranges were expected to elicit floor and ceiling effects, respectively. 

We further set the F0 at vowel onset to follow the speaker's natural correlation which was estimated through a linear regression analysis of all the recorded speech tokens. We did this so that we could determine the approximate corresponding f0 values at each VOT value along the continua as predicted by this talker's VOT. The duration of the vowel was set to follow the natural trade-off relation with VOT reported in @allen1999. This approach closely resembles that taken in @theodore-monto2019, and resulted in continuum steps that sound highly natural [unlike the robotic-sounding stimuli employed in @clayards2008; @kleinschmidt-jaeger2016]. All stimuli are available as part of the OSF repository for this article.

In addition to the critical minimal pair continua we also recorded three words that did not did not contain any stop consonant sounds ("flare", "share", and "rare"). These word recordings were used as catch trials. Stimulus intensity was set to 70 dB sound pressure level for all recordings. 

### Procedure
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu//experiments/DLVOT/series-A/experiment-A.html?list_test=NORM-A-forward-test). The first page of the experiment informed participants of their rights and the requirements for the experiment: that they had to be native listeners of English, wear headphones for the entire duration of the experiment, and be in a quiet room without distractions. Participants had to pass a headphone test, and were asked to keep the volume unchanged throughout the experiment. Participants could only advance to the start of the experiment by acknowledging each requirement and consenting to the guidelines of the Research Subjects Review Board of the University of Rochester. 

On the next page, participants were informed about the task for the remainder of the experiment. They were informed that they would hear a female talker speak a single word on each trial, and had to select which word they heard. Participants were instructed to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. This was done to encourage their full attention.

Each trial started with a dark-shaded green fixation dot being displayed. At 500ms from trial onset, two minimal pair words appeared on the screen, as shown in Figure \@ref(fig:exp1-example-trial). At 1000ms from trial onset, the fixation dot would turn bright green and an audio recording from the matching minimal pair continuum started playing. Participants were required to click on the word they heard. For each participant, /d/-initial words were either always displayed on the left side or always displayed on the right side. Across participants, this ordering was counter-balanced. After participants clicked on the word, the next trial began.


Participants heard 192 target trials (four minimal pair continua, each with 24 VOT steps, each heard twice). In addition, participants heard 12 catch trials. On catch trials, participant saw two written catch stimuli on the screen (e.g., "flare" and "rare"), and heard one of them (e.g. "rare"). Since these recordings were easily distinguishable, they served as a check on participant attention throughout the experiment. 

The order of trials was randomized for each participant with the only constraint that no stimulus was repeated before each stimulus had been heard at least once. Catch trials were distributed randomly throughout the experiment with the constraint that no more than two catch trials would occur in a row. Participants were given the opportunity to take breaks after every 60 trials. Participants took an average of 12 minutes (SD = 4.8) to complete the 204 trials, after which they answered a short survey about the experiment. 

```{r, echo=FALSE, warning=FALSE}
# load formatted dataframe from experiment 1
d.test <- read_csv("../data/d.test.Exp1.csv", show_col_types = F)

# load f0-5ms-into-vowel measurements of stimuli
d.f0.5ms <- 
  read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv", show_col_types = F) %>% 
  select(filename, VOT, f0_5ms_into_vowel) %>% 
  rename(Item.VOT = VOT,
         Item.F0 = f0_5ms_into_vowel,
         Item.Filename = filename) %>% 
  mutate(Item.Filename = paste0(Item.Filename, ".wav"))

d.test %<>% 
ungroup() %>%
  left_join(d.f0.5ms, by = c("Item.Filename", "Item.VOT")) %>% 
              mutate(Item.F0_Mel = normMel(Item.F0))

# mark catch trials rows and check correct
d.test %<>% 
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == TRUE & Item.MinimalPair == Response, TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == FALSE, NA, CatchTrial.Correct)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Excluded.due.to.CatchTrial = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 10, TRUE, FALSE))

# get the rows removed due to catch trial performance
excluded.data.due.to.catch <- 
  d.test %>% 
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == TRUE)

# set RT exclusion criteria and create excluded columns
d.test %<>%
  group_by(ParticipantID) %>%
  filter(Excluded.due.to.CatchTrial == FALSE) %>% 
  mutate(Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
         # scale the log RTs
         Response.log_RT.scaled = scale(Response.log_RT), 
         # this gives each subject's mean log_RT
         Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(Excluded.participant.due.to.mean.RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE))
         # Get participants whose means are more than 3x sd of mean of participant means. Mean of means is the same as mean of all rows because each participant has the same number of rows.
         
excluded.participants.due.to.mean.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE)

d.test %<>% 
  filter(Excluded.due.to.CatchTrial == FALSE & Excluded.participant.due.to.mean.RT == FALSE) %>% 
  group_by(ParticipantID) %>% 
  mutate(Excluded.trial.due.to.RT = ifelse(abs(Response.log_RT.scaled) > 3, TRUE, FALSE))
  
# get the rows removed due to RT
excluded.data.due.to.RT <- 
  d.test %>% 
  filter(Excluded.participant.due.to.mean.RT == TRUE | Excluded.trial.due.to.RT == TRUE) 

# get the proportion of rows excluded from analysis
proportion.excluded <- (nrow(excluded.data.due.to.RT)) / (nrow(d.test))

# make a dataframe after exclusion to be used for analysis
d.test.excluded <- 
  d.test %>% 
  filter(
    Is.CatchTrial == FALSE, 
    Excluded.due.to.CatchTrial == FALSE, 
    Excluded.trial.due.to.RT == FALSE) %>% 
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0))
```


### Exclusions
We excluded from analysis participants who committed more than 2 errors out of the 12 catch trials (<83% accuracy, N = 3), participants with an average reaction time (RT) more than three standard deviations from the mean of the by-participant means (N = 0), and participants who reported not to have used headphones (N = 0) or not to be native (L1) speakers of US English (N = 0). For the remaining participants, trials that were more than three SDs from the participant's mean RT were excluded from analysis (`r proportion.excluded * 100 `%). Finally, we excluded participants (N = 0) who had less than 50% data remaining after these exclusions. 

### Analysis approach
The goal of our behavioral analyses was to address three methodological questions that are of relevance to Experiment 2: (1) whether our stimuli resulted in 'reasonable' categorisation functions, (2) whether these functions differed between the four minimal pair items, and (3) whether participants' categorisation functions changed throughout the 192 test trials.

To address these questions, we fit a single Bayesian mixed-effects psychometric model to participants' categorization responses on critical trials [e.g., @prins2011]. The *lapsing model* only contained an intercept (the response bias in log-odds) and by-participant random intercepts. Similarly, the *model for the lapse rate* only had an intercept (the lapse rate) and by-participants random intercepts. No by-item random effects were included for the lapse rate nor lapsing model since these parts of the analysis---by definition---describe stimulus-*in*dependent behavior. The *perceptual model* included an intercept and VOT, as well as the full random effect structure by participants and items (the four minimal pair continua), including random intercepts and random slopes by participant and minimal pair. We did not model the random effects of trial to reduce model complexity. This potentially makes our analysis of trials in the model anti-conservative. Finally, the models included the covariance between by-participant random effects across the three linear predictors for the lapsing model, lapse rate model, and perceptual model. This allows us to capture whether participants who lapse more often have, for example, different response biases or different sensitivity to VOT (after accounting for lapsing).

We fit the model using the package \texttt{brms} [@R-brms_a] in R [@R; @RStudio]. Following previous work from our lab [@horberg2021rational; @xie2021cross], we used weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we standardized continuous predictors (VOT) by dividing through twice their standard deviation [@gelman2008scaling], and used Student priors centered around zero with a scale of 2.5 units [following @gelman2008weakly] and 3 degrees of freedom. For random effect standard deviations, we used a Cauchy prior with location 0 and scale 2, and for random effect correlations, we used an uninformative LKJ-Correlation prior with its only parameter set to 1, describing a uniform prior over correlation matrices [@Lewandowski2009]. Four chains with 2000 warm-up samples and 2000 posterior samples each were fit. No divergent transitions after warm-up were observed, and all $\hat{R}$ were close to 1.

### Expectations 
Based on previous experiments, we expected a strong positive effect of VOT, with increasing proportions of "t"-responses for increasing VOTs. We did not have clear expectations for the effect of trial other than that responses should become more uniformed (i.e move towards 50-50 "d"/"t"-bias or 0-log-odds) as the experiment progressed [@liu-jaeger2018] due to the un-informativeness of the stimuli. 
Previous studies with similar paradigms have typically found lapse rates of 0-10% [< -2.2 log-odds, e.g., @clayards2008; @kleinschmidt-jaeger2016]. 

```{r, warning=FALSE}
# set the mean and SD values for scaling/unscaling purposes
VOT.mean_exp1 <- mean(d.test.excluded$Item.VOT)
VOT.sd_exp1 <- sd(d.test.excluded$Item.VOT)
Trial.mean <- mean(d.test.excluded$Trial) 
Trial.sd <- sd(d.test.excluded$Trial)
f0.mean_exp1 <- mean(d.test.excluded$Item.F0_Mel)
f0.sd_exp1 <- sd(d.test.excluded$Item.F0_Mel)

d.test.excluded %<>%
  mutate(
    sVOT = (Item.VOT - VOT.mean_exp1) / (2 * VOT.sd_exp1),
    sTrial = (Trial - Trial.mean) / (2 * Trial.sd))

# load (or run) the psychometric model with interaction
fit_mix <- brm(
  bf(
    Response.Voicing == "voiceless" ~ 1,
    mu1 ~ 1 + (1 | g | ParticipantID),
    mu2 ~ 1 + sVOT * sTrial + (1 + sVOT | g | ParticipantID) + (1 + sVOT | h | Item.MinimalPair),
    theta1 ~ 1 + (1 | g | ParticipantID)),
  data = d.test.excluded,
  cores = 4,
  iter = 4000,
  warmup = 2000,
  family = mixture(bernoulli("logit"), bernoulli("logit")),
  control = list(adapt_delta = .99),
  file = "../models/Exp-NORM-lapsing-bias-GLMM")

# get effects of psychometric fit conditioned on VOT interacting with trial
if (file.exists("../models/conditional_effects_VOT_Trial.rds")) {
  psychometric_fit_exp1 <- read_rds("../models/conditional_effects_VOT_Trial.rds")
} else {
  int_conditions <- list(sTrial = sort(unique((d.test.excluded$Trial - Trial.mean)) / (2 * Trial.sd)))
  
  psychometric_fit_exp1 <-
    conditional_effects(
      fit_mix,
      effects = "sVOT:sTrial",
      int_conditions = int_conditions,
      ndraws = 1000,
      plot = F)[[1]]
  
  write_rds(psychometric_fit_exp1, file = "../models/conditional_effects_VOT_Trial.rds")
}

# get the PSE from the fitted categorisation function
PSE.fit_mix <- descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_exp1, VOT.sd_exp1) 

# get posterior samples of intercept and slope, and median qi of the PSEs
post_sample_norm <- fit_mix %>% 
  spread_draws(b_mu2_Intercept, b_mu2_sVOT) %>% 
  mutate(PSE = descale(-(b_mu2_Intercept/b_mu2_sVOT), VOT.mean_exp1, VOT.sd_exp1)) %>% 
  median_qi(PSE)
```



```{r}
# prepare data for predicted categorisation by min pair
newdata <- expand_grid(
  sVOT = (seq(-100, 130, 1) - VOT.mean_exp1) / (2 * VOT.sd_exp1),
  Item.MinimalPair = levels(factor(d.test.excluded$Item.MinimalPair)),
  ParticipantID = levels(factor(d.test.excluded$ParticipantID)),
  sTrial = 0)

# get expectation values for categorisation by min pair through posterior predictions
if (file.exists("../models/categorisation_by_min_pair.rds")) {
  cat_minimalpair <- read_rds("../models/categorisation_by_min_pair.rds")
} else {
  cat_minimalpair <- fit_mix %>%
    epred_draws(
      newdata = newdata,
      ndraws = 1000,
      re_formula = ~ (1 + sVOT | Item.MinimalPair))
  write_rds(cat_minimalpair, file = "../models/categorisation_by_min_pair.rds")
}
```



```{r fitted-categorisation-minimal-pair"}
remove_axes_titles <- theme(axis.title.x = element_blank(),
                            axis.title.y = element_blank())
MinPair_plot <- 
  cat_minimalpair %>% 
  group_by(sVOT, Item.MinimalPair) %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_exp1, VOT.sd_exp1), 
             y = .epred, colour = Item.MinimalPair)) +
  stat_lineribbon(alpha = .9, .width = 0.95) +
  scale_y_continuous("Fitted proportion of 't'-responses") +
  scale_color_manual("Minimal Pair", breaks = c("dilltill", "dimtim", "dintin", "diptip"),  
                     values = c("#002699", "#0040ff", "#668cff", "#b3c6ff"), 
                     labels = c("dill/till", "dim/tim", "din/tin", "dip/tip")) +
  scale_fill_brewer("CI", palette = "Greys", type = "qual") +
  remove_axes_titles +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```



```{r psychometric-plot, fig.width=3.8, fig.height =2.6, message=FALSE}
p.vot <- 
  psychometric_fit_exp1 %>% 
  group_by(sVOT) %>% 
  summarise(estimate__ = mean(estimate__),
         lower__ = mean(lower__),
         upper__ = mean(upper__)) %>% 
  ggplot(aes(x = descale(sVOT, VOT.mean_exp1, VOT.sd_exp1), 
             y = estimate__)) +
  scale_x_continuous("VOT (ms)", breaks = scales::pretty_breaks(n = 4), limits = c(-100, 130)) +
  scale_y_continuous("Fitted proportion of 't' responses") +
  geom_ribbon(
    aes(ymin = lower__, 
        ymax = upper__), alpha = .08) +
  geom_line(linewidth = 1.5, 
            colour = "#333333",
            alpha = .8) +
  geom_errorbarh(
    data = post_sample_norm %>% 
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y), 
    color = "#333333",
    height = 0,
    alpha = .8,
    size = 1, 
    inherit.aes = F) +
  geom_label(data = post_sample_norm %>% 
      mutate(y = 0.01, PSE = round(PSE)),
    mapping = aes(x = PSE, y = y, label = PSE),
    color = "#333333", 
    size = 1.8,
    label.padding = unit(0.18, "lines"),
    inherit.aes = F) +
  annotate(
    geom = "text",
    x = 75,
    y = 0.01, 
    label = paste(round(post_sample_norm[[2]]), "ms", "-", round(post_sample_norm[[3]]), "ms"),
    size = 2.5,
    colour = "darkgray") +
## transformation of by-participant means into empirical logits
   # stat_summary(
   #  data = d.test.excluded %>% 
   #    group_by(ParticipantID, Item.VOT) %>% 
   #    mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
   #    summarise(Response.EmpiricalLogitVoiceless = qlogis((sum(Response.ProportionVoiceless) + .5)/(length(Response.ProportionVoiceless) + 1))), 
   #  mapping = aes(x = Item.VOT, 
   #                y = Response.EmpiricalLogitVoiceless),
   #  geom = "pointrange",
   #  fun.data = function(x){mean_cl_boot(x) %>% mutate(across(c(y, ymin, ymax), ~ plogis(.x)))},
   #  size = .4,
   #  colour = "#c4b7a6",
   #  alpha = .6,
   #  inherit.aes = F) 
  stat_summary(
    data = d.test.excluded %>% 
      group_by(ParticipantID, Item.VOT) %>% 
      mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
      summarise(Response.ProportionVoiceless = mean(Response.ProportionVoiceless)), 
    mapping = aes(x = Item.VOT, 
                  y = Response.ProportionVoiceless),
    geom = "pointrange",
    fun.data = "mean_cl_boot",
    size = .4,
    colour = "#c4b7a6",
    alpha = .6,
    inherit.aes = F) +
  labs(x = "VOT (ms)")
```

(ref:fitted-categorisation-plots) Categorisation functions and points of subjective equality (PSE) derived from the Bayesian mixed-effects psychometric model fit to listeners' responses in Experiment 1. The categorization functions include lapse rates and biases. The PSEs correct for lapse rates and lapse biases (i.e., they are the PSEs of the perceptual component of the psychometric model).\protect\footnote{Here and in Experiment 2, lapse biases were close to uniform (.5/.5). For such scenarios, bias-corrected PSEs will be very similar to uncorrected PSEs. This is also evident in Figure \ref{fig:fitted-categorisation-plots}.} **Left:** Effects of VOT, lapse rate, and lapse bias, while marginalizing over trial effects as well as all random effects. Vertical point ranges represent the mean proportion and 95% bootstrapped CIs of participants' "t"-responses at each VOT step. Horizontal point ranges denote the mean and 95% quantile interval of the points of subjective equality (PSE), derived from the 8000 posterior samples of the population parameters. **Right:** The same but showing the fitted categorization functions for each of the four minimal pair continua. Participants' responses are omitted to avoid clutter.   

```{r fitted-categorisation-plots, warning=FALSE, fig.width=6, fig.height = 3.3, fig.cap="(ref:fitted-categorisation-plots)"}
MinPair_plot <- MinPair_plot + labs(x = "VOT (ms)")

xlab <- p.vot$labels$x
p.vot$labels$x <- MinPair_plot$labels$x <- " "

(p.vot | MinPair_plot) + 
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "top")
```



```{r by-participant-lapse-bias, warning=FALSE, fig.height=2.6, fig.width=6.5}
lapse_participant <- fit_mix %>% spread_draws(r_ParticipantID__theta1[ParticipantID, term], b_theta1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_lapse = b_theta1_Intercept + r_ParticipantID__theta1,
         estimated_lapse = plogis(Participant_lapse) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__theta1, Participant_lapse, estimated_lapse) %>%
  mode_hdci(estimated_lapse)

bias_participant <- fit_mix %>%
  spread_draws(r_ParticipantID__mu1[ParticipantID, term], b_mu1_Intercept) %>%
  group_by(ParticipantID) %>%
  mutate(ParticipantID = factor(ParticipantID),
         Participant_bias = b_mu1_Intercept + r_ParticipantID__mu1,
         estimated_bias = plogis(Participant_bias) * 100) %>%
  select(ParticipantID, term, r_ParticipantID__mu1, Participant_bias, estimated_bias) %>%
  mode_hdci(estimated_bias)

estimate_minpair <- fit_mix %>% spread_draws(r_Item.MinimalPair__mu2[Item.MinimalPair, term], b_mu2_Intercept, b_mu2_sVOT) %>%
  group_by(Item.MinimalPair) %>%
  mutate(predicted_eff = ifelse(term == "Intercept", r_Item.MinimalPair__mu2 + b_mu2_Intercept, r_Item.MinimalPair__mu2 + b_mu2_sVOT)) %>%
  group_by(Item.MinimalPair, term) %>%
  mode_hdci(predicted_eff) %>%
  pivot_wider(names_from = term,
              values_from = c(predicted_eff, .lower, .upper))
```

The lapse rate was estimated to be on the slightly larger side, but within the expected range 
(`r make_CI(fit_mix, "theta1_Intercept", "theta1_Intercept < 0")`). Maximum a posteriori (MAP) estimates of by-participant lapse rates ranged from XX . Very high lapse rates were estimated for four of the participants with one in particular whose CI indicated exceptionally high uncertainty. These lapse rates might reflect data quality issues with Mechanical Turk that started to emerge over recent years [see @REFS; and, specifically for experiments on speech perception, @cummings-theodore2023], and we return to this issue in Experiment 2. 

The response bias were estimated to slightly favor "t"-responses (`r make_CI(fit_mix, "mu1_Intercept", "mu1_Intercept > 0")`), as also visible in Figure \@ref(fig:fitted-categorisation-plots) (left).
Unsurprisingly, the psychometric model suggests high uncertainty about the participant-specific response biases, as it is difficult to reliably estimate participant-specific biases while also accounting for trial and VOT effects (range of by-participant MAP estimates: XX). For all but four participants, the 95% CI includes the hypothesis that responses were unbiased. Of the remaining four participants, three were biased towards "t"-responses and one was biased toward "d"-responses.

There was no convincing evidence of a main effect of trial ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sTrial", "mu2_sTrial < 0")`). Given the slight overall bias towards "t"-responses, the direction of this effect indicates that participants converged towards a 50/50 bias as the test phase proceeded. This is also evident in Figure \@ref(fig:fitted-categorisation-plots) (right). In contrast, there was clear evidence for a positive main effect of VOT on the proportion of "t"-responses ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT", "mu2_sVOT > 0")`). The effect of VOT was consistent across all minimal pair words as evident from the slopes of the fitted lines by minimal pair \@ref(fig:fitted-categorisation-plots) (left). MAP estimates of by minimal pair slopes ranged from . The by minimal-pair intercepts were more varied (MAP estimates: ) with one of the pairs, dim/tim having a slightly lower intercept resulting in fewer 't'-responses on average. In all, this justifies our assumptions that word pair would not have a substantial effect on categorisation behaviour. From the parameter estimates of the overall fit we obtained the category boundary from the point of subjective equality (PSE) `r( descale(-(summary(fit_mix)$fixed["mu2_Intercept", 1] / summary(fit_mix)$fixed["mu2_sVOT", 1]), VOT.mean_exp1, VOT.sd_exp1) ms)` which we use for the design of Experiment 2.

Finally to accomplish the first goal of experiment 1, we look at the interaction between VOT and trial. There was weak evidence that the effect of VOT decreased across trials ($\hat{\beta} =$ `r get_CI(fit_mix, "mu2_sVOT:sTrial", "mu2_sVOT:sTrial < 0")`). The direction of this change---towards more shallow VOT slopes as the experiment progressed---makes sense since the test stimuli were not informative about the talker's pronunciation. Similar changes throughout prolonged testing have been reported in previous work. [@liu-jaeger2018; @liu-jaeger2019; @REFS].

Overall, there was little evidence that participants substantially changed their categorisation behaviour as the experiment progressed. Still, to err on the cautious side, Experiment 2 employs shorter test phases.





# Session Info

```{r session_info, echo=FALSE, results='markup'}
devtools::session_info()
```

