<!-- Do NOT knit this document. It is part of a larger document. Instead knit the main document (my-apa-formatted-article) -->

# General discussion
Over the last 20+ years, landmark studies in adaptive speech perception have demonstrated that listeners' interpretation of speech is not static. Instead, it can change with recent exposure, accommodating differences in pronunciation across talkers. Even a single session of exposure can be sufficient to substantially reduce losses in processing speed or accuracy that listeners might initially experience when exposed to unfamiliar speech patterns. Research on accent adaptation [AA, @eisner2013; @schertz2016; @xie2017], perceptual recalibration [VGPL/LGPL, @eisner-mcqueen2005; @kraljic-samuel2006; @kurumada2018; @norris2003; @reinisch-holt2014; @vroomen2007], and distributional learning [DL, @bejjanki2011; @idemaru-holt2020; @kleinschmidt2020; @nixon2016; @theodore-monto2019] suggests that this flexibility is achieved through changes in listeners' *categorization functions*---the mapping from acoustic or phonetic cues to the phonological categories that form the input to spoken language understanding. These changes in listeners' categorization functions seem to depend on the statistics of the exposure input in ways qualitatively expected under the hypothesis that adaptive speech perception involves distributional learning [@clayards2008; @idemaru-holt2011; @kleinschmidt-jaeger2015; @mcmurray-jongman2011].

As mentioned in the introduction, the pioneering generation of research on adaptive speech perception focused on the question of *whether* adaptation is observed, and *under what conditions* [for review, see @cummings-theodore2023]. <!-- The majority of studies on AA, for example, continue to employ the popular single exposure-test design without incremental testing. In this design, different groups of listeners are exposed to different speech input. The effects of that exposure are then assessed by a single post-exposure test, which typically is held identical across the groups. This design is well suited to assess the qualitative effects of exposure to different types of speech input---for example, whether familiarization with a talker's speech generally leads to improved speech recognition---but it sheds little light on *how* the phonetic properties of the speech input shape changes in listeners' perception. If repeated testing has been been employed in research on AA, it tended to be after considerably longer exposure [e.g., testing on different days, @eisner-mcqueen2006; but see @xie-kurumada2024]. The same largely holds for previous DL studies, which either employed a single batch test following exposure [e.g., @clayards2008; @idemaru-holt2020; @nixon2016; @schertz2016] or repeated testing over the course of multiple days [e.g., @mcclelland1999]. -->  Here, we have followed recent calls to better characterize the functional relation between exposure inputs and changes in listeners' perception [@xie2023; @OTHERS]. In particular, it remains unclear how listeners' categorization functions change incrementally with exposure, and how those changes depend on the amount and cumulative distribution of phonetic evidence in the input listeners have experienced so far.

The present study modified a distributional learning paradigm to investigate these questions. We found that listeners' categorization functions changed incrementally with exposure, and that the direction and magnitude of that change depended on listeners' prior expectations based on previously experienced speech input from other talkers (Prediction 1 from the introduction), and both the amount and distribution of phonetic evidence in the exposure input from the unfamiliar talker (Predictions 2a and 2b, respectively). The Bayesian hypothesis tests we conducted also suggest properties of adaptive speech perception that go beyond these qualitative predictions. These properties further inform theory by characterizing the computational properties of the mechanisms underlying listeners' adaptivity. First, we found that participants' categorization functions changed quickly with exposure, and that the speed of these changes slowed-down with additional exposure. Second, we found evidence of potential constraints on listeners' adaptivity, as well as asymmetries in these constraints depending on whether exposure distributions were shifted downwards or upwards on the VOT continuum. 

In the remainder of this article, we discuss these findings in more detail, and consider their implications for theories of adaptive speech perception. We begin with predictions (1) and (2a,b). Then we turn to questions about the speed of changes, and changes in that speed with additional exposure, before considering potential constraints on the initial moments of adaptive speech perception. We close by considering limitations of the present study and how future work can overcome them.


## Incremental adaptation based on the amount and distribution of phonetic evidence (Predictions 1 and 2a,b)
To the best of our knowledge, the present study is the first to assess the joint effects of prior and recent exposure gradiently unfold with increasing exposure, testing predictions (1) and (2a,b) described in the introduction. While most contemporary theories of adaptive speech perception share these qualitative predictions, few experiments have investigated how listeners' categorization functions change with continued exposure to a phonetic distribution.

We used a repeated exposure-test design to assess incremental changes in listeners' categorization functions from pre-exposure onward. We found that that listeners' categorization function shifted with increasing exposure to shifted phonetic distributions. The direction and magnitude of these shifts developed gradiently and were qualitatively consistent with the predictions of distributional learning accounts [e.g., @apfelbaum-mcmurray2015; @harmon2019; @johnson1997; @kleinschmidt-jaeger2015; @magnuson2020; @assmann-nearey2007; @sohoglu-davis2016; @xie2023]. This was the case both for shifts in listeners' categorization function relative to other exposure conditions (Table \@ref(tab:hypothesis-table-simple-effects-condition)) and for shifts in listeners' categorization function relative to their pre-exposure categorization function (Table \@ref(tab:hypothesis-table-simple-effects-block)). In both cases, every single test we conducted went in the direction predicted by distributional learning theories. We elaborate on these findings.

### Prediction 1: Adaptation begins with, and integrates, listeners' prior experience
The inclusion of a pre-exposure test in our design made it possible to assess prediction (1)---that shifts in listeners' categorization function should depend on how the exposure distributions *relative to listeners' relevant prior experiences*. Two findings of the present study lend support to prediction (1). First, listeners' responses prior to exposure were well approximated based on a database of /d/ and /t/ productions. Second, the direction of changes in listeners' PSEs *relative to pre-exposure* was predicted by the direction of the shift in /d/ and /t/ distributions relative to their distributions in prior experience (Figure \@ref(fig:exposure-means-database-matrix-plot)).

While these effects of prior knowledge are often assumed, this is---to our knowledge---the first time they have been demonstrated for adaptive speech perception. Both findings are predicted by distributional learning accounts that describe adaptive speech perception as incremental interpolation of listeners' prior expectations---based on previously experienced speech input---and the statistics of the exposure input. This includes rational theories of adaptive speech perception [e.g., the ideal adaptor framework, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016], some theories of normalization [e.g., the probabilistic sliding template model, @assmann-nearey2007], as well as episodic [@goldinger1997] and exemplar models [@johnson1997]. Other distributional learning accounts are in principle compatible with our finding but would need to be expanded to incorporate prior expectations [e.g., @bejjanki2011; @mcmurray-jongman2011; see discussion in @persson-jaeger2024; @xie2023].

We emphasize that future tests are necessary to confirm the effects of prior experience observed in the present study. We employed an estimate of participants' prior expectations that is based on a phonetic database of syllable-initial stop productions by speakers of L1-US English. While we chose this database because it matches the speech style of our stimuli, and contained talkers of the same gender (and of relatively similar f0) as the talker used in our experiment, the database is comparatively small. We aimed to remedy this downside by using five-fold cross-validation. This allowed us to express uncertainty about the true range of a typical L1-US English listeners' prior expectations (the gray ribbons in Figure \@ref(fig:plot-fit-PSE)C). It does not, however, remove the need to validate our results based on new participants and larger phonetic databases that are more likely to reflect the prior experience of a 'typical' listener.

### Prediction 2a: Adaptation increases with the amount of exposure
We also find supported prediction (2a)---that the magnitude of changes in listeners' categorization function should gradiently increase with the *amount* of exposure. This prediction received support from the comparisons across blocks: increasing exposure consistently yielded additional shifts in listeners' PSE. This replicates in a DL paradigm recent findings from VGPL/LGPL experiments [@cummings-theodore2023; @kleinschmidt-jaeger2012; @liu-jaeger2018; @liu-jaeger2019; @vroomen2007]. In a particularly informative study, Cummings and Theodore compared shifts in categorization function between groups of listeners after exposure to 1, 4, 10, or 20 lexically labeled shifted /s/ or /`r linguisticsdown::cond_cmpl("ʃ")`/ tokens (each matched by an equal number of unshifted tokens from the opposite category). Shifts in listeners' categorization functions increased with the number of exposure to tokens. @vroomen2007 found similarly increasing shifts in categorization functions *within* participants, comparing the effects of 1, 2, 4, ..., 32 exposures to visually labeled shifted tokens [see also @kleinschmidt-jaeger2012].^[With increasing exposure, the direction of shift begins to reverse [returning to baseline after 128-256 exposures, @kleinschmidt-jaeger2011; @vroomen2007] and can even change directional altogether, depending on the degree of the shift [@kleinschmidt-jaeger2012]. Later work showed that this reversal is predicted by distributional learning models<!-- , which attribute it to the fact that the VGPL paradigm presents the exact same stimulus on each exposure trial --> [@kleinschmidt-jaeger2015].] The present study demonstrated similarly gradient effects with increasing exposure to a mixture of labeled and unlabeled exposure tokens that were randomly sampled from a *distribution* of phonetic tokens, thereby more closely resembling the situation listeners would experience during everyday speech perception. This aspect of our results thus adds to a growing number of similarities in the findings between LGPL/VGPL and DL paradigms, as expected under the hypothesis that changes observed in both paradigms originate in the same underlying mechanisms [see discussions in @kleinschmidt2015; @zheng-samuel2020; @xie2023].

### Prediction 2b: Adaptation depends on the phonetic distribution in the exposure input
Finally, prediction (2b)---that the direction and magnitude of changes in listeners' categorization function should depend on the *phonetic distribution* of the exposure input---received support from the within-block comparisons across exposure conditions: the direction of the shift of the /d/ and /t/ category means in the exposure input correctly predicted the relative ordering of listeners' PSEs in Test 1-4, and shifts in category means of larger magnitude (+40 vs. baseline compared to +10 vs. baseline) yielded larger shifts in listeners' PSE. This extends the findings from previous DL studies [@bejjanki2011; @clayards2008; @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @kleinschmidt-jaeger2016; @nixon2016; @theodore-monto2019] to demonstrate gradient *incremental* adaptation towards the exposure distribution.


## Rapid adaptation with diminishing returns
Having established that exposure led to gradient changes in participants' categorization behavior, we next asked how quickly these changes occurred, and how the rate of change developed with additional exposure. The former informs how plausible it is that the same mechanisms that drive adaptive behavior in the present DL paradigm also underlie adaptive behavior during everyday speech perception. The latter speaks to the nature of the learning mechanisms.

### How quickly can listeners adapt their speech perception?
We found significant shifts in listeners' categorization function even after the briefest exposure tested. Exposure to 24 tokens each of shifted /d/ and /t/ was sufficient to significantly change how listeners interpreted subsequent inputs. Of note, only half of these exposure tokens labeled the intended category, the other half did not. Even when trials were labeled, labeling was indirect rather than through explicit feedback: on labeled trials, the two response options listeners saw both had the same onset stop (e.g., "din" and "dill"). Previous DL studies have assessed exposure effects after *much* longer exposures, typically hundreds of trials [e.g., 192 trials in @harmon2019; 200 in @idemaru-holt2011; 228 in @clayards2008; @kleinschmidt-jaeger2016; 236 in @theodore-monto2019; 456 in @nixon2016]. The present results demonstrate that a fraction of the amount of exposure employed in previous studies is sufficient to elicit changes in listeners' categorization behavior.

This result bears on ongoing discussions as to whether the type of adaptive changes observed in AA and LGPL/VGPL paradigms could plausibly originate in the same mechanisms as those observed in DL paradigms like in the present study [@bradlow-bent2008; @baese-berk2018; @zheng-samuel2020; @xie2023]. A number of studies have demonstrated improvements in the speed or accuracy of speech perception after relatively short exposure to an unfamiliar L2-accented talker [@bradlow2023; @clarke-garrett2004; @xie2018; @xie2021jep]. For example, Xie and colleagues [-@xie2018] found improvements in both the speed and accuracy of cross-modal priming after exposure to only 18 sentences from an L2-accented talker---the shortest tested exposure we are aware of [see also @clarke-garrett2004]. Other AA work has more directly demonstrated that exposure changes listeners' categorization function. For example, @xie2017 found changes in listeners categorization behavior after exposure to only 30 critical L2-accented words. The directionality of these changes was consistent with distributional learning accounts of adaptive speech perception. Together with evidence from additional experiments, Xie and colleagues concluded that "listeners dynamically update their own cue-weighting functions during rapid phonetic adaptation to foreign accents, and critically over much shorter time span[s] than shown in previous studies of second language phoneme learning". By demonstrating that DL paradigms can elicit qualitatively similar changes with similarly little exposure, the present study lends further plausibility to the hypothesis that these changes are driven by the same underlying mechanisms.

Some experiments on LGPL/VGPL have demonstrated effects after even less exposure, with detectable changes in listeners' categorization responses after as few as 2-4 exposures to visually or lexically labeled phonetically tokens [@cummings-theodore2023; @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @liu-jaeger2018; @liu-jaeger2019; @vroomen2007]. In comparing findings across paradigms, future work should keep in mind that DL and LGPL/VGPL paradigms differ in the amount of information conveyed by each exposure token. LGPL/VGPL paradigms typically employ exposure stimuli that are a) labeled and b) auditorily maximally ambiguous---falling between the two categories that the experiment focuses on. Distributional learning accounts predict that such stimuli should be highly informative, leading to comparatively large changes in categorization behavior. This is in line with recent findings: when stimuli in LGPL/VGPL experiments are shifted less than to the point of maximal auditory ambiguity, listeners exhibit smaller shifts in categorization behavior [@babel2019; @kleinschmidt-jaeger2012; @tzeng2021; see also @cummings-theodore2023]. In contrast, DL paradigms a) typically employ only unlabeled stimuli [@clayards2008] or a mixture of unlabeled and labeled stimuli [e.g., @kleinschmidt-jaeger2016 and the present paradigm], and b) reflect a *distribution* of phonetic properties---ranging from more to less expected under listeners' prior expectations. This makes the speech inputs in DL paradigms more similar to what one would expect during exposure to natural accents and other cross-talker differences. But it also means that exposure tokens in DL experiments are, on average, considerably less informative than in an LGPL/VGPL experiment. Future work that aims to compare the speed of adaptive speech perception across these two paradigms should thus do so *relative to the amount of information conveyed by each exposure*.

<!-- TO DO: *could* add a figure here showing how the PSE would be predicted to change for our experiment under a certain kappa, nu combination, and compare that against PSE changes for the same data when all /d/ stimuli are set to *prior* PSE value and all /t/ stimuli are set to prior /t/ (or vice versa) -->

### First fast, then slow: diminishing returns of exposure
Our comparisons across test blocks within each exposure condition found suggestive---but not decisive---evidence that the speed of incremental changes in listeners' PSE decreased with increasing exposure: the same amount and distribution of phonetic evidence yielded smaller *additional* changes in listeners' PSE, the more exposure blocks listeners had already experienced. To the best of our knowledge, this is the first study to report this pattern for adaptive speech perception. A similar pattern of 'diminishing returns' is, however, also indirectly evident in at least one other recent study. Kleinschmidt [-@kleinschmidt2020, Experiment 3] re-analyzes several DL experiments originally presented in @kleinschmidt-jaeger2016. In these experiments, which very much informed the present study, different groups of participants were exposed to VOT distributions that were shifted to different degrees. Unlike the present study, the experiments by Kleinschmidt and Jaeger were not designed to detect incremental changes in listeners' categorization function: they lacked incremental tests over identical VOTs, allowing trial order to confound VOT. Still, Figure 9 from @kleinschmidt2020, replotted here with aesthetic changes as Figure \@ref(fig:kleinschmidt2020), is compatible with the hypothesis that adaptation initially proceeded quickly, and then increasingly more slowly. This is particularly evident for the more extreme exposure conditions, though the data are (even) more noisy than for the present study.

<!-- TO DO: add description of Dave's study; incl. absence of incremental testing, and the way it was 'hacked'/approximated in the modeling in section XXX of his ms. NOTE: this is NOT about shrinkage. it's about diminishing returns -->

Such diminishing returns of exposure are explicitly predicted only by some distributional learning models [e.g., error-based learning models, @davis-sohoglu2020; @harmon2019; @olejarczuk2018; @sohoglu-davis2016; Bayesian ideal adaptors, @kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016]: prior to adaptation, an idealized listener (horizontal gray ribbons in Figure \@ref(fig:plot-fit-PSE)C) would, on average, experience larger prediction errors (mean surprisal per exposure input in baseline condition ${\rm E}[-\log_2 p(category | VOT, f0, vowel\ duration)] =$ `r d.exposure_surprisal %>% filter(Condition.Exposure == "Shift0", IO.Type == "prior") %>% pull(surprisal_mean) %>% round(., 2)` bits; +10 condition = `r d.exposure_surprisal %>% filter(Condition.Exposure == "Shift10", IO.Type == "prior") %>% pull(surprisal_mean) %>% round(., 2)` bits; +40 condition = `r d.exposure_surprisal %>% filter(Condition.Exposure == "Shift40", IO.Type == "prior") %>% pull(surprisal_mean) %>% round(., 2)` bits). As listeners converge towards the distribution of /d/ and /t/ in the exposure condition, they should experience increasingly smaller prediction errors processing the exposure tokens. An idealized learner that has fully converged against the exposure distributions (colored lines in Figure \@ref(fig:plot-fit-PSE)C) would, on average, experience only `r d.exposure_surprisal %>% filter(Condition.Exposure == IO.Type) %>% pull(surprisal_mean) %>% mean(.) %>% round(., 2)` bits of surprisal per exposure input. 

Models of adaptive speech perception that predict adaptation to be a positive monotonic function of the prediction error, thus offer an explanation for the the diminishing returns of exposure observed in the present study. Of note, they do so without introducing arbitrary changes in learning rates or other parameters: it is the decrease in additional information that is gained from additional exposure token that drives the decreasing rate of change in listeners' behavior.^[This also highlights that the observable behavior---e.g., changes in listeners' categorization responses---ought not to be confused with the mechanism itself. For example, a *lack* of changes in behavior does not *necessarily* imply a lack of learning (even beyond the usual caveats that apply to null effects). This would only follow if it is also shown that learning would be expected to lead to changes in behavior. But this would require a learning model, a linking hypothesis, and their application to the exposure input---something that is usually lacking from the interpretation of null effects [cf. @eisner2013; @zheng-samuel2020].] If the pattern of diminishing returns is replicated in future work, this would raise questions as to whether similar predictions follow from other distributional learning accounts [e.g., C-CuRE normalization, @mcmurray-jongman2011; exemplar models, @johnson1997; DNNs, @magnuson2020]. If, on the other hand, future tests fail to replicate these findings, this would be a serious problem for models that predict adaptation to be sensitive to the prediction error.


## Constraints on the early moments of adaptive speech perception?
At first blush, it would be tempting to interpret the 'diminishing returns' as evidence that listeners have converged against the exposure distributions in the input---i.e., that adaptation has successfully completed. In particular, we found that there was at best anecdotal evidence that the final exposure block had *any* additional effect (cf. hypothesis tests in Table \@ref(tab:hypothesis-table-simple-effects-block)). However, the comparison against idealized learners revealed that listeners had *not* actually successfully learned the phonetic distributions of the unfamiliar talker: compared to PSEs predicted for idealized learners that have fully learned the exposure distributions, listeners' exhibited substantially smaller shifts in PSEs. We will refer to this as 'premature convergence', as it would appear to entail a deviation from optimal learning. Furthermore, we found suggestive evidence that the degree to which participants converged against the exposure distributions decreased with increasing magnitude of the shift in those distributions (relative to listeners' prior expectations). To distinguish it from the first finding, we will refer to this as 'shrinkage': there could be premature convergence without shrinkage. Finally, we found evidence that shrinkage was *asymmetric*: leftward shifts in the exposure distributions along the VOT continuum were associated with stronger shrinkage than rightward shifts.

If confirmed by additional data, both shrinkage and premature convergence would constitute potentially severe constraints on the adaptivity of speech perception---at least during the initial moments of exposure before additional memory and learning mechanisms are engaged during, e.g., sleep [for discussion, see @REFS-MORE; @xie2018sleep]. The two constraints would also introduce novel data points that theories of speech perception need to account for: while existing theoretical frameworks can accommodate both constraints, either constraint would substantially narrow the space of plausible models. Existing *models* of adaptive speech perception, for example, do not predict these effects as far as we can tell. Given the potential high relevance of these findings, we review existing evidence for shrinkage and premature convergence, and present an additional post-hoc test to further characterize changes in participants' categorization functions. We then discuss potential explanations.

Two previous studies have reported results that seem to exhibit shrinkage. @kleinschmidt-jaeger2016 exposed five different groups of listeners to VOT distributions for /b/ and /p/ that were shifted to different degrees. The five different exposure conditions were each shifted by 10msecs in VOT relative to the other, but held constant the distance between the /b/ and /p/ mean (always 40ms) and the variance of /b/ and /p/ (both always 8.3ms$^2$). All listeners were exposed to 222 trials of exposure input. Unlike the present study, @kleinschmidt-jaeger2016 did not include a pre-test or incremental intermittent testing. Instead, the effect of exposure was estimated by estimating listeners' categorization functions over the last third of the 222 trials. This revealed adaptation patterns that resemble those of the present study in several ways. First, the further the exposure distributions were shifted relative to the distributions expected from a 'typical' talker, the more listeners' categorization function was shifted in the same direction. Second, an unpublished follow-up analysis of the same data [@kleinschmidt2020] found that larger shifts in the exposure distribution yielded shifts in listeners' categorization functions that were *proportionally smaller* relative to the shifts expected from an idealized learner (shrinkage). Third and finally, the same follow-up analysis found that shrinkage effects were more pronounced for leftward- than rightward-shifted exposure distribution (asymmetric shrinkage).^[@kleinschmidt2020 also presents additional experiments with extreme examples of leftward-shifted exposures. In his Experiment 4, different groups of listeners were exposed to /b/-/p/ distributions for which the /b/ mean was shifted to -20ms, -50ms, or even -80ms, while the /p/ mean remained at 50ms VOT. This experiment replicated the strong shrinkage effects for such leftward shifts: even in the most extreme condition (-80ms), listeners' average PSE was still larger than 20ms. These results should, however, be interpreted with caution since the experiment returned small shifts even for rightward shifts---possibly due to data quality issues with Mechanical Turk.] <!-- TO DO: could put shrinkage figure here that is mentioned as possible addition in the to do. But could wait until submission? --> The present study replicates these findings in an incremental exposure-test paradigm, and for naturalistic stimuli with ecologically more valid cue distributions.

<!-- Unlike the present study, @kleinschmidt-jaeger2016 and @kleinschmidt2020 used synthesized stimuli that sounded robotic, and were not designed to exhibit natural correlations between VOT, f0, and vowel duration. The distribution that listeners were exposed were *not* randomly sampled but rather were perfectly symmetric around the category means, and had identical variance for /b/ and /p/. This follows other DL studies [e.g., @clayards2008; @theodore-monto2019] but deviates from the distributions listeners are familiar with from natural speech. The fact that the present study resulted in 'shrinkage' effects similar to those found in @kleinschmidt2020 suggests that these effects are not an artifact of the lower ecological validity of the stimuli used in previous studies. -->
 
```{r fit-IA-inferred-VOT-f0}
# Make a data frame that splits the entire data in unique exposure-test combinations.
# This data frame will be used for adaptive changes in normalization and category
# representations.
d_for_ASP <-
  d.for_analysis %>%
  ungroup() %>%
  # Exclude catch trials and the final two test blocks since we expect unlearning during those blocks
  # (which is not modeled)
  filter(
    !(Phase == "exposure" & is.na(Item.ExpectedResponse.Voicing)),
    as.numeric(Block) <= 7) %>%
  # Use F0 as intended by generation script. This makes fitting more computationally feasible
  # (336 instead of 1001 unique combinations of group & test locations), and also removes the
  # potential that annotation / measurement error perturb the f0 values. It does, however,
  # make the assumption that f0 is the same across the three minimal pairs. While likely wrong,
  # this assumption better aligns with the fact that the model doesn't have a way to keep track
  # of any lexical, phonological, or phonetic context effects on VOT (and thus no way to predict
  # any potential differences between minimal pairs).
  # rename(VOT = Item.VOT, f0_Mel = Item.F0_target_for_generation_script) %>%
  # Create new condition variable that uniquely identifies what participants have seen at any of the
  # tests and a new block variable that identifies the type and order of blocks. Also create two
  # convenience variables, x (stimulus) and Response.Category (to make matching with the category
  # column of ideal observers/adaptors more straightforward).
  mutate(
    Condition.for_ASP = paste0(Condition.Exposure, List.ExposureBlockOrder, List.ExposureMaterials),
    Block.for_ASP = paste0(Phase, Block),
    x = map2(VOT, f0_Mel, ~ c(.x, .y)),
    Response.Category = factor(ifelse(Response.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/")),
    Item.ExpectedResponse.Category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/"))) %>%
  slice_into_unique_exposure_test(
    condition_var = "Condition.for_ASP",
    block_var = "Block.for_ASP",
    block_order = c("test1", "exposure2", "test3", "exposure4", "test5", "exposure6", "test7")) %>%
  mutate(ExposureGroup = factor(ExposureGroup)) %>%
  select(
    ExposureGroup, Phase, ParticipantID, Block, Trial, x,
    VOT, f0_Mel, vowel_duration, Response.Category, Item.ExpectedResponse.Category)

# fit ideal adaptor with uninformative priors
m_IA_inferred.VOT_f0_vowelduration <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = NULL,
    Sigma_0 = NULL,
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_uninformative priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))
```

```{r summarise-PSE-predictions-of-IA}
groups <- get_group_levels_from_stanfit(m_IA_inferred.VOT_f0_vowelduration)
# get logistic model predictions of the IA's categorisation
# then prepare the data for plotting
d.IA_predicted_PSE <-
  if (file.exists("../models/d.IA_predicted_PSE.rds")) {
    readRDS("../models/d.IA_predicted_PSE.rds") 
    } else {
    get_IBBU_predicted_response(m_IA_inferred.VOT_f0_vowelduration, groups = groups) %>%
      fit_logistic_regression_to_model_categorization() %>% 
      group_by(group) %>%
      median_hdci(PSE) %>%
      # duplicate the no-exposure row; 1 for each condition
      mutate(rows = ifelse(group == "no exposure", 3, 1)) %>%
      uncount(rows) %>%
      mutate(
        Condition.Exposure = factor(c(rep("Shift0", 3), rep("Shift10", 3), rep("Shift40", 3), c("Shift0", "Shift10", "Shift40"))),
        Block = ifelse(group != "no exposure", str_replace(group, ".*(\\d)", "\\1"), group),
        Block = factor(ifelse(group == "no exposure", 1, Block))) %>%
      add_block_labels()
    
    saveRDS(d.IA_predicted_PSE, "../models/d.IA_predicted_PSE.rds")
  }
```

(ref:plot-IA-human-PSE) Comparison of shifts in listener PSEs over the first four test blocks against that expected by an ideal adaptor [a distributional learning model that describes ideal information integration, @kleinschmidt-jaeger2015]. Point ranges and dashed horizontal lines are identical to those in Figure \@ref(fig:plot-fit-PSE). Colored ribbons indicate the 95%-CI for the PSEs predicted by the ideal adaptor fit to listeners' responses. The vertical bar to the right of the panel indicates the range of talker-specific PSEs an idealized listener might have experienced in previous exposure: the mean and 2.5%-to-97.5% quantile range of talker-specific PSEs derived from Bayesian ideal observers fit to all talkers in the phonetic database of isolated word productions presented in Figure \@ref(fig:exposure-means-database-matrix-plot). We return to this range when discussing possible explanations for mismatches between the ideal adaptor and participants' responses. <!-- TO DO: remove redundancy between this code and Figure 4c code. It's bad practice to simply copy and paste code. (re: reproducibility) -->

```{r plot-IA-human-PSE, fig.width=6.5, fig.height=3.5, fig.cap="(ref:plot-IA-human-PSE)", fig.pos="ht"}
# Compute distribution of talker-specific PSEs from Chodroff & Wilson (2018) database against which to compare human behavior
d.talkerPSEs <-
  # get each talker's boundary estimate
  get_IO_categorization(
    data = d.chodroff_wilson.isolated,
    cues = c("VOT"),
    groups = c("Talker", "gender"),
    with_noise = TRUE,
    VOTs = seq(0, 85, .5)) %>% 
  # center talker PSEs to human PSE at block 1 of experiment
  mutate(
    center_constant = mean(d.true_shift$PSE_block1) - mean(PSE),
    PSE_centered = PSE + center_constant) %>%
  summarise(
    lower_PSE = quantile(PSE_centered, probs = .025),
    mean_PSE = mean(PSE_centered),
    upper_PSE = quantile(PSE_centered, probs = .975))

d.estimates %>%
  filter(Block %in% c(1:7)) %>%
  ggplot(
    aes(
      x = Block.plot_label, y = PSE,
      ymin = PSE.lower, ymax = PSE.upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2)),
      xmax = c(seq(1.5, 7.5, 2))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = position_dodge(.3), size = 1.8) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  stat_summary(geom = "line", position = position_dodge(.3)) +
  geom_ribbon(
    data = d.IA_predicted_PSE,
    mapping = aes(x = Block.plot_label, ymin = .lower, ymax = .upper, fill = Condition.Exposure, group = Condition.Exposure),
    alpha = .1,
    position = position_dodge(.3),
    inherit.aes = F) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  geom_hline(
    data = d.io.categorization %>% filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")),
    mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4) +
  geom_text(
    data = d.true_shift %>%
      filter(Phase == "exposure") %>%
      group_by(Condition.Exposure, true_shift) %>%
      summarise(),
    mapping = aes(
      x = 8, y = c(25, 35, 65),
      label = paste(round(true_shift), "(100%)"),
      colour = Condition.Exposure),
    colour = colours.condition,
    fontface = "bold",
    size = 2.7,
    inherit.aes = F) +
  annotate(
    "crossbar",
    x = 7.7, y = d.talkerPSEs$mean_PSE, ymin = d.talkerPSEs$lower_PSE, ymax = d.talkerPSEs$upper_PSE,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  scale_x_discrete("Block") +
  scale_y_continuous("PSE (ms VOT)") +
  scale_colour_manual(
    "Condition",
    labels = c("baseline", "+10ms", "+40ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  coord_cartesian(xlim = c(0.75, 7), clip = "off") +
  theme(
    axis.text.x = element_text(angle = 22.5, hjust = 1),
    plot.margin = unit(c(1, 3.2, 1, 1), "lines"),
    legend.position = "none")
```

Unlike for shrinkage, we are not aware of previous evidence for premature convergence. This is largely because the detection of such a pattern requires an incremental exposure-test paradigm, in order to detect a premature 'flattening off' of adaptivity, and such paradigms have been lacking. We therefore conducted an additional test to further quantify whether changes in participants' categorization functions were indeed flatter than one would expect under ideal information integration. To this end, we fit a distributional learning model to listeners' data. This allows us to compare changes in listeners' behavior to those expected under an ideal---as opposed to ideal*ized*---learner.^[Following @qian2016, we use the term *idealized* learner for models that are given privileged access to information that listeners have to infer (e.g., the correct exposure statistics of the experiment), and the term *ideal* learner for models that implement a theory of ideal information integration [for a list of assumptions made by the ideal adaptor, see appendix of @kleinschmidt-jaeger2015].] Specifically, we used an ideal adaptor model [@kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016]. <!-- TO DO: fill in --> Univariate instances of this model have previously achieved high accuracy in modeling changes in listeners categorization functions after LGPL/VGPL [e.g., for /s/-/`r linguisticsdown::cond_cmpl("ʃ")`/, @cummings-theodore2023; /b/-/d/, @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012] or DL exposure [e.g., for /b/-/p/, @kleinschmidt-jaeger2016; @kleinschmidt2020; /g/-/k/, @theodore-monto2019]. We extended the model to multivariate categories over VOT, f0, and vowel duration, and fit it to listeners' responses in our experiment (for details, see SI \@ref(sec:XXX)). <!-- TO DO: fill in more info here. Perhaps we should take a model that uses, as priors, the mu_0 and Sigma_0 from the gray ribbon in the result figure --> Unlike the psychometric model we used for analysis, a distributional learning model specifies how information is integrated across exposure trials. This substantially decreases the degrees of freedom the model has to fit listeners' responses. For example, the ideal adaptor we fit to listeners' responses in the first four test blocks has 21 parameters, <!-- TO DO: if we end up using a model with prior mu, Sigma, this reduces to 3 DFs! --> compared to 25 population-level ('fixed') and 337 group-level ('random') effect parameters for the psychometric mixed-effect model fit to the same data. <!-- TO DO: substitute with inline r-expression to make clearer where these numbers come from --> In the ideal adaptor, unlike for the psychometric model, listeners' responses during different test blocks and across different exposure conditions jointly constrain the parameters of model (due to assumption of ideal information integration within and across exposure blocks). 

For the present data, the ideal adaptor model predicts the proportion of listeners' "t"-responses with an $R^2 = .XXX$, compared to $R^2 = .XXX$ for the psychometric model. <!-- TO DO: fill in--> Comparison of the two models found that XXX. <!-- TO DO: get LOOICs for both models and compare. --> This suggests that the ideal adaptor model provides a decent explanation for participants' behavior with substantially fewer degrees of freedom. We leave further analysis of this model to future work. Here we focus on what this model can tell us about the incremental build-up of adaptation.

Despite the decent quantitative fit, the ideal adaptor exhibits a systematic deficiency in explaining participants' responses.  Figure \@ref(fig:plot-IA-human-PSE) shows the PSEs inferred from the ideal adaptor, along with listeners' PSEs (the same data as in Figure \@ref(fig:plot-fit-PSE)C but focused on blocks up to Test 4). Notably, the ideal adaptor *under*-predicts changes in listeners' PSEs in early blocks---a tendency that is most evident for the exposure conditions with more extreme shifts (baseline and +40). Similarly, though less obvious, the ideal adaptor *over*-predicts listeners' PSE in later blocks (see the baseline and +40 conditions in Test 4). To understand this pattern, it is helpful to recognize that rational models of distributional learning describe listeners' expectations after exposure as the weighted mean of prior expectations and the observed exposure statistics [@kleinschmidt-jaeger2015]. This means that the speed with which a learner adapts at the start of exposure is expected to be predictive of how fast a learner adapts after additional exposure. Put differently, ideal adaptors constrain the rate at which changes in PSEs can change across exposure trials. This means that an ideal adaptor can *either* explain the fact that listeners' PSEs changed rapidly at the start of exposure *or* explain the fact that there seem to be little to no changes in listeners' PSE after the second exposure block. But it cannot explain both aspects of our data. It thus appears that participants indeed converged prematurely in their behavior, relative to what would be expected under the specific model of ideal information integration that we employed.

Next, we discuss possible explanations for such premature convergence and for shrinkage. We do, however, emphasize again that additional studies are necessary to confirm each of the two constraints, and to better characterize their properties. We return to this under future directions.




```{r compute-IO-human-accuracy}
# get io accuracy on shift conditions they were trained on
# prior io gets no training so prior-io-accuracy will be evaluated on all three conditions
io.accuracy <- 
  io %>% 
  select(-c(fold, x)) %>% 
  filter(., Condition.Exposure == "prior") %>% 
      # make 3 rows of prior io condition then join each prior with one of the three exposure conditions
      mutate(rows = 3) %>% 
      uncount(rows) %>% 
      mutate(match = c("Shift0", "Shift10", "Shift40"), 
             Condition.Exposure = str_c(Condition.Exposure, match)) %>% 
      right_join(d.for_analysis %>% 
                   filter(Phase == "exposure") %>% 
                   nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
                   group_by(Condition.Exposure) %>% 
                   # get 1 set of exposure trials per condition
                   slice_sample(n = 1) %>% 
                   unnest(data) %>% 
                   reframe(Item.VOT, Item.f0_Mel, vowel_duration, category) %>% 
                   rename(VOT = Item.VOT, f0_Mel = Item.f0_Mel, intended.category = category) %>% 
                   # nest the cues into one column
                   mutate(x = pmap(list(VOT, f0_Mel, vowel_duration), ~ c(..1, ..2, ..3)),
                          # create a temporary variable to enable the join between prior io and each set of exposure from each condition
                          match = Condition.Exposure) %>% 
                   select(c(match, intended.category, x)) %>%  
                   nest(x = c(x, intended.category))) %>% 
      select(-match) %>% 
  # get IO categorisation with both decision rules
  mutate(
    categorization.proportional = 
      map2(x, io, ~ 
             get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% 
             filter(category == "/t/")),
    categorization.criterion = 
      map2(x, io, ~ 
             get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "criterion") %>% 
             filter(category == "/t/"))) %>% 
  unnest(c(3:5),
         names_sep = "_") %>% 
  select(!ends_with(c("observationID", "_x", "_category"))) %>% 
  # for all items that are not intended /t/ take 1 minus the posterior of /t/
  mutate(categorization.proportional_response = ifelse(x_intended.category == "/t/", categorization.proportional_response, 1 - categorization.proportional_response),
         # where criterion rule responds 1 that is a /t/
         categorization.criterion_response = ifelse(categorization.criterion_response == 1, "/t/", "/d/"),
         categorization.criterion_response = ifelse(categorization.criterion_response == x_intended.category, T, F)) %>% 
  group_by(Condition.Exposure) %>% 
  summarise(across(c(categorization.proportional_response, categorization.criterion_response), mean)) %>% 
  bind_rows(
    io %>% 
  select(-c(fold, x)) %>% 
  filter(Condition.Exposure != "prior") %>% 
  right_join(d.for_analysis %>% 
               filter(Phase == "exposure") %>% 
               nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
               group_by(Condition.Exposure) %>% 
               # get 1 set of exposure trials per condition
               slice_sample(n = 1) %>% 
               unnest(data) %>% 
               reframe(Item.VOT, category) %>% 
               rename(x = Item.VOT, intended.category = category) %>% 
               nest(x = c(x, intended.category))) %>% 
  mutate(
    categorization.proportional = map2(x, io, ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>% filter(category == "/t/")),
    categorization.criterion = map2(x, io, ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "criterion") %>% filter(category == "/t/"))) %>%
  unnest(c(3:5),
         names_sep = "_") %>% 
  select(!ends_with(c("observationID", "_x", "_category"))) %>% 
  # for all items that are not intended /t/ take 1 minus the posterior of /t/
  mutate(categorization.proportional_response = ifelse(x_intended.category == "/t/", categorization.proportional_response, 1 - categorization.proportional_response),
         # where criterion rule responds 1 that is a /t/
         categorization.criterion_response = ifelse(categorization.criterion_response == 1, "/t/", "/d/"),
         categorization.criterion_response = ifelse(categorization.criterion_response == x_intended.category, T, F)) %>% 
  group_by(Condition.Exposure) %>% 
  summarise(across(c(categorization.proportional_response, categorization.criterion_response), mean))) %>% 
  rename(Condition.io = Condition.Exposure) %>% 
  mutate(Condition.Exposure = factor(rep(c("Shift0", "Shift10", "Shift40"), 2))) %>% 
  pivot_longer(cols = starts_with("categorization"),
               names_to = "decision",
               values_to = "accuracy") %>% 
  mutate(Condition.io = ifelse(str_detect(Condition.io, "prior"), "prior", Condition.io),
         decision = str_replace(decision, "categorization\\.(.*)_response", "\\1"))

# get predictions of the fitted psychometric model
fit_exposure <- readRDS("../models/exposure-standard-priorSD15-0.999.rds")
fit_test <- readRDS("../models/test-standard-priorSD15-0.995.rds")
psych.model_accuracy <- 
  get_pyschometric_accuracy(fit_exposure, d.for_analysis, "exposure", VOT.sd_exposure) %>% 
  bind_rows(
    get_pyschometric_accuracy(fit_test, d.for_analysis, "test", VOT.sd_test)) %>% 
  add_block_labels()

# get human accuracy on unlabelled exposure trials
human.accuracy <- 
  d.for_analysis %>%
  filter(Phase == "exposure" & Item.Labeled == F) %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  summarise(participant.accuracy = mean(Response.Correct),
            mean.accuracy = mean(participant.accuracy)) %>% 
  group_by(Condition.Exposure, Block) %>% 
  summarise(lower = quantile(mean.accuracy, probs = .025),
            median = quantile(mean.accuracy, probs = .5),
            upper = quantile(mean.accuracy, probs = .975))
rm(fit_exposure, fit_test)
```

(ref:plot-IO-human-accuracy) Changes across blocks and conditions in participants' accuracy, as estimated from the psychometric mixed-effects model fit to participants' responses. Accuracies are shown for two different decision rules: Luce's choice rule (responding proportional to posterior probability of category) and the criterion choice rule (always responding the category that has higher posterior probability). As in Figure \@ref(fig:plot-fit-PSE)C, point ranges represent the posterior medians and their 95% CI derived from the psychometric model. Horizontal dashed lines indicate accuracy expected from an idealized learner (an ideal observer model that has fully learned the exposure distributions) and horizontal shaded ribbons indicate the 95%-CI expected from an idealized pre-exposure listener.

```{r plot-io-human-accuracy, fig.width=7.5, fig.height=5.5, fig.cap="(ref:plot-IO-human-accuracy)"}
human_io_accuracy <- 
  human.accuracy %>% 
  mutate(decision = "human") %>% 
  bind_rows(
    crossing(
      Condition.Exposure = c("Shift0", "Shift10", "Shift40"), 
      Block = c(1, 3, 5, 7),
      lower = NA,
      median = NA,
      upper = NA, 
      decision = "human")) %>% 
  add_block_labels() %>% 
  full_join(psych.model_accuracy) %>% 
  group_by(decision, Condition.Exposure) %>% 
  arrange(Block, .by_group = T) %>% 
  ungroup() %>% 
  mutate(top_panel = ifelse(decision == "proportional" | decision == "human", "Luce's choice rule", NA),
         bottom_panel = ifelse(decision == "criterion" | decision == "human", "Criterion choice rule", NA),
         decision = fct_relevel(decision, c("human", "proportional", "criterion")))

p1 <- human_io_accuracy %>% 
  filter(decision == "proportional") %>% 
  ggplot(aes(x = Block.plot_label, y = median, 
             ymin = lower, ymax = upper, 
             colour = Condition.Exposure, 
             shape = decision, 
             group = decision)) +
  geom_hline(
    data = io.accuracy %>% 
      filter(decision == "proportional" & Condition.io != "prior"),
    mapping = aes(yintercept = accuracy, color = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4,
    show.legend = F) +
  geom_hline(
    data = io.accuracy %>% 
      filter(decision == "proportional" & Condition.io == "prior"),
    mapping = aes(yintercept = accuracy),
    linewidth = .8,
    colour = "#d2d4dc",
    alpha = 0.9,
    inherit.aes = F) +
  geom_pointrange(position = position_dodge2(.4), size = .4) +
  stat_summary(geom = "line", position = position_dodge2(.5)) +
   labs(y = "Accuracy on exposure trials\n(of psychometric model)") +
  scale_colour_manual("Condition", values = colours.condition, labels = c("baseline", "+10", "+40"), guide = F) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) + 
  coord_cartesian(ylim = c(.5, 1)) +
  guides(shape = "none") +
  facet_grid(top_panel ~ Condition.Exposure, 
             labeller = labeller(Condition.Exposure = c("Shift0" = "baseline", "Shift10" = "+10", "Shift40" = "+40")))

p2 <- human_io_accuracy %>% 
  filter(decision == "criterion") %>% 
  ggplot(aes(x = Block.plot_label, y = median, 
             ymin = lower, ymax = upper, 
             colour = Condition.Exposure, 
             group = decision)) +
  geom_hline(
    data = io.accuracy %>% 
      filter(decision == "criterion" & Condition.io != "prior"),
    mapping = aes(yintercept = accuracy, color = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4,
    show.legend = F) +
  geom_hline(
    data = io.accuracy %>% 
      filter(decision == "criterion" & Condition.io == "prior"),
    mapping = aes(yintercept = accuracy),
    linewidth = .8,
    colour = "#d2d4dc",
    alpha = 0.9) +
  geom_pointrange(position = position_dodge2(.4), size = .4) +
  stat_summary(geom = "line", position = position_dodge2(.5)) +
   labs(y = "Accuracy on exposure trials\n(of psychometric model)") +
  scale_colour_manual("Condition", values = colours.condition, labels = c("baseline", "+10", "+40"), guide = F) +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 30, vjust = .99, hjust = .99)) + 
  coord_cartesian(ylim = c(.5, 1)) +
  guides(shape = "none") +
  facet_grid(bottom_panel ~ Condition.Exposure, 
             labeller = labeller(Condition.Exposure = c("Shift0" = "baseline", "Shift10" = "+10", "Shift40" = "+40")))
(p1 / p2) 
```

```{r testing-shrinkage, results='asis'}
fit_test.simple_effects_condition <- readRDS("../models/test-nested-condition.rds")
fit_test_nested_within_condition_and_block <- readRDS("../models/test-nested_slope-priorSD15-0.995.rds")
hyp.shrinkage <-
  hypothesis(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(", d.io.categorization$intercept_scaled[[79]], "- mu2_IpasteCondition.ExposureBlocksepEQxShift40x7) <", "abs(", d.io.categorization$intercept_scaled[[70]], "- mu2_IpasteCondition.ExposureBlocksepEQxShift10x7)"),       str_c("abs(", d.io.categorization$intercept_scaled[[70]], "- mu2_IpasteCondition.ExposureBlocksepEQxShift10x7) <", "abs(", d.io.categorization$intercept_scaled[[61]], "- mu2_IpasteCondition.ExposureBlocksepEQxShift0x7)")),
    robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

make_hyp_table(
    hyp.shrinkage,
    c(
      "$PSE_{+40_{predicted}} vs. PSE_{+40} < PSE_{+10_{predicted}} vs. PSE_{+10}$",
      "$PSE_{+10_{predicted}} vs. PSE_{+10} < PSE_{baseline_{predicted}} vs. PSE_{baseline}$"),
    caption = "Is the distance of the fitted PSE from the idealised learner predicted PSE different between the conditions? This table tests the hypothesis of shrinkage, i.e. the further the distributional shift from listener prior expectations, the greater the distance from the expected boundary.")
```



### Potential explanations
Explanations for the observed patterns of shrinkage and premature convergence can be grouped into three classes. One class of explanations appeals to methodological artifacts. A second class of explanations attributes the observed patterns to a failure to take into account aspects of our design. Under either of the first two types of explanations, the present data do *not* constitute evidence against the hypothesis that adaptive speech perception achieves ideal information integration. A third class of explanations appeals to previously unrecognized constraints on adaptive speech perception. 

...

One explanation of both shrinkage and premature convergence appeals to the priors we used in our psychometric model. Following standards in the literature, we employed a weakly regularizing priors Student $t$ prior for all population-level effects. This prior favors small coefficient estimates, regularizing the estimated shifts in PSEs towards zero. As this regularization is particularly strong for more extreme shifts in the PSE, it is theoretically possible that our priors caused both shrinkage and premature convergence [the same holds for the shrinkage estimates reported in @kleinschmidt2020, which were derived from Bayesian mixed-effect logistic regression with similar priors]. This would make these findings artifacts of our data analysis approach, rather than findings of theoretical interest. 

Two considerations make this an unlikely explanation. First, as the Student $t$ prior is symmetric around zero, it cannot explain *asymmetric* shrinkage. Second, even the largest estimated shifts were well within the 95% highest posterior density interval of the prior. This suggests that the prior was not strong enough to cause the size of the observed shrinkage, not to mention the appearance of premature convergence. Still, we decided to address this possibility more directly. We refit the psychometric model once with a substantially weaker prior (SD of Student $t$ = 5) and once with an even weaker uniform prior. In both cases, the shrinkage pattern in participants' PSEs remained qualitatively unchanged. <!-- TO DO: is this what you did? Let me know. -->

Alternative explanations appeal to convenience assumptions we made in fitting the ideal adaptor model, and how these assumption mismatch information that is available to listeners [for a detailed list of assumptions, see appendix of @kleinschmidt-jaeger2015]. Under these types of explanations, shrinkage and premature convergence do *not* constitute new constraints on adaptivity but rather reflect unconstrained learning. For example, unlike the ideal adaptor model in Figure \@ref(fig:plot-IA-human-PSE), listeners might learn from the unlabeled test trials. The fact that we observed changes in participants' categorization function from Test 4-6, without intermittent exposure blocks, suggests as much. Critically, test tokens had by design identical phonetic properties across all exposure conditions. Inevitably then, their location relative to the exposure tokens *differed* between conditions. If listeners integrate test tokens into their estimate of the talker's accent, this might explain the observed shrinkage and premature convergence: at the end of Test 4, 36 out of 180 trials (20%) that participants had experienced were test tokens.^[If listeners adapt over a moving time-window (rather than over all inputs from a talker), or in other ways weight more recent information more strongly, this would further increase the relative implact of test tokens on listeners' categorization responses during test.] While it is difficult to assess this explanation without a specific model of how listener learn from unlabeled tokens, one consideration suggests that it is not sufficient to explain our data. To estimate how much learning test tokens might elicit in the different conditions, we calculated the surprisal of the test token under the idealized learners. For an idealized learner that has *fully* learned the exposure distribution (cf. colored dashed lines in Figure \@ref(fig:plot-fit-PSE)C), test stimuli would convey about the same amount of surprisal in the baseline and +10 conditions (${\rm E}[-\log_2 p(VOT | {\rm idealized\ learner})] =$ `r d.test_surprisal %>% filter(Condition.Exposure == "Shift10") %>% pull(surprisal_mean) %>% round(., 1)` bits), compared to larger surprisal in the + 40 condition (`r d.test_surprisal %>% filter(Condition.Exposure == "Shift40") %>% pull(surprisal_mean) %>% round(., 1)` bits). At least based on these general considerations, learning from test tokens alone would thus predict more shrinkage for the +40 condition than for the baseline condition---the opposite of what we observed. Future work can further address this question by developing and applying unsupervised adaptation models to our data [e.g., @harmon2019; @olejarczuk2018; @yan-jaeger2018].

Another possible explanation that appeals to mismatches between listeners and the ideal adaptor model refers to knowledge of cross-category correlations. It is known, for example, that the VOT and f0 means of L1-US English /d/ and /t/ categories are correlated [@REFS-incl-but-not-limited-to-chodroff]. The ideal adaptor model in Figure \@ref(fig:plot-IA-human-PSE) did not model these correlations. It is thus conceivable that prior expectations about these or other cross-category correlations were violated by our exposure distributions, keeping listeners from fully adapting. The present experiment cannot rule out this explanation. We note, however, that correlations between category means alone would seem unlikely to explain our result. First, these correlations are known to be comparatively weak for /d/ and /t/ [@chodroff-wilson2018], thus carrying little information. Second, as far as we can tell, integration of these correlations into the ideal adaptor would be expected to lead to *faster* convergence against the exposure distribution, rather than to shrinkage or premature convergence.

Finally, it is possible that asymmetric shrinkage and/or premature convergence are real, replicable findings that cannot be reduced to methodological artifacts or the most obvious shortcomings of the ideal adaptor model. We discuss three such explanations. The first such explanation is discussed by @kleinschmidt2020: listeners might struggle to extrapolate to phonetic continuum steps that were not observed during exposure. If extrapolation difficulty was to spell out as shrinkage---e.g., because listeners have more uncertainty about how to categorize test tokens further away from the exposure distribution, leading them to upweight prior expectations for such tokens---this might explain *some* of our findings. However, it would fail to fully explain our results, for the same reason that unsupervised learning over the unlabeled test tokens is unlikely to fully explain our findings: while shrinkage was largest in the baseline condition, categorization of test tokens did not require a lot of extrapolation in this condition (see surprisal comparison above, and tickmarks in Figure \@ref(fig::exp2-design-distribution)).

The second explanation refers to the goal of learning, specifically the loss function that learners seek to minimize. In Figure \@ref(fig:changes-in-accuracy), we followed previous work and calculated accuracy based on the assumption of Luce's choice rule [@luce1959]. This is the choice rule implicitly or explicitly assumed in many, perhaps most, models of adaptive speech perception [e.g., the Rescorla-Wagner model in @harmon2019; ideal adaptor, @kleinschmidt-jaeger2015]. 


 + DL might be correct but ideal adaptor *model* as used here might be wrong
  + goal of learning (loss function) is not approximation of distributions (i.e., maximizing likelihood of joint distribution of cues and categories) but e.g., maximization of accuracy under a choice rule.
   + COULD (but maybe not?) discuss potential that observed adaptation maximizes accuracy under the choice rule. use psychometric function fit during unlabeled exposure trials to calculate *accuracy* (not likelihood) on labeled trials under criterion and under proportional matching decision rules. compare against accuracy if ideal observers categorization functions are used instead.
  + limitations in incremental adaptation (that might potentially be overcome with longer-term exposure, cf. discussion in xie2023): e.g., reweighting mixture of previously learned talker-models (can't go beyond previous experience). @estes1986 (discussed in @massaro-friedman1990) hypothesized "Perhaps people use exemplar-based categorization early in learning before a reliable summary description is developed" (this is quoting @massaro-friedman1990, not estes)



That listeners in the +40 conditioned moved their boundaries more freely corresponds to the distribution of /t/ tokens in encountered speech. Figure \@ref(fig:exposure-means-database-matrix-plot) shows the widely distributed VOT values for /t/ across two different speech styles indicating that listeners are likely to have encountered a wide range of /t/ tokens in their lifetime. In contrast, the distribution of /d/ tokens is much more narrow in variability. The asymmetry in the distribution of /t/ and /d/ tokens in the input is likely to have resulted in listeners having stronger constraints on movements of the left-shifted conditions since adapting would mean interpreting more /d/s as /t/s under prior expectations that /d/s are much more tightly clustered in the cue space. Furthermore, as the voiced category is shifted further leftwards more negative VOT tokens will be included in exposure. Pre-voicing in L1-US English while quite common, is not a primary cue for identifying the voiced category. While it could reinforce perception of a voiced category research on pre-voicing effects in true-voicing languages suggest it is unlikely to elicit gradient responses in the perception of the voiced category [@vanalphen-mcqueen2006; @vanalphen-smits2004; @clare-schertz2022]. This means that for the voiced category as VOT gets smaller and moves into the negative region there is less information to be gained from attending to the changes. This flip-side to the effect on perception as VOT increases makes sense because there is not a category to the left of the voiced one in a dual-contrast language (it's /d/s all the way down).


## Limitations and future directions
The present study set out to investigate incremental adaptation to a single talker, whose pronunciations were shifted relative to listeners' prior expectations. The conclusions we have discussed so far should be interpreted in light of several  limitations.

First off, our experiment investigated incremental adaptation to a single female talker's productions of syllable-initial /d/-/t/ by L1-US English listeners. In particular, our exposure conditions shifted the distribution of VOT and, by extension, f0 and vowel duration. As adaptive speech perception---including its generalization across categories, phonetic contexts, and talkers---can depend on the exposure talker, the phonetic contrast or the phonetic cues it involves [e.g., @eisner-mcqueen2005; @kraljic-samuel2007; @mitterer; @xie2017; @xie2021jep], future work is necessary to assess how general the present findings are.

Second, the present study shares with other distributional learning paradigms that a small number of minimal pair items was repeated many times, with only minimal phonetic differences embedded in otherwise constant phonetic contexts (e.g., the vowel following /d/-/t/ was always the same), and presented in isolation. This sacrifice of ecological validity was motivated by our goal to test stronger predictions about the direction and relative magnitude of effects (rather than merely the existence of effects). It does, however, mean that the speech input that participants experienced in the experiment differed from everyday encounters with unfamiliar talkers: listeners typically experience *connected* speech from unfamiliar talkers, which tends to be produced with faster speech rates and comes with additional segmentation challenges; while the same phonetic contrast might appear many times, it will not necessarily appear in the same phonetic context, least of all in the same word; and the speech of talkers with unfamiliar accents often deviate from listeners' expectations in more than a single segmental contrast. Comparatively little is known about adaptive speech perception under such conditions since only a handful of studies has employed connected speech stimuli beyond isolated sentences [e.g., @reinisch2014]. Encouragingly, these studies have replicated the effects found for ecologically less valid exposures. Still, it is possible that, for example, the rapidity of adaptive changes in the present study over-estimates the adaptivity of everyday speech perception. There is evidence, for example, that the repetition of minimal pair recordings can affect listeners categorization responses [@lancia-winter2013], perhaps because it helps listeners isolate relevant differences between recordings.

Third, as already mentioned, some of tests were conducted post-hoc, and thus require replication. In particular, future experiments with longer exposure would provide more decisive tests of premature convergence and (asymmetric) shrinkage. Additional data from future applications of the incremental exposure-test paradigm with different phonetic shifts will also facilitate stronger quantitative tests of models of adaptive speech perception [in the spirit of @guest-martin2021; @yarkoni-westfall2017; @xie2023]. In a recent review of the field, Xie and colleagues demonstrated that the signature findings of some of the most popular paradigms in adaptive speech perception do not distinguish between radically different theoretical accounts: qualitative improvements in speech recognition can be explained by mechanisms ranging from early pre-linguistic perceptual normalization, changes in the representations of phonetic categories, or upstream changes in decision-making. Xie and colleagues concluded that the effective comparisons of these theories will require quantitative data sets that constrain the way in which listeners' categorization behavior changes depending on the amount and nature of the input. The the use of incremental testing and multiple exposure conditions with different phonetic shifts provides such data.

<!-- A second reason for the relative scarcity of research on incremental changes in speech perception might be that repeated testing comes with its own unique challenges. For example, if test tokens are sampled from natural accents---the most common approach in AA research---these tokens can themselves contain information about the target accent, thereby masking exposure effects [for a particularly clear demonstration, see @xie-kurumada2024]. Even for paradigms that employ synthesized stimuli, or otherwise control the informativity of test tokens [e.g., @chodroff-wilson2020], repeated testing can come with challenges. As already discussed, there is now evidence that repeated testing over unlabeled uniform test continua can reduce the effects of exposure [@scharenborg-janse2013; @cummings-theodore2023; @zheng-samuel2023; @liu-jaeger2018; @liu-jaeger2019; @giovannone-theodore2021; @tzeng2021]. This is compatible with some distributional learning accounts of adaptive speech perception [for discussion, see @kleinschmidt-jaeger2015]: if the same learning mechanisms that operate during labeled and unlabeled exposure trials continue to operate during unlabeled test trials, the unexpected uniform distribution over the VOT continuum during test blocks should begin to undo the effects of preceding exposure.^[The specific predictions depend on the---as of yet unknown---way listeners adapt to unlabeled speech inputs [for an initial comparisons of several candidate models for unsupervised adaptation, see @yan-jaeger2018]. Additionally, it is possible that adaptation to unlabeled inputs involves different mechanisms than adaptation to labeled trials.] In the present study, we replicated this effect of repeated testing for the final three test blocks. -->

# Conclusions
Research on adaptive changes in speech perception has made great strides since foundational works in the 90s and 00s. Now that the existence of adaptive changes in speech perception is no longer in question, recent reviews of the field have emphasized the need to develop novel paradigms that can inform the functional relation between exposure inputs and changes in listeners' perception [@xie2023; @OTHERS]. The present study is a response to this call. We set out to more clearly characterize the incremental unfolding of adaptation to changes in the realization of a simple two-way phonetic contrast. This allowed us to assess previously untested predictions of distributional learning accounts of adaptive speech perception. In line with these theories, we found that listeners initially draw on prior experience with other talkers to recognize input from the unfamiliar talker. This comes at the cost of reduced recognition accuracy. With increasing exposure, listeners then adapted their categorization responses, improving recognition accuracy. The incremental unfolding of these changes followed the prediction of distributional learning accounts---in particular, accounts that predict changes in listeners' perception to depend on the prediction error (or the amount of new information) associated with each new exposure input. Finally, we found suggestive evidence that adaptivity, at least during the earliest moments of exposure, is limited: while adaptation was rapid, it also slowed down and seemed to converge against a stable state long before listeners approached the recognition accuracy expected from an idealized learner. The specific nature of these constraints strikes us as an important target for future work, as they potentially impose novel constraints on theories of adaptive speech perception.

The insights we reported were facilitated by both Bayesian psychometric mixed-effects analysis and normative models of adaptive speech perception. Psychometric models have long been used in research on, for example, visual decision-making [e.g., @klein2001; @prins2011; @wichmann-hill2001]. Here, we introduced a mixed-effects version of such a model in order to fit a single model across all participants, while correcting for participant-specific lapse rates and while modeling block-by-block changes in participants' psychometric functions. While such models used to require expensive software, they can now be fit in freely available software [\texttt{R} @R-base; \texttt{rstan}, @R-stan; @R-stanarm; \texttt{brms} @R-brms_a]. Similarly, there are now R libraries that facilitate the fitting and evaluation of ideal observers and adaptors [@beliefupdatr, @MVBeliefUpdatr]. The R markdown available on OSF provides a starting point for other researchers interested in either type of model.
