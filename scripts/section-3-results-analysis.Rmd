```{r load-libraries}
require(tidyverse)
require(magrittr)

require(brms)
require(MVBeliefUpdatr)
require(phonR)
require(rsample)

source("functions.R")
```

## Results

```{r model-fit-test-blocks, include=FALSE}
# Storing some information about the data set submitted for analysis
# (to facilitate unscaling predictions back into the original scales for plotting)
d.mean_sd_scaling <-
  d_for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")
contrast_type <- "difference"

# Simplifying model with uniform bias
fit_mix_test <-
  fit_model(data = d_for_analysis,
          phase = "test",
          formulation = "standard",
          priorSD = 15, 
          adapt_delta = .995)

# fit difference coded model on exposure block
fit_mix_exposure <-
  fit_model(data = d_for_analysis,
          phase = "exposure",
          formulation = "standard",
          priorSD = 15, 
          adapt_delta = .999)
```

We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models, using brms [@R-brms_a] in R [@R; @RStudio, for details, see SI, \@ref(sec:analysis-approach)]. Psychometric models account for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while commonplace in research on speech perception [but see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the present experiment, however, lapse rates were negligible (`r make_CI(fit_mix_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. 

Each psychometric model regressed participants' categorization responses against the full factorial interaction of VOT, exposure condition, and block, while including the maximal random effect structure (see SI, \@ref(sec:analysis-approach). Figure \@ref(fig:plot-fit-slope-PSE) summarizes the results that we describe in more detail next. Panels A and B show participantsâ€™ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panels C and D show the slope and point of subject equality (PSE)---i.e., the point at which participants are equally likely to respond "d" and "t"---of the categorization function across blocks and conditions. These panels facilitate comparison across blocks within each exposure condition. Here we focus on the test blocks, which were identical within and across exposure conditions. Analyses of the exposure blocks are reported in the SI (\@ref(sec:exposure-analyses)), and replicate all effects found in the test blocks. 

We begin by presenting the overall effects, averaging across all test blocks. This part of our analysis matches previous work, which has focused on the overall effect of exposure across the entire experiment ['batch tests', e.g., @clayards2008; @kleinschmidt-jaeger2016; @nixon2016; @theodore-monto2019] and/or during a single post-exposure test block [e.g., @kleinschmidt2020]. Then we turn to the goals of this study---to characterize the incremental changes in participants' categorization responses as a function of exposure and, in particular, to test 1) whether we replicate the sublinear effects of exposure observed in previous work under the ecologically more valid stimuli and distributions employed in the present work, and 2) whether we can begin to distinguish between the predictions of the model learning and selection hypotheses. 

```{r fit-nested-models-for-intercepts-slopes-plot, message=FALSE}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_mix_test_nested <-
  fit_model(data = d_for_analysis,
          phase = "test",
          formulation = "nested_slope",
          priorSD = 15, 
          adapt_delta = .995)
  
fit_mix_exposure_nested <-
  fit_model(data = d_for_analysis,
            phase = "exposure",
            formulation = "nested_slope",
            priorSD = 15, 
            adapt_delta = .999)

# Extract intercepts and slopes from test and exposure model
get_intercepts_and_slopes <-
  . %>%
  gather_draws(`b_mu2_IpasteCondition.ExposureBlocksepEQ.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(.variable = gsub("b_mu2_IpasteCondition.ExposureBlocksepEQxShift(\\d{1,2})x(\\d{1}.*$)", "Shift\\1.\\2", .variable),
         term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept")) %>%
  separate(col = .variable, into = c("Condition.Exposure", "Block"), sep = "\\.") %>%
  mutate(Block = ifelse(str_detect(Block, "VOT"), str_replace(Block, "(\\d{1}):VOT_gs", "\\1"), Block)) %>%
  pivot_wider(names_from = term, values_from = ".value") %>%
  relocate(c(Condition.Exposure, Block, Intercept, slope, .chain, .iteration, .draw))

d.estimates <-
  full_join(
    fit_mix_test_nested %>% get_intercepts_and_slopes(),
    fit_mix_exposure_nested %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    PSE = ifelse(
      Block %in% c(2, 4, 6),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_exposure),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_test))) %>% 
  group_by(Condition.Exposure, Block) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), median = median, upper = ~ quantile(.x, probs = .975)))) 
```



```{r predictions-exposure-conditions-by-IOs}
# Database for prior
d.chodroff_wilson <-
  get_ChodroffWilson_data(
    database_filename = "../data/all_observations_with_non-missing_vot_cog_f0.csv",
    min.n_per_talker_and_stop = 25,
    limits.VOT = c(-Inf, Inf),
    limits.f0 = c(0, 350),
    max.p_for_multimodality = .1) %>%
  filter(poa == "/d/-/t/") %>%
  group_by(Talker, category) %>%
  mutate(n = n()) %>%
  group_by(Talker) %>%
  # subsample n tokens, as determined by category with fewer tokens
  mutate(
    n_min = min(n),
    n_category = n_distinct(category)) %>%
  # select talkers with both /d/ and /t/ observations
  filter(n_category == 2) %>%
  group_by(Talker, category) %>%
  sample_n(size = first(n_min)) %>%
  filter(gender == "female") %>% 
  ungroup() %>% 
  mutate(across(
    c("VOT", "f0", "f0_Mel", "f0_semitones"),
    list(CCuRE = function(x) apply_ccure(x, data = .)),
    .names = "{.col}.CCuRE")) 
```



```{r}
# Make ideal observer based on exposure conditions
make_VOT_IOs_from_exposure <- function(data){ 
  data %>%
    
  # This takes all of the actual data to make ideal observers. Alternatively, one could 
  # first distinct the data to one full list of each condition.
  make_MVG_ideal_observer_from_data(
    group = "Condition.Exposure", 
    cues = c("VOT"), 
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
  # Add prior based on Chodroff & Wilson (2018)
  # (comment out to not express gray reference line below)
  bind_rows(
    d.chodroff_wilson %>%
      make_MVG_ideal_observer_from_data(
        cues = c("VOT"), 
        Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>% 
      mutate(Condition.Exposure = "prior")) %>%
  nest(io = -c(Condition.Exposure))
}  

io <-    
  make_VOT_IOs_from_exposure(
   d_for_analysis %>% 
      filter(Phase == "exposure") %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      slice_sample(n = 1) %>% 
      unnest(data) %>% 
      mutate(
        category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")) %>% 
      rename(VOT = Item.VOT, f0 = Item.F0_Mel)) 

# Estimate the intercept, slope, and PSE that these ideal observers would have 
# were they analyzed the same way the human data is analyzed.
get_logistic_parameters_from_model <- function(
    model, 
    x, 
    model_col = "model", 
    groups = NULL, 
    resolution = 10^12
) {
  f <- if (any(map(model[[model_col]], is.MVG_ideal_observer) %>% unlist())) 
    get_categorization_from_MVG_ideal_observer else
      if (any(map(model[[model_col]], is.NIW_ideal_adaptor) %>% unlist())) 
        get_categorization_from_NIW_ideal_adaptor else stop("Model type not recognized.")
  
  model %>%
    # Cross in test tokens
    left_join(x, by = "Condition.Exposure") %>%
    mutate(x = map(x, ~ c(.x))) %>%
    nest(x = x) %>%
    # Get categorization proportions (turned into counts below)
    mutate(
      categorization =
        map2(x, !! sym(model_col),
             ~ f(
               x = .x$x, model = .y, decision_rule = "proportional") %>%
               mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>%
    unnest(cols = categorization, names_repair = "unique") %>%
    # Prepare data frame for logistic regression 
    pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_d = round(`response_/d/` * .env$resolution), n_t = .env$resolution - n_d) %>%
    group_by(!!! syms(groups)) %>%
    nest() %>%
    # Fit logistic regression and extract relevant information
    # (the regression only uses VOT regardless of what cues are used for the categorization
    # so that this matches the analysis of the human responses)
    mutate(
      model_unscaled = map(data, ~ glm(
        cbind(n_t, n_d) ~ 1 + VOT, 
        family = binomial, 
        data = .x)),
      intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
      slope_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
      model_scaled = map(data, ~ glm(
        cbind(n_t, n_d) ~ 1 + I((VOT - VOT.mean_test) / (2 * VOT.sd_test)), 
        family = binomial, 
        data = .x)),
      intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
      slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
      PSE = -intercept_unscaled/slope_unscaled)
}

# Test points at which responses were measured
d.io.categorization.test <- get_logistic_parameters_from_model(
  io %>% filter(Condition.Exposure != "prior"), 
  d_for_analysis %>%  
    filter(Phase == "test") %>%
    distinct(Condition.Exposure, Item.VOT) %>%
    rename(x = Item.VOT), 
  model_col = "io", 
  groups = "Condition.Exposure") %>% 
  mutate(Phase = "test") %>% 
  select(Condition.Exposure, Phase, intercept_scaled, slope_scaled, PSE) %>% 
  crossing(Block = c(1, 3, 5, 7, 8, 9))

d.io.categorization.exposure <- get_logistic_parameters_from_model(
  io %>% filter(Condition.Exposure != "prior"), 
  d_for_analysis %>%  
    filter(Phase == "exposure", Item.Labeled == F) %>%
    group_by(Condition.Exposure) %>%
    # Since blocks differ in which VOTs are unlabelled, we include all blocks
    # (we are simulating the expected parameters across all exposure blocks)
    filter(ParticipantID == first(ParticipantID)) %>% 
    select(Item.VOT) %>% 
    rename(x = Item.VOT), 
  model_col = "io", 
  groups = "Condition.Exposure") %>% 
  mutate(Phase = "exposure") %>% 
  select(Condition.Exposure, Phase, intercept_scaled, slope_scaled, PSE) %>% 
  crossing(Block = c(2, 4, 6))

d.io.categorization <- 
  rbind(d.io.categorization.exposure, d.io.categorization.test) %>% 
    mutate(across(c(Phase, Condition.Exposure, Block), factor)) 
```



```{r prepare-plot-intercepts-slopes}
p.across_blocks <-
  d.estimates %>% 
  ggplot(aes(x = Block, y = Intercept_median,
             ymin = Intercept_lower, ymax = Intercept_upper,
             colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.55, 8.55, 1)), 
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = position_dodge(.3), size = 1) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  stat_summary(geom = "line", position = position_dodge(.3)) +
  scale_x_discrete("Block", labels = c("1" = "Test 1", "2" = "Exposure 1", "3" = "Test 2", "4" = "Exposure 2", "5" = "Test 3", "6" = "Exposure 3", "7" = "Test 4", "8" = "Test 5", "9" = "Test 6")) +
  scale_colour_manual("Condition",
    labels = c("baseline", "+10ms", "+40ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 22.5, hjust = 1))

p.intercept_1to7 <-
  p.across_blocks +  
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = intercept_scaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .7, 
    alpha = 0.3,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("intercept")

p.slope_1to7 <-
  p.across_blocks +  
  aes(y = slope_median, ymin = slope_lower, ymax = slope_upper) +
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = slope_scaled, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, linewidth = .9, alpha = 0.5,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("slope (log-odds/ms)")

d.true_shift <- 
  d.estimates %>% 
  # join the ideal PSEs from io predictions
  left_join(d.io.categorization %>% 
              dplyr::select(Phase, Condition.Exposure, Block, PSE) %>% 
              rename(PSE.io_predicted = PSE), by = c("Condition.Exposure", "Block")) %>%
  mutate(
    mean_PSE_block1 = mean(c(d.estimates$PSE_median[1], d.estimates$PSE_median[10], d.estimates$PSE_median[19])),
    true_shift = PSE.io_predicted - mean_PSE_block1,
    proportion_shift = (PSE_median - mean_PSE_block1)/true_shift,
    proportion_shift = ifelse(Condition.Exposure != "Shift40", -(proportion_shift), proportion_shift))


# compute PSEs from the 92 talkers in Chodroff-Wilson corpus
d.talkerPSEs <- get_IO_categorization(
    data = d.chodroff_wilson,
    cues = c("VOT"),
    groups = c("Talker", "gender"),
    with_noise = TRUE,
    VOTs = seq(0, 85, .5)) %>% 
  mutate(center_constant = d.true_shift$mean_PSE_block1[1] - mean(PSE),
                        PSE_centered = PSE + center_constant) %>% 
  summarise(lower_PSE = quantile(PSE_centered, probs = .025), 
            mean_PSE = mean(PSE_centered),
            upper_PSE = quantile(PSE_centered, probs = .975)) 
# start with the unnormalised just VOT for overall range
# then use the normalised VOT and F0 to get the PSE

p.PSE_1to7 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms)") +
  aes(y = PSE_median, ymin = PSE_lower, ymax = PSE_upper) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  geom_label(
    data = d.true_shift %>% filter(Block != 1), 
  mapping = aes(label = paste(scales::percent(proportion_shift, accuracy = .1))),
  position = position_dodge(.3),
  label.size = .15,
  label.padding = unit(.18, "lines"),
  size = 2.2,
  show.legend = F) +
  geom_step(
    data = d.io.categorization,
    mapping = aes(x = Block, y = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .8, 
    alpha = 0.5, 
    direction = "mid",
    inherit.aes = F) +
  geom_text(
   data = d.true_shift %>% 
     filter(Phase == "exposure") %>% 
     group_by(Condition.Exposure, true_shift) %>% 
     summarise(),
   mapping = aes(
    x = 10.2,
    y = c(24, 33.2, 65),
    label = paste(round(true_shift), "(100%)"),
    colour = Condition.Exposure),
   colour = colours.condition,
   fontface = "bold",
   size = 3,
   inherit.aes = F) +
  annotate(
    "crossbar",
    x = 9.7,
    y = d.talkerPSEs$mean_PSE,
    ymin = d.talkerPSEs$lower_PSE,
    ymax = d.talkerPSEs$upper_PSE,
    width = 0.1,
    colour = "black",
    alpha = .6) +
    coord_cartesian(
      xlim = c(0.75, 9),
      clip = "off") +
    theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"))

p.estimates_1to7 <-  
  (p.slope_1to7 | p.PSE_1to7) +
  plot_layout(guides = "collect") &
  theme(legend.position = "none", axis.text = element_text(size = 8))
```



```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
get_conditional_effects <- function(model, data, phase) {
  conditional_effects(
    x = model,
    effects = "VOT_gs:Condition.Exposure",
    conditions = make_conditions(
      data %>%
        filter(Phase == .env$phase & Item.Labeled == FALSE) %>%
        prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = "difference"),
      vars = c("Block")),
    method = "posterior_epred",
    ndraws = 500,
    re_formula = NA)
}

cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_mix_exposure, d_for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_mix_test, d_for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(1, 3, 5, 7, 8, 9), "test", "exposure"),
    Block = factor(case_when(
      Block == 1 ~ "Test 1",
      Block == 3 ~ "Test 2",
      Block == 5 ~ "Test 3",
      Block == 7 ~ "Test 4",
      Block == 8 ~ "Test 5",
      Block == 9 ~ "Test 6",
      Block == 2 ~ "Exposure 1",
      Block == 4 ~ "Exposure 2",
      Block == 6 ~ "Exposure 3")),
    Block = fct_relevel(Block, c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3",  "Test 4", "Test 5", "Test 6")))

label_colour <- tibble(
  Block = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
  Block.colour = c("grey", "black", "grey", "black", "grey", "black", "grey")) %>%
  mutate(Block = factor(Block, levels = Block, ordered = T))

p.fit_1to7 <- cond_fit_test_exposure %>%
  filter(Block %in% c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4")) %>%
  ggplot() +
  geom_rect(
    data = label_colour,
    aes(xmin = -Inf, xmax = Inf,
        ymin = 1.05, ymax = 1.3,
        fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "black" = "white")) +
  new_scale_fill() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition.Exposure,
    ymin = lower__, ymax = upper__, fill = Condition.Exposure), alpha = .1) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition.Exposure,
    color = Condition.Exposure),
    linewidth = .7,
    alpha = 0.6) +
  geom_rug(
    data = d_for_analysis %>%
      group_by(Phase, Block) %>%
      filter(Block %in% c(1:7)) %>%
      mutate(
        Block = factor(case_when(
          Block == 1 ~ "Test 1",
          Block == 3 ~ "Test 2",
          Block == 5 ~ "Test 3",
          Block == 7 ~ "Test 4",
          Block == 2 ~ "Exposure 1",
          Block == 4 ~ "Exposure 2",
          Block == 6 ~ "Exposure 3")),
        Block = fct_relevel(Block, c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3",  "Test 4"))) %>%
      distinct(Item.VOT),
    aes(x = Item.VOT),
    alpha = 0.5,
    colour = "grey",
    inherit.aes = F) +
  stat_summary(
    data = d_for_analysis %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE) %>%
      mutate(
        Block = factor(case_when(
          Block == 1 ~ "Test 1",
          Block == 2 ~ "Exposure 1",
          Block == 3 ~ "Test 2",
          Block == 4 ~ "Exposure 2",
          Block == 5 ~ "Test 3",
          Block == 6 ~ "Exposure 3",
          Block == 7 ~ "Test 4"))) %>%
      group_by(Condition.Exposure, Block),
    fun.data = mean_cl_boot,
    mapping = aes(x = Item.VOT,
                  y = Response.Voiceless,
                  colour = Condition.Exposure),
    geom = "pointrange",
    size = 0.1,
    alpha = 0.7,
    position = position_dodge2(width = 2),
    inherit.aes = F) +
  scale_x_continuous("VOT (ms)") +
  scale_y_continuous("Proportion \"t\"-responses") +
  scale_color_manual(
    "Condition",
    labels = c("baseline", "+10ms", "+40ms"),
    values = colours.condition,
    aesthetics = c("color", "fill")) +
  coord_cartesian(clip="off", ylim=c(0, 1)) +
  facet_grid(~ Block, scales = "free_x", space = "free_x") +
  theme(legend.position = "top",
        legend.justification = "right",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        strip.background = element_rect(fill = NA),
        strip.text.x.top = element_text(colour = "black"))

p.fit_7to9 <- 
  cond_fit_test_exposure %>%
  filter(Block %in% c("Test 4", "Test 5", "Test 6")) %>%
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition.Exposure,
    ymin = lower__, ymax = upper__, fill = Condition.Exposure), alpha = .1) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition.Exposure,
    color = Condition.Exposure),
    linewidth = .7,
    alpha = 0.6) +
  geom_rug(
    data = d_for_analysis %>%
      group_by(Phase, Block) %>%
      filter(Block %in% c(7:9)) %>%
      mutate(
        Block = factor(case_when(
          Block == 7 ~ "Test 4",
          Block == 8 ~ "Test 5",
          Block == 9 ~ "Test 6")),
        Block = fct_relevel(Block, c("Test 4", "Test 5", "Test 6"))) %>%
      distinct(Item.VOT),
    aes(x = Item.VOT),
    alpha = 0.5,
    colour = "grey",
    inherit.aes = F) +
  stat_summary(
    data = d_for_analysis %>%
      filter(Block %in% c(7:9), Item.Labeled == FALSE) %>%
      mutate(
        Block = factor(
          case_when(
            Block == 7 ~ "Test 4",
            Block == 8 ~ "Test 5",
            Block == 9 ~ "Test 6"))) %>%
      group_by(Condition.Exposure, Block),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      colour = Condition.Exposure),
    geom = "pointrange",
    size = 0.1,
    alpha = 0.7,
    position = position_dodge2(width = 2),
    inherit.aes = F) +
  scale_x_continuous("VOT (ms)") +
  scale_y_continuous("Proportion \"t\"-responses") +
  scale_color_manual(
    "Condition",
    labels = c("baseline", "+10ms", "+40ms"),
    values = colours.condition,
    aesthetics = c("color", "fill")) +
  facet_wrap(. ~ Block, nrow = 1) +
  theme(
    legend.position = "none",
    legend.justification = "right",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.title.y = element_blank(),
    strip.background = element_rect(fill = "grey"),
    strip.text.x = element_text(colour = "black"))
```

(ref:plot-fit-slope-PSE) Summary of results. **Panel A:** Changes in listeners psychometric categorization functions as a function of exposure, from Test 1 to Test 4 with all intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks since labeled trials provide no information about listeners' categorization function). Point ranges indicate the mean proportion of "t"-responses and their 95% bootstrapped CI. Lines and shaded intervals show the MAP predictions and 95% posterior CIs of a Bayesian mixed-effects psychometric model fit to participants' responses. **Panel B:** Same as Panel A but for the final three test blocks without intervening exposure. Test 4 is shown as part of both Panels A and B. **Panels C \& D:** Changes across blocks in the slope and boundary (point-of-subjective-equality, PSE) of the categorization functions shown in Panels A-B. Point ranges represent the posterior medians and their 95% CI. Dashed reference lines show the intercepts and PSEs that naive (non-rational) learner would be expected to converge against after sufficient exposure (an ideal observer model that knows the exposure distributions). Percentage labels indicate the amount of shift as a proportion of the expected shift under an ideal observer.

\begin{landscape}

```{r plot-fit-slope-PSE, fig.width=12.5, fig.height=8, fig.cap="(ref:plot-fit-slope-PSE)"}
p.fit_1to7 + p.fit_7to9 + p.estimates_1to7 +
  plot_layout(
    design = "
AAAAAAAAA
BBB######
DDDDDDDDD
",
heights = c(1, 1, 2)) +
   plot_annotation(tag_levels = 'A', tag_suffix = ")") & 
  theme(plot.tag = element_text(face = "bold")) 
```

\end{landscape}

### Does exposure affect participants' categorizations (averaging across all blocks)?
We first used the psychometric mixed-effects model to assess whether the exposure conditions had the expected effects across all test blocks *relative to each other*. Unsurprisingly, participants were more likely to respond "t" the larger the VOT ($`r get_bf(fit_mix_test, "mu2_VOT_gs > 0")`$). Critically, exposure affects participants' categorization responses in the expected direction. Marginalizing across all blocks, participants in the +40 condition were less likely to respond "t" than participants in the +10 condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift40vs.Shift10 < 0")`$) or the baseline condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift40vs.Shift10 + mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). There was also evidence---albeit less decisive---that participants in the +10 condition were less likely to respond "t" than participants in the baseline condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). That is, the +10 and +40 conditions resulted in categorization functions that were shifted rightwards compared to the baseline condition, as also visible in Figures \@ref(fig:plot-fit-slope-PSE).

This replicates previous findings that exposure to changed VOT distributions changes listeners' categorization responses [for /b/-/p/: @clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020; for /g/-/k/, @theodore-monto2019]. Having established that exposure affected categorization, we turn to the questions of primary interest. Incremental changes in participants' categorization responses can be assessed from three mutually complementing perspectives. First, we compare how exposure affects listeners' categorization responses relative to other exposure conditions. This tests how early in the experiment differences between exposure conditions began to emerge. Second, we compare how exposure affects listeners' categorization responses within each condition relative to listeners' responses prior to any exposure. This assesses how the exposure conditions relate to participants' prior expectations. Most importantly, however, it tests the subtly different predictions of the model learning and selection hypotheses---whether changes in listeners' categorization responses are strongly constrained. Third and finally, we compare changes in listeners' responses to those expected from an ideal observer that has fully learned the exposure distributions. This tests whether the sublinear effects observed in @kleinschmidt-jaeger2016 replicate in our repeated exposure-test paradigm with the improvements the present study makes to ecological validity.

### Comparing across exposure conditions: How quickly does exposure begin to affect participants' responses? 
Figure \@ref(fig:plot-fit-slope-PSE)A suggests that differences between exposure conditions emerged early in the experiment: already in Test 2, listener's categorization functions seem to be shifted rightwards (larger PSEs) in the +40 condition compared to the +10 condition, and in the +10 condition compared to the baseline condition. This is confirmed by the Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects). Prior to any exposure, during Test 1, participants' responses did not differ across exposure condition (all BFs > XXX). After exposure to only 24 /d/ and 24 /t/ stimuli, during Test 2, participants' responses differed between exposure conditions (BFs > 13.7). The difference between the +40 condition and the +10 or baseline condition kept increasing with exposure up to Test 4. Additional hypothesis tests in Table \@ref(tab:hypothesis-table-interactions) show that the change from Test 1 to 2 was largest (BF = 57.82), followed by the change from Test 2 to 3 (BF = 10), with only minimal changes from Test 3 to 4 (BF = 1.68). Qualitatively paralleling the changes across blocks for the +40 condition, the change in the difference between the +10 and baseline conditions was largest from Test 1 to 2 (BF = 5.42), and then somewhat decreased from Test 2 to Test 4 (BFs < 1). The comparison across exposure conditions thus suggests that changes in listeners' categorization responses emerged quickly---indeed, they were present already *during* the first exposure block (see SI, \@ref(sec:exposure-analyses))---but then leveled off. The comparison across exposure conditions also yields one result that is, at first blush, surprising: while the difference between the +10 and the baseline condition emerged already after the first exposure block, this difference *de*creased, rather than increased, with additional exposure from Test 2 to 3 (see second row of Table \@ref(tab:hypothesis-table-interactions)). We return to this effect below.

Tables \@ref(tab:hypothesis-table-simple-effects) and \@ref(tab:hypothesis-table-interactions) also reveal the consequences of repeated testing. The difference between exposure conditions decreased from Test 4 to 6 (BFs > 4.3; see also Figure \@ref(fig:plot-fit-slope-PSE)B & D). On the final test block, the +10 condition did not differ any longer from the baseline condition. Only the differences between the +40 condition relative to the +10 and baseline conditions persisted, albeit substantially reduced compared to Test 4. This pattern of results replicates previous findings that repeated testing over uniform test continua can undo the effects of exposure [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019], and extends them from perceptual recalibration paradigms to distributional learning paradigms [see also @kleinschmidt2020]. One important methodological consequence of these findings is that longer test phases do not necessarily increase the statistical power to detect effects of adaptation [unless analyses take the effects of repeated testing into account, as in the approach developed in @liu-jaeger2018]. Analyses that average across all test tokens---as remains the norm---are bound to systematically underestimate the adaptivity of human speech perception.

```{r fit-nested-blocks-simple-effects, results='hide'}
# nested model to get simple effects of Condition embedded in block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor")
)

fit_mix_test_nested_block <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d_for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested_block-sample-priorSD15-0.995.rds")

# get tidy df of nested test simple effects within block 
simple_effects_condition <-
  tidy(fit_mix_test_nested_block, effects = "fixed") %>%
  filter(term != "theta1_(Intercept)")
```



```{r hypothesis-table-simple-effects, results='asis'}
# hypotheses to answer questions "when did change first emerge?" 
hyp_contrasts_nested_block <-
  c(
    "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
    "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
    "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
    "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0")

hyp_contrasts_nested_block <- hypothesis(fit_mix_test_nested_block, hyp_contrasts_nested_block, robust = T)
hyp_contrasts_nested_block <- hyp_contrasts_nested_block$hypothesis %>% dplyr::select(-Star)

# translate hypotheses into intelligible statements
hyp_contrasts_nested_block_readable <- tibble(Hypothesis = c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline", "+40 vs. +10", "+40 vs. baseline"), 5)))

table.simple_effects <- 
  make_hyp_table(hyp_contrasts_nested_block_readable, hyp_contrasts_nested_block, caption = "When did exposure begin to affect participants' categorization responses? When, if ever, were these changes undone with repeated testing? This table summarizes the simple effects of the exposure conditions for each test block.") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (no additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (no additional exposure)", 16, 18)
table.simple_effects
```

```{r hypothesis-table-interactions, results='asis'}
# hypotheses to answer question, "was the change incremental between blocks?"
hyp_interactions <- c(
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 < 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 < 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 < 0",
  "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1) < 0",
  
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 > 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 > 0",
  
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
  "(mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",
  
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
  "mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
  
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 < 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 < 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 < 0",
  "(mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 + 
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + 
    mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) < 0",
  
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
  "mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + 
   mu2_Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0")

hyp_interactions <- hypothesis(fit_mix_test, hyp_interactions, robust = T)
hyp_interactions <- hyp_interactions$hypothesis %>% dplyr::select(-Star)

# translate the hypotheses above into intelligible statements
hyp_interactions_readable <- 
  tibble(Hypothesis_readable = rep(c(
  # Comparing differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{PSE}$",
  "Block 2 to 3: increased $\\Delta_{PSE}$",
  "Block 3 to 4: increased $\\Delta_{PSE}$",
  "{\\em Block 1 to 4: increased $\\Delta_{PSE}$}",
  "Block 4 to 5: decreased $\\Delta_{PSE}$",
  "Block 5 to 6: decreased $\\Delta_{PSE}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{PSE}$}"), 3))

table.interactions <- 
  make_hyp_table(
    hyp_interactions_readable,
    hyp_interactions,
    caption = "Was there incremental change from test block 1 to 4? Did these changes dissipate with repeated testing from block 4 to 6? This table summarizes the interactions between exposure condition and block, whether the differences between exposure conditions changed from test block to test block.") %>% 
  pack_rows("Difference in +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in +40 vs. baseline", 15, 21) 

table.interactions
```

### Comparing within exposure conditions: How quickly does exposure begin to affect participants' responses? 
Next, we compared how exposure affects listeners' categorization responses within each condition relative to listeners' responses prior to any exposure. These changes are summarized for the slope and PSE in Figure \@ref(fig:plot-fit-slope-PSE)C & D, respectively. This visualization makes apparent two aspects of participants' behavior that were not readily apparent in the statistical comparisons we have summarized so far. First, while the PSEs for the +40 and +10 conditions were shifted rightwards compared to the baseline condition, both the +10 and the baseline condition actually shift leftwards relative to their pre-exposure starting point in Test 1. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-block-effects). 

### Results summary
This study was set up with several objectives in mind. We aimed to replicate previous findings on distributional learning [@kleinschmidt-jaeger2016] while introducing changes to the design to a) increase the ecological validity of results b) illuminate how soon distributional learning effects can be detected and c) allow investigation into the incremental process of belief updating as predicted by the IA framework. 
[POSSIBLE TO INCLUDE HERE IF THIS IS INTRODUCED AS A SECONDARY OBJECTIVE WHEN DESCRIBED IN THE METHODS: In setting the three exposure conditions we also noted a fourth possible investigation, that is, to test for the presence of "shrinkage" as first discussed in [@kleinschmidt-jaeger2016; @kleinschmidt2020]. In implementing the study this last objective could not be satisfactorily answered therefore we leave its elaboration to the discussion section.]

In consonance with previous studies we find that listeners changed their categorization behavior in the direction of the shift in the exposure talker's VOT distributions. This provides new evidence that listeners do respond to talker statistics when the stimuli are more human-like and sampled from distributions that replicate the variability one would encounter in real life. In test block 1 participants in all groups converged on the same prior categorisation function but then their boundaries spread apart after the first exposure block. Regression analysis showed evidence in favour of the differences in boundary estimates between conditions in test blocks 2 to 4, and these differences were consistent with the direction of the distributional shift. The +10ms condition had a boundary to the right of the baseline condition and the +40ms group had a boundary right of the +10ms condition. This order of the boundary placements was maintained throughout all test blocks after the onset of exposure but their differences began to narrow from test block 5 suggesting a dissipation of distributional learning without further informative exposure.  

A second finding from this study which remained opaque in previous work was that categorization differences between the groups emerged very early on after exposure. It took as few as 48 exposure trials for a clear difference to emerge between the groups. Although we do not yet know if learning was already present prior to the 48 trials, that it does not take hundreds of exposures for listeners to exhibit changes in categorizations aligns with other speech adaptation studies employing different paradigms such as perceptual recalibration and L2 accent adaptation (@bradlow2008; @clarke2004; @norris2006). 

We found some evidence for incremental change in categorisation boundaries as listeners received more input of the talker's cue distributions although this was not always clear from one block to another due to the uncertainty in boundary estimates. Looking at the PSE estimates at each block as a proportion of the ideal boundary implied by their respective distributions (labels Fig. 6), in the +40ms condition listeners increased the shift by roughly 10 percent in the third test block (after 96 exposure trials) from the second block but appeared to regress slightly in test block 4. In the +10ms condition boundaries did shift incrementally after each exposure block buthe proportion of  while in the baseline condition, listeners showed a slight regression in test block 3 before increasing their shift towards the implied boundary in test block 4. These mixed patterns between the conditions do not clearly tell us

In this experiment we also found that the bulk of the maximum boundary shift that each group would make by the end of all 144 exposures was achieved after the first 48 exposure trials. In the +40ms condition listeners achieved their maximum shift in test block 3


What is common to all three conditions is that none of the groups converged on the category boundary implied by the exposure distributions of their respective conditions. 

(ref:exposure-means-database-density) Placement of exposure stimuli relative to an estimate of typical phonetic distributions for 6914 word-initial /d/ and /t/ productions in L1-US English [based on 92 talkers in @chodroff-wilson2018]. The outermost contour of each category shows the 95% density quantile. Points show the category means of the exposure condition.

```{r getting-means-at-exposure-by-condition, eval=FALSE}
VOT.database_mean <- mean(d.chodroff_wilson$VOT.CCuRE)
f0_Mel.database_mean <- mean(d.chodroff_wilson$f0_Mel.CCuRE)

cond_means_exposure <- d_for_analysis %>%
  filter(Phase == "exposure") %>%
  group_by(Condition.Exposure) %>%
  summarise(across(c(VOT.CCuRE, F0_Mel.CCuRE), mean, .names = "{.col}_mean")) %>% 
  mutate(
    VOT_diff_database = VOT.CCuRE_mean - VOT.database_mean,
    F0_diff_database = F0_Mel.CCuRE_mean - f0_Mel.database_mean)

```



```{r centering}
# center exposure cues to speech corpus means
d_for_analysis %<>%
  ungroup() %>% 
  mutate(
    VOT.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(VOT = Item.VOT, Talker = "new"),
      cue = "VOT"),
    F0_Mel.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(f0_Mel = Item.F0_Mel, Talker = "new"),
      cue = "f0_Mel"),
    category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))) 
```


```{r prepare-quantile-plot}
# specify quantiles for density plots
quantile_levels <- c(.05, .25, .5, .75, .95)
d_breaks <- density_quantiles(x = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(VOT.CCuRE),
                               y = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(f0_Mel.CCuRE),
                               quantiles = quantile_levels)
t_breaks <- density_quantiles(x = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(VOT.CCuRE),
                               y = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(f0_Mel.CCuRE),
                               quantiles = quantile_levels)

# matching breaks in each category to corresponding quantile
d_quantile <- quantile_levels
t_quantile <- quantile_levels
names(d_quantile) <- d_breaks
names(t_quantile) <- t_breaks
quantile_breaks <- tibble(quantile_levels, d_breaks, t_breaks)

p.density <-
  d.chodroff_wilson %>%
  ggplot(aes(x = VOT.CCuRE, y = f0_Mel.CCuRE, linetype = category, group = category)) +
  geom_density2d(
    data = . %>% filter(category == "/d/"),
    aes(colour = d_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = d_breaks) +
  geom_density2d(
    data = . %>% filter(category == "/t/"),
    aes(colour = t_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = t_breaks) +
  scale_y_continuous("F0 (Mel)", limits = c(118, 360)) +
  scale_x_continuous("VOT (ms)", limits = c(-12, 125), breaks = scales::breaks_width(25)) +
  scale_colour_gradient("Quantiles",
                        high = "#e6e6e6",
                        low = "#000000",
                        guide = "colourbar",
                        breaks = quantile_levels,
                        labels = scales::percent(quantile_levels)) +
  theme(legend.position = "top",
        legend.text = element_text(size = 7)) +
  #plot centered means
  new_scale_color() +
  geom_point(
    data = d_for_analysis %>%
      filter(Phase == "exposure") %>%
      group_by(Condition.Exposure, category) %>%
      summarise(across(c(VOT.CCuRE, F0_Mel.CCuRE), mean)),
    mapping = aes(x = VOT.CCuRE, y = F0_Mel.CCuRE, colour = Condition.Exposure, shape = category),
    size = 2,
    alpha = 0.8,
  inherit.aes = F,
  show.legend = T) +
  scale_colour_manual("Condition",
             labels = c("baseline", "+10ms", "+40ms"),
             values = colours.condition) +
  guides(colour = "none",
         linetype = guide_legend(title = "Category"),
         shape = guide_legend(title = "Category"))
```



```{r exposure-means-database-density, fig.width=base.width*2.5+1, fig.height=base.height*2.5, warning=FALSE, fig.cap="(ref:exposure-means-database-density)"}
p.density +
  plot_layout(ncol = 1, guides = "collect") &
  theme(legend.position = "top")
```

To understand this pattern, it is helpful to relate our exposure conditions to the distribution of VOT in listeners' prior experience. Figure \@ref(fig:exposure-means-database-density) shows the mean and covariance of our exposure conditions relative to the distribution of VOT by talkers of L1-US English [based on @chodroff-wilson2018]. This comparison offers an explanation as to why the baseline condition (and to some extent the +10 condition) shift leftwards with increasing exposure, whereas the +40 condition shifts rightwards: relative to listeners' prior experience our baseline condition actually presented lower-than-expected category means; of our three exposure conditions, only the +40 condition presented larger-than-expected category means. That is, once we take into account how our exposure conditions relate to listeners' prior experience, both the direction of changes from Test 1 to 4 *within* each exposure condition, and the direction of differences *between* exposure conditions receive an explanation.

```{r hypothesis-table-simple-block-effects, results='asis'}
caption = "In what direction did exposure shift participants' responses compared from block to block? This table summarizes the simple effect of block for each exposure condition."
```

Second, the reason for the slight decrease in the difference between the +10 and baseline conditions observed in Tables \@ref(tab:hypothesis-table-simple-effects) and \@ref(tab:hypothesis-table-interactions) (visible in Figure \@ref(fig:plot-fit-slope-PSE)D as the decreasing difference between the green and red line) is *not* due to a reversal of the effects in the +10 condition. Rather, both conditions are changing in the same direction but the baseline condition stops changing after Test 2, which reduces the difference between the +10 and baseline conditions (see Table \@ref(tab:hypothesis-table-simple-effects)). The comparison across blocks thus suggests a rather uniform picture across all exposure conditions: participants' responses initially changed rapidly with exposure; with increasing exposure, these changes did not only slow down but seem to hit a hard constraint. Participants in the leftwards-shifted baseline condition did not exhibit any further changes in their categorization responses beyond Test 2. Similarly, participants in the rightwards-shifted +40 condition did not exhibit any further changes in their categorization responses beyond Test 3. Only participants in the leftward-shifted +10 condition still exhibit changes across blocks even form Test 3 to 4. But, perhaps tellingly, those participants also never reached the degree of shift that was evident in the baseline condition.  

### Constraints on cumulative changes
Finally, Figures \@ref(fig:plot-fit-slope-PSE)C & D also compare participants' responses against those of an ideal observer that has fully learned the exposure distributions.





