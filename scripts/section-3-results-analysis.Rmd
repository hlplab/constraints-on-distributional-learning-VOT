```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")

# Store parameters for model fitting
d.mean_sd_scaling <-
  d.for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]
rm(d.mean_sd_scaling)

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")
contrast_type <- "difference"
```

# Results

```{r model-fit-test-blocks, include=FALSE}

fit_test <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .995)

fit_exposure <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .999)
```

We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models, using \texttt{brms} [@R-brms_a] in \texttt{R} [@R; @RStudio].^[Fitting the models separately side-steps concerns about how differences in the VOT distribution during exposure blocks might affect the analysis of test blocks. For the test analyses, it also removes any potential collinearity between effects of exposure and effects of VOT. The SI reports additional analyses over the combined data, including extensions of the psychometric models to include non-parametric smooths, as used in generalized additive mixed-effects models. All analyses replicate the findings reported here.] Psychometric models account for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while still common in research on speech perception [for exceptions, see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the present experiment, lapse rates were negligible (`r print_CI(fit_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. <!-- This lapse rate compares favorably against those assumed or reported in prior work [e.g., @clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020]. -->

The psychometric models for exposure and test blocks each regressed participants' categorization responses against the full factorial interaction of VOT, block, and exposure condition, along with the maximal random effect structure (by-subject intercepts and slopes for VOT, block, and their interaction, and by-item intercept and slopes for the full factorial design; see SI, \@ref(sec:analysis-approach)). All hypothesis tests reported below are based on these models. Figure \@ref(fig:plot-fit-slope-PSE) summarizes the results that we describe in more detail next. Panels A and B show participantsâ€™ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panels C and D show the slope and point of subject equality (PSE)---i.e., the point at which participants are equally likely to respond "d" and "t"---of the categorization function across blocks and conditions. These panels facilitate comparison across blocks within each exposure condition. Here we focus on the test blocks, which were identical within and across exposure conditions. Analyses of the exposure blocks are reported in the SI (\@ref(sec:SI-analysis-exposure-blocks)), and replicate all effects found in the test blocks. 

We begin by presenting the overall effects, averaging across all test blocks. This part of our analysis resembles previous work, which analyzed the *average* effect of exposure across the entire experiment ['batch tests', e.g., @clayards2008; @kleinschmidt-jaeger2016; @nixon2016; @theodore-monto2019]. <!-- Unlike those previous studies, however, we compare responses over test trials that are physically identical across conditions and placed throughout the experiment. This reduces the risk that assumptions baked into the analysis approach (e.g., linearity along the acoustic-phonetic continuum) bias the results. --> Then we the address the questions about incremental adaptation that motivated our experiment---testing the predictions described in the introduction. 

```{r fit-nested-models-for-intercepts-slopes-plot, message=FALSE}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_test_nested <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "nested_slope",
    priorSD = 15, 
    adapt_delta = .995)

fit_exposure_nested <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "nested_slope",
    priorSD = 15, 
    adapt_delta = .999)
```

```{r extract-intercepts-slopes-from-test-and-exposure-models}
d.estimates <-
  full_join(
    fit_test_nested %>% get_intercepts_and_slopes(),
    fit_exposure_nested %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    # Adding unscaled slope here to have a more intuitive scale on which to compare VOT slopes 
    # It is also one way to put the slopes for both exposure and test on the same scale (since 
    # the SDs for exposure and test differ). 
    slope_unscaled = slope / (2 * ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test)),
    PSE = 
      descale(
        -Intercept/slope, 
        VOT.mean_test, 
        ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test))) %>% 
  group_by(Condition.Exposure, Block) %>%
  summarise(
    across(
      c(Intercept, slope, slope_unscaled, PSE),
      list(lower = ~ quantile(.x, probs = .025), median = median, upper = ~ quantile(.x, probs = .975)))) %>% 
  relabel_blocks()
```

```{r}
# Make ideal observers that are fit on the exposure VOTs of the three exposure conditions.
# These ideal observers can be thought of as reflecting a learner that has fully learned 
# the exposure distributions. Note that we include perceptual noise in the ideal observers
# (estimated in Kronrod et al., 2016) to make their perception more human-like.
io <-    
  make_VOT_IOs_from_exposure(
    d.for_analysis %>% 
      filter(Phase == "exposure") %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      # Subset data to a single participant per exposure condition. This is sufficient since 
      # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics 
      # (mean and SD) at the end of the experiment, which were identical across participants 
      # in each condition. 
      # 
      # Note that this approach trains the ideal observers on the *actual* VOT distribution 
      # that participants heard, not on the theoretical distributions these VOTs were sampled
      # from. This is in line with our goal to simulate behavior of an idealized participant
      # who has fully learned the exposure distributions.
      slice_sample(n = 1) %>% 
      unnest(data) %>% 
      mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))) %>% 
      # only VOT is used for categorising stimuli
  left_join(d.for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Item.VOT) %>% 
  rename(x = Item.VOT) %>% 
    nest(x = x)) %>% 
  # Add prior based on Chodroff & Wilson (2018). Note that this database has substantially higher 
  # VOT variance than our exposure conditions, which we based on estimates from Kronrod et al. 
  # (2016). Uses all three cues; duration and spectral noise taken from Kronrod et al.
  bind_rows(
    d.chodroff_wilson.isolated %>%
      make_MVG_ideal_observer_from_data(
        cues = c("VOT", "f0_Mel", "vowel_duration"),
        Sigma_noise = matrix(c(80, 0, 0, 0, 878, 0, 0, 0, 80), nrow = 3, dimnames = list(c("VOT", "f0_Mel", "vowel_duration"), c("VOT", "f0_Mel", "vowel_duration")))) %>%
      mutate(Condition.Exposure = "prior") %>% 
      # join in the test stimuli. All 3 cues are used here because we assume listeners' prior experience would take into account all cues
      left_join(d.for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Item.VOT, Item.f0_Mel, Item.vowel_duration) %>% 
        mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.vowel_duration), ~ c(..1, ..2, ..3))) %>% 
  select(Condition.Exposure, x) %>% 
  mutate(Condition.Exposure = "prior") %>% 
  nest(x = x)) %>% 
    nest(io = -c(Condition.Exposure, x)))  

# Get logistic parameter estimates for a simulated ideal observer listener. 
# Unlike for the analysis of participants' responses, there is no need to use a psychometric
# model since ideal observers have no attentional lapses (i.e., lambda = 0). 
d.io.categorization <- 
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io, 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      crossing(Block = c(1:9)) %>%
  mutate(Phase = ifelse(Block %in% c(2, 4, 6), "exposure", "test")) %>% 
  select(Condition.Exposure, Phase, Block, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>% 
  relabel_blocks() %>% 
  mutate(across(c(Phase, Condition.Exposure, Block, Block.plot_label), factor)) 
```

```{r prepare-plot-intercepts-slopes}
p.across_blocks <-
  d.estimates %>% 
  ggplot(
    aes(
      x = Block.plot_label, y = Intercept_median,
      ymin = Intercept_lower, ymax = Intercept_upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.55, 8.55, 1)), 
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = position_dodge(.3), size = 1) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  stat_summary(geom = "line", position = position_dodge(.3)) +
  scale_x_discrete("Block") +
  scale_colour_manual("Condition",
    labels = c("baseline", "+10ms", "+40ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 22.5, hjust = 1)) 

p.intercept_1to7 <-
  p.across_blocks +  
  geom_hline(
    data = d.io.categorization,
    # Using scaled intercept (which is keeping the location at which the intercept is evaluated constant 
    # across exposure and test: it's always the mean VOT of test)
    aes(yintercept = intercept_scaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .7, 
    alpha = 0.3,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("Intercept")

p.slope_1to7 <-
  # Plotting participants' and ideal observers' slopes. We're transforming both types of slopes back 
  # into the unit ms VOT. This makes sure that the different slopes are actually comparable across 
  # exposure and test blocks (participants) and ideal observers, each of which (might) have different
  # SDs that were used during the calculation of the scaled slopes. 
  # 
  # (scaled slopes are good for *effect size* comparison but---perhaps confusingly---that's not the 
  # same as making sure that the effects are actually expressed in the same units.)
  p.across_blocks +  
  # Using UNscaled slope because slopes are comparable across scales anyway and this is more intuitive
  # (remove _unscaled to switch to slopes over Gelman-scaled VOT)
  aes(y = slope_unscaled_median, ymin = slope_unscaled_lower, ymax = slope_unscaled_upper) +
  # These are the slopes of the ideal observers
  geom_line(
    data = d.io.categorization,
    # Using UNscaled slope because slopes are comparable across scales anyway and this is more intuitive
    # (substitute _unscaled to _scaled to switch to slopes over Gelman-scaled VOT)
    aes(x = Block.plot_label, y = slope_unscaled, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, linewidth = .9, alpha = 0.5,
    inherit.aes = F) +
  scale_y_continuous("slope (log-odds/ms VOT)")

# get the relative distance of the IO-predicted PSEs from the starting point PSE at test 1
d.true_shift <- 
  d.estimates %>% 
  # Join the PSEs from ideal observers
  left_join(
    d.io.categorization %>% 
      dplyr::select(Phase, Condition.Exposure, Block, PSE) %>% 
      rename(PSE.io_predicted = PSE), by = c("Condition.Exposure", "Block")) %>%
  group_by(Condition.Exposure) %>% 
  mutate(
    PSE_block1 = 
      case_when(
        Condition.Exposure == "Shift0" ~ d.estimates$PSE_median[1], 
        Condition.Exposure == "Shift10" ~ d.estimates$PSE_median[10],
        Condition.Exposure == "Shift40" ~ d.estimates$PSE_median[19]),
    true_shift = PSE.io_predicted - PSE_block1,
    proportion_shift = (PSE_median - PSE_block1)/true_shift) %>% 
  relabel_blocks()

# Compute distribution of talker-specific PSEs from Chodroff & Wilson (2018) corpus
d.talkerPSEs <- 
  # get each talker's boundary estimate
  get_IO_categorization(
    data = d.chodroff_wilson.isolated,
    cues = c("VOT"),
    groups = c("Talker", "gender"),
    with_noise = TRUE,
    VOTs = seq(0, 85, .5)) %>% 
  # center talker PSEs to human PSE at block 1 of experiment
  mutate(
    center_constant = mean(d.true_shift$PSE_block1) - mean(PSE),
    PSE_centered = PSE + center_constant) %>% 
  summarise(
    lower_PSE = quantile(PSE_centered, probs = .025), 
    mean_PSE = mean(PSE_centered),
    upper_PSE = quantile(PSE_centered, probs = .975)) 

p.PSE_1to7 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms VOT)") +
  aes(y = PSE_median, ymin = PSE_lower, ymax = PSE_upper) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  geom_label(
    data = d.true_shift %>% filter(Block != 1), 
    mapping = aes(label = paste(scales::percent(proportion_shift, accuracy = .1))),
    position = position_dodge(.3),
    label.size = .15,
    label.padding = unit(.18, "lines"),
    size = 2.2,
    show.legend = F) +
  geom_hline(
    data = d.io.categorization,
    mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .8, 
    alpha = 0.4, 
    direction = "mid",
    inherit.aes = F) +
  geom_text(
    data = d.true_shift %>% 
      filter(Phase == "exposure") %>% 
      group_by(Condition.Exposure, true_shift) %>% 
      summarise(),
    mapping = aes(
      x = 10.2, y = c(25, 35, 65),
      label = paste(round(true_shift), "(100%)"),
      colour = Condition.Exposure),
    colour = colours.condition,
    fontface = "bold",
    size = 3,
    inherit.aes = F) +
  annotate(
    "crossbar",
    x = 9.7, y = d.talkerPSEs$mean_PSE, ymin = d.talkerPSEs$lower_PSE, ymax = d.talkerPSEs$upper_PSE,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  coord_cartesian(xlim = c(0.75, 9), clip = "off") +
  theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"))

p.estimates_1to7 <-  
  (p.slope_1to7 | p.PSE_1to7) +
  plot_layout(guides = "collect") &
  theme(legend.position = "none", axis.text = element_text(size = 8))
```

```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_exposure, d.for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_test, d.for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(2, 4, 6), "exposure", "test")) %>% 
  relabel_blocks()

label_colour <- 
  tibble(
    Block.plot_label = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
    Block.colour = c("grey", "white", "grey", "white", "grey", "white", "grey")) %>%
  mutate(Block.plot_label = factor(Block.plot_label, levels = Block.plot_label, ordered = T))

p.fit_1to7 <- 
  cond_fit_test_exposure %>%
  filter(Block %in% c(1:7))  %>%
  ggplot() +
  geom_rect(
    data = label_colour,
    aes(xmin = -Inf, xmax = Inf,
        ymin = 1.05, ymax = 1.3,
        fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "white" = "white")) +
  new_scale_fill() +
  geom_linefit(data = d.for_analysis %>%
      group_by(Phase, Condition.Exposure, Block.plot_label) %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE),
      x = Item.VOT,
      y = estimate__,
      fill = NA,
      legend.position = "top",
      legend.justification = "right") +
  coord_cartesian(clip = "off", ylim = c(0, 1))

p.fit_7to9 <- 
  cond_fit_test_exposure %>%
  filter(Block %in% c(7:9)) %>%
  ggplot() +
  geom_linefit(data = d.for_analysis %>%
      group_by(Condition.Exposure, Block) %>%
      filter(Block %in% c(7:9)),
      x = Item.VOT,
      y = estimate__,
      fill = "grey",
      legend.position = "none") +
  theme( axis.title.y = element_blank())
```

(ref:plot-fit-slope-PSE) Summary of results. **Panel A:** Changes in listeners psychometric categorization functions as a function of exposure, from Test 1 to Test 4 with all intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks since labeled trials provide no information about listeners' categorization function). Point ranges indicate the mean proportion of participants' "t"-responses and their 95% bootstrapped CI. Lines and shaded intervals show the *maximum a posteriori* (MAP) estimates and 95% posterior CIs of a Bayesian mixed-effects psychometric model fit to participants' responses. **Panel B:** Same as Panel A but for the final three test blocks without intervening exposure. Test 4 is shown as part of both Panels A and B. **Panels C \& D:** Changes across blocks in the slope and boundary (point-of-subjective-equality, PSE) of the categorization functions shown in Panels A \& B. Point ranges represent the posterior medians and their 95% CI. Dashed reference lines show the intercepts and PSEs that naive learner would be expected to converge against after sufficient exposure (an ideal observer model that has fully learned the exposure distributions). Percentage labels indicate the amount of shift as a proportion of the expected shift under an ideal observer.

\begin{landscape}

```{r plot-fit-slope-PSE, fig.width=12.5, fig.height=8, fig.cap="(ref:plot-fit-slope-PSE)"}
p.fit_1to7 + p.fit_7to9 + p.estimates_1to7 +
  plot_layout(
    design = "
AAAAAAAAA
BBB######
DDDDDDDDD
",
heights = c(1, 1, 2)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") & 
  theme(plot.tag = element_text(face = "bold")) 
```

\end{landscape}

```{r}
rm(p.across_blocks, p.estimates_1to7, p.PSE_1to7, p.slope_1to7, p.fit_1to7, p.fit_7to9)
```

<!-- TO DO: If we need to cut, this section could probably go. It's only here to 'ease in' readers. -->
## Conceptual replication (averaging over test blocks)
Unsurprisingly, participants were more likely to respond "t" the longer the VOT ($`r get_bf(fit_test, "mu2_VOT_gs > 0")`$). Critically, exposure affected participants' categorization responses in the expected direction. Marginalizing over all test blocks, participants in the +40 condition were less likely to respond "t" than participants in the +10 condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift40vs.Shift10 < 0")`$) or the baseline condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift40vs.Shift10 + mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). There was also evidence---albeit less decisive---that participants in the +10 condition were less likely to respond "t" than participants in the baseline condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). That is, the +10 and +40 conditions resulted in categorization functions that were shifted rightwards compared to the baseline condition, as also evident in Figures \@ref(fig:plot-fit-slope-PSE).

This conceptually replicates previous findings that exposure to different VOT distributions changes listeners' categorization responses [for /b/-/p/: @clayards2008; @kleinschmidt2015; @kleinschmidt2020; for /g/-/k/, @theodore-monto2019]. Next, we turn to the questions of primary interest. Incremental changes in participants' categorization responses can be assessed from three mutually complementing perspectives. First, we compare how exposure affects listeners' categorization responses *relative to other exposure conditions*. This is the perspective taken in previous studies, but extended to test *how early* in the experiment differences between exposure conditions begin to emerge. Second, we compare how exposure *incrementally changes* listeners' categorization responses from block to block within each condition, relative to listeners' responses prior to any exposure. Third, we compare changes in listeners' responses to those expected from an ideal observer that has fully learned the exposure distributions. This analysis has the potential to identify *constraints on cumulative adaptation*. As we show below, these latter two perspectives---which are only made available through the use to incremental, repeated testing---afford stronger tests of the predictions laid out in the introduction, and suggest previously unrecognized constraints on the early moments of adaptive speech perception. For all three analyses, we initially focus on Tests 1-4 with intermittent exposure. Following that, we analyze the effects of repeated testing during Tests 4-6. Though research---including some our of own previous work---tends to interpret tests as passive windows into the effects of exposure, test stimuli *also* constitute part of the exposure input listeners' receive. As we discuss below, this has both methodological and theoretical consequences.

```{r}
rm(fit_test_nested, fit_exposure, fit_exposure_nested, cond_fit_test_exposure)
```

## How quickly does exposure affect listeners' categorization responses? (comparing exposure conditions within each test block)
Figure \@ref(fig:plot-fit-slope-PSE)A suggests that differences between exposure conditions emerged early in the experiment: already in Test 2, listeners in the +10 condition have shifted their categorization functions rightwards relative to the baseline condition, and listeners in the +40 condition have shifted their in categorization functions even further rightwards. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-condition). Prior to any exposure, during Test 1, participants' responses did not differ across exposure condition. This result is predicted by models of adaptive speech perception under the assumptions that (a) participants in the different groups have similar prior experiences, and that (b) our sample size is sufficiently large to yield stable estimates of listeners' categorization function.

During Test 2, after exposure to only 24 /d/ and 24 /t/ stimuli (thereof half labeled), participants' categorization responses already differed between exposure conditions (BFs > 14). The differences between exposure conditions that emerged at this point were all in the direction predicted by models of adaptive speech perception. Additional analyses reported in the SI (\@ref(sec:exposure-analyses)) found that listeners' categorization functions had already changed *during* the first exposure block, in line with Figure \@ref(fig:plot-fit-slope-PSE)A. This suggests that changes in listeners' categorization responses emerged *quickly* at the earliest point tested---after only a fraction of exposure trials previously tested in similar paradigms. 

The effects of the three exposure conditions continued to persist until Test 4. Table \@ref(tab:hypothesis-table-simple-effects-condition) does, however, indicate an interesting non-monotonic development in the way that listeners' categorization function changed. While the difference between the +40 condition and both the baseline and +0 condition continued to increase numerically with increasing exposure (increasingly larger magnitude of negative estimates in Tests 2-4), the same was not the case for the difference between the +10 and the baseline condition. Instead, the difference between the +10 and baseline condition *reduced* with increasing exposure (while maintaining its direction). This seemingly unexpected development turns out to be potentially important in understanding incremental adaptation, and we continue to discuss it below.

```{r fit-simple-effects-condition, results='hide'}
# Get simple effects of Condition nested under block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_condition <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-condition.rds")
```

```{r hypothesis-table-simple-effects-condition, results='asis'}
hyp.simple_effects_condition <- 
  hypothesis(
    fit_test.simple_effects_condition, 
    c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  hyp.simple_effects_condition, 
  c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline", "+40 vs. +10", "+40 vs. baseline"), 5)), 
    caption = "When did exposure begin to affect participants' categorization responses? When, if ever, were these changes undone with repeated testing? This table summarizes the simple effects of the exposure conditions for each test block. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses).") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)

rm(fit_test.simple_effects_condition)
```

## Incremental adaptation from prior expectations (comparing block-to-block changes within exposure conditions)
Next, we compare how exposure affected listeners' categorization responses from block to block *within* each exposure condition. To facilitate visual comparison, Figure \@ref(fig:plot-fit-slope-PSE)C & D summarize these changes for the slope and PSE, respectively. Focusing for now on Tests 1-4, this highlights four aspects of participants' behavior that were not readily apparent in the statistical comparisons presented so far. 

```{r fit-simple-effects-block, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift0:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift10:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift40:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_block <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Condition.Exposure/(Block * VOT_gs) + 
      (0 + Block * VOT_gs | ParticipantID) + 
      (0 + Condition.Exposure/(Block * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-block.rds")
```

```{r hypothesis-table-simple-effects-block, results='asis', fig.cap=""}
hyp.simple_effects_block <- 
  hypothesis(
    fit_test.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",
      
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 + mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
    
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 + mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
    hyp.simple_effects_block,
    c(rep(c(
  # Comparing differences blocks within conditions
  "Block 1 to 2: decreased PSE",
  "Block 2 to 3: decreased PSE",
  "Block 3 to 4: decreased PSE",
  "{\\em Block 1 to 4: decreased PSE}",
  "Block 4 to 5: increased PSE",
  "Block 5 to 6: increased PSE",
  "{\\em Block 4 to 6: increased PSE}"), 2),
  "Block 1 to 2: increased PSE",
  "Block 2 to 3: increased PSE",
  "Block 3 to 4: increased PSE",
  "{\\em Block 1 to 4: increased PSE}",
  "Block 4 to 5: decreased PSE",
  "Block 5 to 6: decreased PSE",
  "{\\em Block 4 to 6: decreased PSE}"),
    caption = "Was there incremental change from test block 1 to 4? Did these changes dissipate with repeated testing from block 4 to 6? This table summarizes the simple effects of block for each exposure condition. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses).") %>% 
  pack_rows("Difference between blocks: baseline", 1, 7) %>%
  pack_rows("Difference between blocks: +10", 8, 14) %>%
  pack_rows("Difference between blocks: +40", 15, 21) 

rm(fit_test.simple_effects_block)
```

```{r, fig.caption="TEMPORARILY PUTTING SLOPES PLOT HERE", results='asis'}
hyp_interaction.condition_block_VOT <-
  hypothesis(
    fit_test,
    c(
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3) > 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 < 0",
      "mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 < 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5) < 0",
      
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 > 0",
      "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 > 0",
      "(mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test4vs.Test3 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test4vs.Test3 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test3vs.Test2 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test3vs.Test2 +
    mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test2vs.Test1 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test2vs.Test1) > 0",

    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0",
    "mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test5vs.Test4 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test5vs.Test4 +
   mu2_VOT_gs:Condition.Exposure_Shift10vs.Shift0:Block_Test6vs.Test5 + mu2_VOT_gs:Condition.Exposure_Shift40vs.Shift10:Block_Test6vs.Test5 > 0"),
   robust = T) %>%
  .$hypothesis %>%
  dplyr::select(-Star)

table.interactions.condition_block_VOT <-
  make_hyp_table(
    hyp_interaction.condition_block_VOT,
    rep(c(
  # Comparing slope differences in +10 vs. baseline between blocks
  "Block 1 to 2: increased $\\Delta_{slope}$",
  "Block 2 to 3: increased $\\Delta_{slope}$",
  "Block 3 to 4: increased $\\Delta_{slope}$",
  "{\\em Block 1 to 4: increased $\\Delta_{slope}$}",
  "Block 4 to 5: decreased $\\Delta_{slope}$",
  "Block 5 to 6: decreased $\\Delta_{slope}$",
  "{\\em Block 4 to 6: decreased $\\Delta_{slope}$}"), 3),
    caption = "Did the slope differences between exposure conditions change from block to block? This table summarizes the interactions between exposure condition and block---specifically, whether the differences in slopes between exposure conditions changed from test block to test block.",
  digits = 3) %>%
  pack_rows("Difference in slopes: +10 vs. baseline", 1, 7) %>%
  pack_rows("Difference in slopes: +40 vs. +10", 8, 14) %>%
  pack_rows("Difference in slopes: +40 vs. baseline", 15, 21)
table.interactions.condition_block_VOT
```

First, while the PSEs for the +40 and +10 conditions were shifted rightwards compared to the baseline condition, both the +10 and the baseline condition seem to shift *left*wards relative to their pre-exposure starting point in Test 1. This is supported by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-block). The evidence for the leftward shifts is quite weak for the +10 condition (BF = 3.5 for changes from Test 1 to 4), for which the PSE changes comparatively little across tests, but it is stronger for the baseline condition (BF = 7.6). In contrast, the +40 condition is clearly shifted rightwards relative to pre-exposure (BF = 45.2). To understand this pattern, it is helpful to relate the three exposure conditions to the distribution of VOT in listeners' prior experience. Figure \@ref(fig:exposure-means-database-matrix-plot) shows a matrix plot of pairwise distributions for the three cues that are relevant to distinguishing L1-US English /d/ and /t/ [based on databases of two speech styles in @chodroff-wilson2018] with the category means of our exposure conditions. This comparison offers an explanation as to why the baseline condition (and to some extent the +10 condition) shift leftwards with increasing exposure, whereas the +40 condition shifts rightwards: relative to listeners' prior experience, only the +40 condition presented larger-than-expected category means, whereas the baseline condition and, to some extent, the +10 condition presented lower-than-expected category means. That is, once we take into account how our exposure conditions relate to listeners' prior experience, both the direction of changes from Test 1 to 4 *within* each exposure condition (Table \@ref(tab:hypothesis-table-simple-effects-block)), and the direction of differences *between* exposure conditions receive an explanation (Table \@ref(tab:hypothesis-table-simple-effects-condition)). To further illustrates this point, the gray dashed line in Figure \@ref(fig:plot-fit-slope-PSE)D shows the PSE predicted by a Bayesian ideal observer trained on the distribution of VOT, f0, and vowel duration in Figure \@ref(fig:exposure-means-database-matrix-plot). Following @xie2023, we included perceptual noise in the ideal observer [estimates taken from @kronrod2016, for details, see SI, \@ref(sec:prior-idealized-listeners)].  While not a perfect fit, this line qualitatively predicts listeners' pre-exposure PSE in Test 1. 

(ref:exposure-means-database-matrix-plot) Placement of exposure stimuli relative to an estimate of typical phonetic distributions for `r nrow(d.chodroff_wilson)` word-initial /d/ and /t/ productions by `r length(unique(d.chodroff_wilson$Talker))` female L1 talkers of US English in @chodroff-wilson2018. For details, see SI \@ref(sec:SI-phonetic-data)]. Points show the category means of the exposure conditions.

```{r exposure-means-database-matrix-plot, fig.width=base.width*4, fig.height=base.height*2.5+1, fig.cap="(ref:exposure-means-database-matrix-plot)"}
d.chodroff_wilson %>%
  group_by(speechstyle, category) %>%
  filter(if_all(c(VOT, f0_Mel, vowel_duration), ~ abs(scale(.x)[,1]) < 3.5)) %>%
  ggplot(aes(color = category, fill = category)) +
  geom_autopoint(aes(shape = speechstyle), alpha = .4, show.legend = F) +
  geom_autodensity(aes(linetype = speechstyle), position = "identity", alpha = .5) +
  stat_ellipse(aes(x = .panel_x, y = .panel_y, linetype = speechstyle)) +
  scale_color_manual(
      "Category",
      labels = c("/d/", "/t/"),
      values = colours.category_greyscale,
      aesthetics = c("color", "fill")) +
  new_scale_color() +
  geom_autopoint(
    data = d.for_analysis %>% filter(Phase == "exposure") %>% 
  group_by(Condition.Exposure, category) %>% 
  summarise(across(c(VOT, f0_Mel, vowel_duration), mean)), 
  mapping = aes(shape = category, colour = Condition.Exposure), 
  alpha = .8, size = 1.8,
  show.legend = F,
  inherit.aes = F) +
  scale_colour_manual(
             labels = c("baseline", "+10ms", "+40ms"),
             values = colours.condition) +
   guides(colour = "none",
         linetype = guide_legend(title = "Speech style", override.aes = list(fill = NA, shape = c(0, 3)))) +
  facet_matrix(vars(c(VOT, f0_Mel, vowel_duration)), layer.lower = c(1, 4:5), layer.diag = 2,
               layer.upper = c(3, 4:5)) +
  theme(legend.position = "top")
```

Second, similar reasoning about the exposure distributions relative to listeners' prior experience offers an explanation for the relative lack of changes in the slope of listeners categorization function, visible in Figure \@ref(fig:plot-fit-slope-PSE)C. Slope changes, or lack thereof, have received comparatively little attention in previous work [but see @clayards2008; @theodore-monto2019] but they are an important part of the empirical facts that theories of speech perception need to account for [see also @kleinschmidt2020]. Compared to the changes in PSEs in Panel D, changes in the *slope* of listeners' categorization functions in Panel C were similar across exposure conditions ($0.XX \leq$ BFs $\leq X.XX$; SI, \@ref(sec:SI-slope-tests)).<!-- TO DO: these should be the simple slopes of exposure within each test block--> Slopes also changed little relative to listeners' categorization responses in Test 1 ($0.27 \leq$ BFs $\leq 4.66$; see SI, \@ref(sec:SI-slope-tests)).<!-- TO DO: these should be the simple slopes of block within each exposure condition--> Both of these findings are in line with distributional learning models of adaptive speech perception [@kleinschmidt-jaeger2015], given that the variance of /d/ and /t/ was (a) held constant across all three exposure conditions, and (b) designed to approximate the variance of /d/ and /t/ in typical speech input. Indeed, the same ideal observer model used to predict listeners' pre-exposure PSE also quite closely predicts listeners' pre-exposure slopes in Test 1, prior to any exposure (gray dashed lines in  Figure \@ref(fig:plot-fit-slope-PSE)C; we discuss the colored dashed lines in the next section).

Third, returning to PSEs, the estimates in Table \@ref(tab:hypothesis-table-simple-effects-block) suggest that listeners' PSEs changed most from Test 1 to Test 2, and then changed less and less with additional exposure up to Test 4 (smaller magnitude of estimates compared to earlier test blocks). This is particularly pronounced for the two conditions that shifted the most relative to pre-exposure, the baseline condition and the +40 condition. This pattern is predicted by models of adaptive speech perception that are sensitive to the prediction error experienced while processing speech. This includes models that assume error-based learning [@sohoglu-davis2016; see also discussion in @davis-sohoglu2020; @harmon2019] as well as Bayesian belief-updating models [@kleinschmidt-jaeger2015; for demonstration, see @jaeger2019].

Fourth and finally, Panel D also begins to illuminate the reasons for the non-monotonic development of the +10 and baseline conditions relative to each other, discussed in the previous section. In particular, this non-monotonicity does *not* appear due to a reversal of the effects in either of the two exposure conditions. Rather, both exposure conditions continue to change listeners' categorization function in the same direction from Test 1 to Test 4. However, after the rapid change from the pre-exposure Test 1 to the first post-exposure Test 2, listeners' categorization responses in the baseline condition did not change as much as in the +10 condition. Additional Bayesian hypothesis tests reported in the SI (\@ref(sec:hypothesis-test-test-interactions)) suggest that these differences in the incremental effects of the two conditions are credible (BF = XXX).<!-- TO DO: this is the interaction test. Pls add info from SI --> This explains the reduction in the difference between the +10 and baseline conditions discussed in the previous section. It does, however, raise the question *why* listeners' responses in the baseline condition did not change further with increasing exposure. Our third and final perspective on the incremental changes induced by exposure begins to address this question.

## Constraints on cumulative adaptation (comparing exposure effects against idealized learner models)
Figure \@ref(fig:plot-fit-slope-PSE)C-D also compare participants' responses against those of an idealized learner that has fully learned the exposure distributions (red, green, and blue dashed lines). Specifically, we fit Bayesian ideal observers against the labeled VOT distributions of each exposure condition, using the same approach used for the idealized pre-exposure listeners (gray dashed lines). The dashed lines represent the slopes and PSEs, respectively, that are expected from such idealized learners (for details, see SI \@ref(sec:SI-bias-correction)).<!-- TO DO: that section should explain the general approach and also contain the bias corrected estimates. -->
This makes it possible to assess whether---or how much---listeners have converged against the exposure distributions. We make two observations. 

First, the slopes of listeners' categorization functions in Panel C approximate those predicted by the idealized learner models: many of the 95% CIs overlap with the dashed lines. At the same time, listeners' slopes do not seem to fully converge against those expected from an idealized learner.^[Without the inclusion of perceptual noise, ideal observers predict much steeper categorization functions [see also @feldman2009; @kronrod2016]. This offers a potential explanation for the mismatch between the ideal observer predictions and human categorization responses when perceptual noise is not considered [@clayards2008].] Second, a similar tendency is much more clearly observed for the PSEs. Panel D suggests that listeners did *not* converge against the exposure distributions. The percentage labels in Panel D quantify the degree to which listeners adapted their PSE towards the statistics of the exposure condition: 0% would correspond to no change relative to the listeners' PSE in Test 1, and 100% would correspond to complete convergence against the PSE predicted for an idealized learner. This highlights an asymmetry between the condition resulting in rightward shifts of the categorization function (+40), and the conditions resulting in leftward shifts (baseline and +10). On the one hand, the predicted PSEs of an idealized learner for the +40 and baseline conditions are shifted approximately by about the same amount relative to listeners' pre-exposure PSE in Test 1. However, the degree to which listeners converged against these predicted PSEs differed substantially between the two conditions, with cumulative adaptation proceeding almost twice as far in the rightward-shifted +40 condition (in Test 4: `r d.true_shift %>% filter(Condition.Exposure == "Shift40" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()` towards idealized PSE) compared to the leftward-shifted baseline condition (`r d.true_shift %>% filter(Condition.Exposure == "Shift0" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()`). Comparing within just the leftward-shifted conditions, we find that relative shift is smaller for the baseline condition, compared to the +10 condition (`r d.true_shift %>% filter(Condition.Exposure == "Shift10" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()`). We return to these findings in the general discussion.

<!-- TO DO: considering pointing out that these constraints are *not* predicted by IBBU models, though it's still not clear whether that's actually the case -->

## Effects of repeated testing

```{r calculate-test-surprisal}
d.test_surprisal <-
  d.for_analysis %>% 
  filter(Phase == "test") %>% 
  distinct(Item.VOT) %>%
  cross_join(io) %>% 
  filter(Condition.Exposure != "prior") %>% 
  # For each test stimulus, calculate the sum of densities of /d/ and /t/ over 
  # that VOT (i.e., the density of the marginal VOT distribution)
  mutate(
    density = map2_dbl(
      Item.VOT, 
      io, 
      function(x, y) 
        pmap(
          list(y$mu, y$Sigma, y$Sigma_noise),
          ~ dmvnorm(x, ..1, ..2 + ..3)) %>%
        reduce(`+`)),
    surprisal = -log(density))

# # Visualize marginal density
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = density, color = Condition.Exposure)) +
#       geom_line()
# 
# # Visualize surprisal
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = surprisal, color = Condition.Exposure)) +
#       geom_line()

d.test_surprisal %<>%
  group_by(Condition.Exposure) %>%
  summarise(across(surprisal, list(mean = mean, sd = sd)))
```

Finally, we briefly summarize the effects of repeated testing. Some models of adaptive perception predict that exposure to uniformly distributed test tokens will reduce the effect of preceding exposure [@kleinschmidt-jaeger2015; for relevant discussion, see also @lancia-winter2013]. In line with these theories, there is evidence that the effects of exposure reduced from Test 4 to Test 6 (see Tables \@ref(tab:hypothesis-table-simple-effects-condition) and \@ref(tab:hypothesis-table-simple-effects-block)). In Table \@ref(tab:hypothesis-table-simple-effects-block), this is evident in a reversal of the direction of the block-to-block changes for Tests 5-6, compared to Tests 1-4. For the +40 exposure condition, these block to block changes went from rightward shifts in Tests 1-4 to leftward shifts in Tests 5-6 (BF = 10.4). For the baseline condition, block to block changes went from leftward to rightward shifts (BF = 7.3). The only exposure condition for which no clear reversal was observed is the +10 condition (BF = 1.3). Two factors likely contributed to this. First, this condition exhibited the smallest exposure effects, limiting the power to detect a reversal of those effects. Second, the +10 condition is also the condition, for which the marginal distribution of VOT during test blocks (mean = 35.8 ms, SD = 22.2 ms) most closely resembled the distribution during exposure (mean = 36.5, SD = 25.9), compared to the baseline (mean = 26.5 ms) or +40 condition (mean = 66.5 ms; exposure SDs were identical across conditions).^[This does not necessarily entail that test trials were more expected in the +10 condition, so that listeners experienced smaller prediction errors. For example, for an ideal observer that has *fully* learned the exposure distribution (cf. colored dashed lines in Figure \@ref(fig:plot-fit-slope-PSE)C-D), test stimuli conveyed about the same amount of surprisal in the baseline and +10 conditions (mean surprisal = `r d.test_surprisal %>% filter(Condition.Exposure == "Shift10") %>% pull(surprisal_mean) %>% round(., 1)` bits), compared to larger surprisal in the + 40 condition (`r d.test_surprisal %>% filter(Condition.Exposure == "Shift40") %>% pull(surprisal_mean) %>% round(., 1)` bits).]

As a consequence of repeated testing, exposure effects were substantially smaller in Test 6 than in Test 4 (see Table \@ref(tab:hypothesis-table-simple-effects-condition): while the effects of the +40 condition relative to the other two exposure conditions were still credible even in Test 6 (BFs > 24), this was no longer the case for the effect of the +10 condition relative to the baseline condition (BF = 1.6). This pattern of results replicates previous findings from LGPL [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @tzeng2021], and extends them to distributional learning paradigms [see also @kleinschmidt2020]. One important methodological consequence is that longer test phases do not necessarily increase the statistical power to detect effects of adaptation [unless analyses take the effects of repeated testing into account, as in the approach developed in @liu-jaeger2018]. Analyses that average across all test tokens---as remains the norm---are bound to systematically underestimate the adaptivity of human speech perception.<!-- ^[@kraljic-samuel2006 is sometimes cited as finding LGPL exposure effects even after 480 test trials over a uniform test continuum. This is, however, misleading. Kraljic and Samuel used four *different* uniform test continua over two different phonetic contrasts (/b/-/p/ and /d/-/t/). Each test session consisted of 10 randomized repetitions of 6 test trials. Kraljic and Samuel never tested (or made any claims about) whether exposure effects were still detectable during the 10th repetition. Rather they report *average* effects across the 10 repetitions (like other LGPL studies), which is perfectly compatible with the hypothesis that repeated testing reduces the effects of exposure [see @liu-jaeger2018].] -->


