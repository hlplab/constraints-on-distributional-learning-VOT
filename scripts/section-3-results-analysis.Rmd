```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")


# Store parameters for model fitting
d.mean_sd_scaling <-
  d.for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]
rm(d.mean_sd_scaling)

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")

```

# Results {#sec:results}
As outlined under Methods, we present two types of analyses. We first use a Bayesian mixed-effects psychometric model to estimate listeners' categorization functions prior to, during, and after the three exposure blocks. Psychometric models provide a theory-agnostic way to test predictions 1-4, while accounting for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while still common in research on speech perception [for exceptions, see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the advantages of Bayesian mixed-effects psychometric models, we refer to @prins2023, though we note that we implemented a mixture model formulation of such models in the freely available statistics software \texttt{R} [@R-base], using the amazing general-purpose package \texttt{brms} for Bayesian regression modeling [@R-brms_a]. After we present the results of the psychometric analysis, we introduce the ideal adaptor model, and fit it to the same data. Unlike the psychometric model, the ideal adaptor is a model of distributional learning, and we compare it against the *much* less constrained psychometric model to assess how good a model of listeners' behavior distributional learning provides. This also helps us identify the limitation of the ideal adaptor model that we mentioned in the introduction.

```{r model-fit-test-blocks, include=FALSE}
fit_test <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "standard",
    priorSD = 15,
    adapt_delta = .995)

fit_exposure <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "standard",
    priorSD = 15,
    adapt_delta = .999)
```

## Bayesian mixed-effect psychometric analysis
We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models. The full model specification with additional pointers to tutorials is available in the SI (\@ref(sec:analysis-approach)). Compared to previous work [@clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020], lapse rates were negligible (`r print_CI(fit_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. The SI reports additional analyses over the combined exposure and test data, including extensions of the psychometric models to include lapse rates that can vary by block (\@ref(sec:analysis-lapse)) and non-parametric smooths to model non-linear effects of VOT and exposure (\@ref(sec:GAMM)). All analyses replicate the findings reported here.

The psychometric models for exposure and test blocks each regressed participants' categorization responses against the full factorial interaction of VOT, block, and exposure condition, along with the maximal random effect structure (by-participant intercepts and slopes for VOT, block, and their interaction, and by-item intercept and slopes for the full factorial design; see SI, \@ref(sec:analysis-approach)). All hypothesis tests reported below are based on these models. The Bayes factors (BF) we report quantify the strength of evidence from the data for one hypothesis over an alternative hypothesis. BFs < 3 are sometimes described as indicating "anecdotal" evidence for the hypothesis; BFs between 3 and 10, "moderate" evidence; between 10 and 30, "strong" evidence; and > 30, "very strong" evidence [@jeffreys1961; @kruschke-liddell2018]. We do, however, also report the posterior probability of each hypothesis. This provides a measure of how probable the hypothesis is given the data (assuming uniform prior probabilities for both the proposed hypothesis and its alternative), which might be more intuitive to readers unfamiliar with Bayesian data analysis.

We used the psychometric model to assess incremental changes in participants’ categorization responses from three mutually complementing perspectives (all based on the same psychometric mixed-effects model). First, we compare the effects of different exposure conditions within each block. This is the perspective taken in previous distributional learning experiment, but extended to test *how early* in the experiment differences between exposure conditions begin to emerge. Second, we compare *within* each condition how exposure *incrementally changes* listeners' categorization responses from block to block within each condition, relative to listeners' responses prior to any exposure. Together with the first perspective, this allows us to test predictions (1 - *prior expectations*)-(2a,b - *exposure amount & distribution*). Third, we compare changes in listeners' responses to those expected from an ideal observer that has fully learned the exposure distributions. This analysis has the potential to identify constraints on cumulative adaptation. As we show, the latter two perspectives---made possible by the incremental exposure-test paradigm---afford stronger tests of predictions (3 - *diminishing returns*) and (4 - *learn to convergence*), and suggest previously unrecognized constraints on the early moments of adaptive speech perception. For all three analyses, we initially focus on Tests 1-4 with intermittent exposure. Finally, we analyze the effects of repeated testing without intermittent exposure blocks during Tests 4-6. This reveals that such testing has under-appreciated methodological and theoretical consequences.

## How early does exposure begin to affect listeners' categorization responses? (comparing exposure conditions within each test block)
Figure \@ref(fig:plot-fit-PSE) summarizes the results that we describe in more detail next. Panels A and B show participants’ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panel C summarizes changes in listeners' PSE across blocks and conditions. This highlights how the type and amount of phonetic input affect listeners' categorization functions. Here we focus on the test blocks, which were identical within and across exposure conditions. Analyses of the exposure blocks replicate all effects found in the test blocks (SI, \@ref(sec:SI-exposure-block-analysis)).

```{r fit-nested-models-for-intercepts-slopes-plot, message=FALSE}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_test_nested_within_condition_and_block <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "nested_within_condition_and_block",
    priorSD = 15,
    adapt_delta = .995)

fit_exposure_nested_within_condition_and_block <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "nested_within_condition_and_block",
    priorSD = 15,
    adapt_delta = .999)
```


```{r fit-simple-effects-condition, results='hide'}
# Get simple effects of Condition nested under block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_condition <-
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-condition.rds")
```

```{r extract-intercepts-slopes-from-test-and-exposure-models}
d.psychometric_intercept.slope.PSE.draws <-
  full_join(
    fit_test_nested_within_condition_and_block %>% get_intercepts_and_slopes(),
    fit_exposure_nested_within_condition_and_block %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    # Adding unscaled slope here to have a more intuitive scale on which to compare VOT slopes
    # It is also one way to put the slopes for both exposure and test on the same scale (since
    # the SDs for exposure and test differ).
    # intercepts have been centered to VOT means at test therefore are comparable, intercepts for both test and exposure indicate log-odds of response at the same VOT value
    slope_unscaled = slope / (2 * ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test)),
    PSE_scaled =  -Intercept/slope,
    PSE_unscaled =
      descale(
        -Intercept/slope,
        VOT.mean_test,
        ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test)))
```

(ref:plot-fit-PSE) Summary of Bayesian mixed-effect psychometric analysis. **A)** Changes in listeners' categorization functions depending on exposure, from Test 1-4 and intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks). Point ranges indicate the mean proportion of participants' "t"-responses and their 95% bootstrapped CIs. Lines and shaded intervals show the *maximum a posteriori* (MAP) estimates and 95%-CIs of the psychometric model fit to participants' responses. **B)** Same as A) but for the final three test blocks without intervening exposure (Test 4 is repeated here for ease of comparison). **Panel C:** Changes across blocks and conditions in listeners' PSE of the lapse-corrected categorization functions from A) & B) (for changes in the slope of that function, see SI, \@ref(sec:io-bias-correction)). Point ranges represent the posterior medians and 95%-CIs derived from the psychometric model. Horizontal dashed lines indicate 95%-CIs of the PSEs expected from an idealized learner. Percentage labels indicate the degree of shift in PSE by participants as a proportion of the expected shift under the idealized learners (for details, see SI, \@ref(sec:shrinkage-test-all)). Horizontal gray ribbon indicates the 95%-CIs of the PSEs expected from an idealized pre-exposure listener.

\begin{landscape}

```{r}
# Create data frame for pre-exposure IO. To avoid over-fitting, we use 5-fold cross-validation.
# We use the isolated speech data from Chodroff and Wilson, and randomly cut it into 5 folds.
set.seed(920)
# d.prior <-
#   d.chodroff_wilson.isolated %>%
#   group_by(Talker, category) %>%
#   mutate(fold = sample(1:5, n(), replace = T))

# Make pre-exposure ideal observers based on Chodroff and Wilson *and* ideal observers
# that are fit on the exposure VOTs of the three exposure conditions (idealized learners).
# These ideal observers can be thought of as reflecting a learner that has fully learned
# the exposure distributions. Note that we include perceptual noise in the ideal observers
# (estimated in Kronrod et al., 2016) to make their perception more human-like.
io <-    
  bind_rows(
    # Add prior based on Chodroff & Wilson (2018). Note that this database has substantially higher
    # VOT variance than our exposure conditions, which we based on estimates from Kronrod et al.
    # (2016). Uses all three cues; duration and spectral noise taken from Kronrod et al.
    make_IOs_from_data(
      data = d.prior,
      cues = c("VOT", "f0_Mel", "vowel_duration"),
      group = "fold") %>%
      nest(io = -fold) %>%
      mutate(fold = str_c("prior", fold)) %>%
      rename(Condition.Exposure = fold) %>%
      ungroup() %>%
      # Join in the data that the pre-exposure IOs will be applied to:
      cross_join(
        d.for_analysis %>%
          filter(Phase == "test") %>%
          distinct(Condition.Exposure, Item.VOT, Item.f0_Mel, Item.VowelDuration) %>%
          mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(...))) %>%
          select(x) %>%
          nest(x = x)),
    # Add idealized learner IOs for each exposure condition
   make_IOs_from_data(
      d.for_analysis %>%
        filter(Phase == "exposure") %>%
        group_by(ParticipantID, Condition.Exposure) %>%
        nest(data = -c(ParticipantID, Condition.Exposure)) %>%
        group_by(Condition.Exposure) %>%
        # Subset data to a single participant per exposure condition. This is sufficient since
        # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics
        # (mean and SD) at the end of the experiment, which were identical across participants
        # in each condition.
        #
        # Note that this approach trains the ideal observers on the *actual* VOT distribution
        # that participants heard, not on the theoretical distributions these VOTs were sampled
        # from. This is in line with our goal to simulate behavior of an idealized participant
        # who has fully learned the exposure distributions.
        slice_sample(n = 1) %>%
        unnest(data) %>%
        mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")),
      cues = "VOT",
      groups = "Condition.Exposure") %>%
  nest(io = -c(Condition.Exposure)) %>%
      # Join in the data that the IOs will be applied to. For exposure IOs, only VOT is used for
      # categorizing stimuli (since these IOs are trained on only VOT). Here, we use the test tokens
      # to estimate PSEs for all IOs, and assume that these are constant across all exposure and
      # test blocks. In the SI, we present additional visualizations that obtain PSEs separately
      # for each block. This allows us to detect potential biases in the estimation of PSEs that
      # would result from the distribution of phonetic cues (we don't find any notable biases of
      # that form, which is why we present a simpler approach in the main text).
      left_join(
        d.for_analysis %>%  
          filter(Phase == "test") %>%
          distinct(Condition.Exposure, Item.VOT) %>%
          rename(x = Item.VOT) %>%
          nest(x = x)))

# There are two different ways to obtain PSEs from ideal observers along a specified phonetic continuum.
# One approach is to estimate the *actual* PSE (either analytically or through optimization, as implemented
# in the function get_PSE_from_IO). While this approach avoids the introduction of linearity assumptions,
# this is also its downside: we do not know *listeners' actual PSEs*, but rather estimate them in the
# psychometric mixed-effects model under the assumption of linear effects of the phonetic continuum on
# the log-odds of listeners' "t"-responses. The second approach to estimating PSEs for ideal observers
# avoids this issue by subjecting the ideal observed to the exact same estimation approach used to estimate
# listeners' PSEs. This approach also has the advantage that it allows the estimation of an intercept and
# slope (paralleling what we do for listeners' categorization function). This is the approach we take here.
#
# get_logistic_parameters_from_model() gets PSE, intercept, and slope by repeatedly sampling responses from
# the ideal observer. The function then fits an ordinary logistic regression to these responses (estimation
# of lapsing rates via a psychometric model is unnecessary since the ideal observers are *known* to have 0
# lapses). This replicates for the ideal observers any potential biases that might be introduced by the
# linearity assumption of our psychometric model.
#
# We note here that, in retrospect, our caution might be overkill since analyses in the SI suggest that the linearity
# assumption introduced only minor (non-directional) variation into the intercept, slope, and PSE estimates.
d.IO_intercept.slope.PSE <-
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io,
      model_col = "io",
      groups = "Condition.Exposure") %>%
  select(Condition.Exposure, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>%
  ungroup() %>%
  mutate(across(Condition.Exposure, factor))
```

```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_exposure, d.for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_test, d.for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(2, 4, 6), "exposure", "test")) %>%
  add_block_labels()

p.fit_1to7 <-
  cond_fit_test_exposure %>%
  filter(Block %in% c(1:7))  %>%
  ggplot() +
  geom_rect(
    data =
      tibble(
        Block.plot_label = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
        Block.colour = c("grey", "white", "grey", "white", "grey", "white", "grey")) %>%
      mutate(Block.plot_label = factor(Block.plot_label, levels = Block.plot_label, ordered = T)),
    aes(
      xmin = -Inf, xmax = Inf,
      ymin = 1.05, ymax = 1.31,
      fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "white" = "white")) +
  new_scale_fill() +
  geom_linefit(
    data = d.for_analysis %>%
      group_by(Phase, Condition.Exposure, Block.plot_label) %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE),
    x = Item.VOT,
    y = estimate__,
    fill = NA,
    legend.position = "top",
    legend.justification = "right") +
  coord_cartesian(clip = "off", ylim = c(0, 1))

p.fit_7to9 <-
  cond_fit_test_exposure %>%
  filter(Block %in% c(7:9)) %>%
  ggplot() +
  geom_linefit(
    data = d.for_analysis %>%
      group_by(Condition.Exposure, Block) %>%
      filter(Block %in% c(7:9)),
    x = Item.VOT,
    y = estimate__,
    fill = "grey",
    legend.position = "none") +
  theme(axis.title.y = element_blank())
```

```{r prepare-plot-intercepts-slopes}
# Store ideal PSEs predicted by IOs
predictedPSE_scaled_40 <- get_IO_predicted_PSE(condition = "Shift40")
predictedPSE_scaled_10 <- get_IO_predicted_PSE(condition = "Shift10")
predictedPSE_scaled_0 <- get_IO_predicted_PSE(condition = "Shift0")
predictedPSE_scaled_prior <- get_IO_predicted_PSE(condition = "prior")

# Compute proportional shifts by draw
d.psychometric_intercept.slope.PSE.median <-
  d.psychometric_intercept.slope.PSE.draws %>%
  group_by(Condition.Exposure, Block, .draw) %>%
  arrange(.draw, by_group = T) %>%
  # add the idealized learner PSEs
  mutate(
    predictedPSE_scaled =
      case_when(
        Condition.Exposure == "Shift0" ~ predictedPSE_scaled_0,
        Condition.Exposure == "Shift10" ~ predictedPSE_scaled_10,
        Condition.Exposure == "Shift40" ~ predictedPSE_scaled_40),
    predictedPSE_unscaled = descale(predictedPSE_scaled,  VOT.mean_test, VOT.sd_test)) %>%
  nest(data = -c(Condition.Exposure, .draw)) %>%
  mutate(data = map(data, ~ get_prop_shift_by_draw(.x))) %>%
  unnest(data) %>%
  group_by(Condition.Exposure, Block) %>%
  median_hdci() %>%
  add_block_labels()

pos <- position_dodge(.3)

p.across_blocks <-
  d.psychometric_intercept.slope.PSE.median %>%
  ggplot(
    aes(
      x = Block.plot_label, y = Intercept,
      ymin = Intercept.lower, ymax = Intercept.upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.6, 8.6, 1)),
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = pos, size = 1) +
  geom_linerange(linewidth = .6, position = pos, alpha = .5) +
  stat_summary(geom = "line", position = pos) +
  scale_x_discrete("Block") +
  scale_colour_manual("Condition",
    labels = c("-20ms", "-10ms", "+20ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 22.5, hjust = 1))

 p.PSE_1to9 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms VOT)") +
  aes(y = PSE_unscaled, ymin = PSE_unscaled.lower, ymax = PSE_unscaled.upper) +
  geom_rect(
    data =
      d.IO_intercept.slope.PSE %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      summarise(
        PSE.lower = mean(PSE) - sd(PSE) / sqrt(length(PSE)) * 1.96,
        PSE.upper = mean(PSE) + sd(PSE) / sqrt(length(PSE)) * 1.96),
    mapping = aes(xmin = -Inf, ymin = PSE.lower, ymax = PSE.upper, xmax = Inf),
    fill = "#d2d4dc",
    alpha = .3,
    inherit.aes = F) +
  geom_linerange(linewidth = .6, position = pos, alpha = .5) +
  geom_hline(
       data = d.IO_intercept.slope.PSE %>%
         filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")),
       mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
       linetype = 2,
       linewidth = .8,
       alpha = 0.4) +
   coord_cartesian(xlim = c(0.75, 9), clip = "off") +
   theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"), legend.position = "none")
```

```{r plot-fit-PSE, fig.width=12.5, fig.height=7, fig.cap="(ref:plot-fit-PSE)", cache=FALSE, fig.keep='all'}
p.fit_1to7 +
  p.fit_7to9 +
  (p.PSE_1to9 +
     geom_label(
       data = d.psychometric_intercept.slope.PSE.median %>% filter(Block != 1),
       mapping = aes(label = percent(prop.shift_unscaled), colour = Condition.Exposure),
       size = 3,
       position = position_dodge(.5)) +
     annotate(
       geom = "text",
       x = c(rep(10, 3)),
       y = c(25, 35, 65),
       label = d.psychometric_intercept.slope.PSE.median %>%
         mutate(true_shift = paste(round(predictedPSE_unscaled - PSE_unscaled), "(100%)")) %>%
         filter(Block == 1) %>% pull(true_shift),
       colour = colours.condition,
       fontface = "bold",
       size = 2.7) +
     annotate(
       geom = "text",
       x = c(rep(10, 3)),
       y = c(23, 33, 63),
       label = c(rep("idealized learner", 3)),
       colour = colours.condition,
       fontface = "bold",
       size = 2.7)) +
  plot_layout(
    design = "
AAAAAAAAA
BBBDDDDDD
###DDDDDD
",
heights = c(1.1, 1.1, 1.8)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

\end{landscape}

```{r}
# Not removing the PSE panel since we replot and augment it further down
rm(fit_exposure, fit_exposure_nested_within_condition_and_block, p.fit_1to7)
```

```{r hypothesis-table-simple-effects-condition, results='asis'}
hyp.simple_effects_condition <-
  get_hyp_data(
  fit_test.simple_effects_condition,
  hyp.list = c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0")
)

make_hyp_table(
  fit_test.simple_effects_condition,
  hyp.simple_effects_condition,
  c("$PSE_{-10ms} = PSE_{-20ms}$",
    "$PSE_{+20ms} = PSE_{-10ms}$",
    "$PSE_{+20ms} = PSE_{-20ms}$",
    rep(c("$PSE_{-10ms} > PSE_{-20ms}$",
          "$PSE_{+20ms}  > PSE_{-10ms}$",
          "$PSE_{+20ms} > PSE_{-20ms}$"), 5)),
    caption = "The simple effects of the exposure conditions for each test block. This analysis asks how early exposure starts to affect participants’ categorization responses, and when (if ever) these changes were undone with repeated testing. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses). Predicted nulls for Test 1 were tested using the Savage-Dickey density ratio---BF > 1 indicates increased evidence in favor of the null.") %>%
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>%
  pack_rows("Test block 3", 7, 9) %>%
  pack_rows("Test block 4", 10, 12) %>%
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>%
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)
```

Figure \@ref(fig:plot-fit-PSE)A suggests that differences between exposure conditions emerged very early in the experiment. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-condition), which we discuss next. Prior to any exposure, during Test 1, participants' responses did not differ across exposure conditions. This result is predicted by models of adaptive speech perception under the assumptions that (a) participants in the different groups have similar prior experiences, and that (b) our sample size is sufficiently large to yield stable estimates of listeners' categorization function.^[<!-- This comparatively simple procedure estimates the BF as the ratio of the posterior and prior density over the null. --> The moderate BFs for these hypothesis tests are due to our use of regularizing priors, which have non-negligible density over the null [for an introduction to the Savage-Dickey method, see @wagenmakers2010]. Rather than to test the null against more plausible alternative priors, which would predictably increase the evidence for the null, we appeal to readers' intuition: the 90%-CIs of the comparisons for Test 1 are all approximately centered around zero; there is very little evidence in favor of an effect in either direction.] Equality of pre-exposure behavior across exposure groups is also implicitly assumed---but rarely tested---in the interpretation of most studies on adaptive speech perception [when it is tested, it often turns out that this assumption is *not* necessarily warranted, presumably due to insufficient sample sizes, cf. @kleinschmidt2020].

During Test 2, after exposure to only 24 /d/ and 24 /t/ stimuli (thereof half labeled), participants' categorization responses already differed between exposure conditions (BFs > `r get_bf(fit_test.simple_effects_condition, "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`). All differences between exposure conditions that emerged at Test 2 followed prediction 2b (*exposure distributions*). Additional analyses reported in the SI (\@ref(sec:SI-exposure-block-analysis)) found that listeners' categorization functions had already changed in the predicted direction during the first *exposure* block, in line with Figure \@ref(fig:plot-fit-PSE)A. This suggests that changes in listeners' categorization responses emerged quickly at the earliest point tested---after only a fraction of exposure trials previously tested in similar paradigms.

The effects of the three exposure conditions persisted until Test 4, always in line with prediction 2b. Table \@ref(tab:hypothesis-table-simple-effects-condition) does, however, indicate an interesting non-monotonic development. While the difference between the +20ms condition and both the -20ms and -10ms condition continued to increase numerically with increasing exposure (increasingly larger magnitude of negative estimates in Tests 2-4), the same was not the case for the difference between the -10ms and the -20ms condition. Instead, the difference between the -10ms and -20ms condition *reduced* with increasing exposure (while maintaining its direction; from $\hat{\beta} =$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0", est = T)` to $\hat{\beta} =$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0", est = T)`, see Table \@ref(tab:hypothesis-table-simple-effects-condition)). At first blush, this non-monotonicity appears to contradict prediction (2a) that the magnitude of exposure effects should increase with increasing exposure. In the next section, we show that the results do, in fact, *support* prediction (2a) when listeners' prior expectations are considered. Indeed, the seemingly unexpected non-monotonicity---which would be impossible to detect without repeated testing---turns out to be important for understanding incremental adaptation.

## Incremental adaptation from prior expectations (comparing block-to-block changes within exposure conditions)
Next, we compare how listeners' categorization responses changed from block to block *within* each exposure condition. This allows us to understand changes in listeners' categorization function relative to listeners' pre-exposure behavior, thereby assessing the joint effects of predictions (1 - *prior expectations*) and (2a,b - *exposure amount & distributions*). To facilitate visual comparison across blocks and conditions, Figure \@ref(fig:plot-fit-PSE)C summarizes the block-to-block changes in listeners' PSE. Focusing for now on Tests 1-4, this highlights three aspects of participants' behavior that were not readily apparent in the statistical comparisons presented so far.

```{r fit-simple-effects-block, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift0:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift10:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift40:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_block <-
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Condition.Exposure/(Block * VOT_gs) +
      (0 + Block * VOT_gs | ParticipantID) +
      (0 + Condition.Exposure/(Block * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-block.rds")
```

```{r hypothesis-table-simple-effects-block, results='asis'}
hyp.simple_effects_block <-
  get_hyp_data(
    fit_test.simple_effects_block,
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",                     
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",

      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 + mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",

      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",

      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 + mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0"))

make_hyp_table(
  fit_test.simple_effects_block,
  hyp.simple_effects_block,
    c(rep(c(
  # Comparing differences blocks within conditions
  "Block 1 to 2: decreased PSE",
  "Block 2 to 3: decreased PSE",
  "Block 3 to 4: decreased PSE",
  "{\\em Block 1 to 4: decreased PSE}",
  "Block 4 to 5: increased PSE",
  "Block 5 to 6: increased PSE",
  "{\\em Block 4 to 6: increased PSE}"), 2),
  "Block 1 to 2: increased PSE",
  "Block 2 to 3: increased PSE",
  "Block 3 to 4: increased PSE",
  "{\\em Block 1 to 4: increased PSE}",
  "Block 4 to 5: decreased PSE",
  "Block 5 to 6: decreased PSE",
  "{\\em Block 4 to 6: decreased PSE}"),
    caption = "Was there incremental change from test block 1 to 4? Did these changes dissipate with repeated testing from block 4 to 6? This table summarizes the simple effects of block for each exposure condition. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses). Italicized rows assess incremental changes across multiple block, increasing the sensitivity to detect effects.") %>%
  pack_rows("Difference between blocks: -20ms", 1, 7) %>%
  pack_rows("Difference between blocks: -10ms", 8, 14) %>%
  pack_rows("Difference between blocks: +20ms", 15, 21)
```

First, while the PSEs for the +20ms and -10ms conditions were shifted rightwards compared to the -20ms condition, both the -10ms and the -20ms condition seem to shift *left*wards relative to their pre-exposure starting point in Test 1. Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-block) find moderate support for a leftward shift from Test 1 to 4 in both the -10ms condition (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0", bf = T)`) and the -20ms condition (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0", bf = T)`). In contrast, there was strong support that the +20ms condition shifted rightwards relative to pre-exposure (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0", bf = T)`).

To understand this pattern, it is helpful to relate the three exposure conditions to the phonetic distribution in listeners' prior experience. Figure \@ref(fig:prior-distributions-and-exposure-conditions)A shows the exposure means for /d/ and /t/ relative to the distributions of three important cues to the word-initial /d/-/t/ contrast in L1-US English. For this purpose, we chose a database of word-initial /d/s and /t/s from isolated word productions [@chodroff-wilson2018]. Compared to databases of connected speech, these isolated recordings more closely approximate the speech rate of our stimuli [which is known to affect the perception of VOT, @allen-miller1999; @utman1998]. The SI \@ref(sec:placement-of-exposure-stimuli) provides the same visualization, while also showing the distributions for `r nrow(d.chodroff_wilson.isolated)` words produced in connected speech by `r length(unique(d.chodroff_wilson.isolated$Talker))` female talkers from the same database (confirming the conclusions reached here). <!-- TFJ: this para might need some edits if we already introduce the database earlier; just some small changes that make clear that this is the same database used to develop the idealized pre-exposure listener. -->

Figure \@ref(fig:prior-distributions-and-exposure-conditions)A offers an explanation as to why the -20ms condition (and to some extent the -10ms condition) shift leftwards with increasing exposure, whereas the +20ms condition shifts rightwards: relative to the distribution of VOT for /d/ and /t/ in listeners' prior experience, only the +20ms condition presents category means that are clearly larger than expected along VOT, whereas the -20ms condition and, to some extent, the -10ms condition presented lower-than-expected category means. That is, once we take into account how our exposure conditions relate to listeners' prior experience (prediction 1), both the direction of changes from Test 1 to 4 *within* each exposure condition (Table \@ref(tab:hypothesis-table-simple-effects-block)), and the direction of differences *between* exposure conditions receive an explanation (Table \@ref(tab:hypothesis-table-simple-effects-condition)). To further illustrate this point, the horizontal gray ribbon in Figure \@ref(fig:plot-fit-PSE)C shows the range of PSEs predicted by an idealized pre-exposure listener (see Methods).

Second, we find support for prediction 3 about the *diminishing returns* of additional exposure predicted by some theories of adaptive speech perception. The estimates in Table \@ref(tab:hypothesis-table-simple-effects-block) suggest that listeners' PSEs changed most substantially from Test 1 to Test 2, and then changed less and less with additional exposure up to Test 4 (smaller magnitude of estimates compared to earlier test blocks). This seems to be particularly pronounced for the -20ms condition and the +20ms condition---the two conditions that exhibited the largest shifts relative to pre-exposure. As mentioned in our Open Science statement, our experiment was not designed to have high power to assess such *changes in the magnitude of the shifts* across the block within each condition. We did, however, conduct post-hoc hypothesis tests to assess the support for this pattern. These tests found anecdotal to moderately strong evidence in support of prediction (3 - *diminishing returns*). For the +20ms condition, the shift from Test 1 to 2 was larger than the shift from Test 2 to 3 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < mu2_Condition.ExposureShift40:Block_Test3vs.Test2")`), which was larger than the shift from Test 3 to 4 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < mu2_Condition.ExposureShift40:Block_Test4vs.Test3")`). Comparing the change from Test 1 to 2 against the change from Test 3 to 4, there was stronger support that the speed of changes in the PSE decreased (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < mu2_Condition.ExposureShift40:Block_Test4vs.Test3")`). For the -20ms condition, the shift from Test 1 to 2 was larger than the shift from Test 2 to 3 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift0:Block_Test3vs.Test2")`), which was almost identical, but slightly smaller, than the shift from Test 3 to 4 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > mu2_Condition.ExposureShift0:Block_Test3vs.Test2")`). Again, a comparison of the change from Test 1 to 2 against the change from Test 3 to 4, yielded the strongest support that the speed of changes in the PSE decreased (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift0:Block_Test4vs.Test3")`). For both the +20ms and the -20ms condition, there was only anecdotal evidence that the final exposure block resulted in *any additional* shift in listeners' PSE (BFs $\leq$ `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0", bf = T)`, cf. Table \@ref(tab:hypothesis-table-simple-effects-block)).

```{r, include=F}
# Assessing evidence for changes in the PSEs within each condition
hypothesis(
  fit_test.simple_effects_block,
  c(
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift10:Block_Test3vs.Test2",
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3"),
  robust = T)
```

Third and finally, Panel C also begins to illuminate the reasons for the non-monotonic development of the -10ms and -20ms conditions relative to each other, discussed in the previous section. In particular, this non-monotonicity does *not* appear due to a reversal of the effects in either of the two exposure conditions. Rather, both exposure conditions continue to change listeners' categorization function in the same direction from Test 1 to Test 4, in line with predictions (2a) and (2b). However, after the rapid change from the pre-exposure Test 1 to the first post-exposure Test 2, listeners' categorization responses in the -20ms condition did not change as much as in the -10ms condition. In fact, listeners' categorization function in the -20ms condition seems to have plateaued after the first exposure block.

This explains the reduction in the difference between the -10ms and -20ms conditions discussed in the previous section. It does, however, raise the question *why* listeners' responses in the -20ms condition did not change further with increasing exposure. One explanation would be that participants in the -20ms condition did for some reason---including chance---fully learn the relevant phonetic distributions within a single block of exposure, whereas participants in the -10ms condition did not. The third and final perspective we provide on incremental changes in participants' behavior suggests that this was *not* the case.

## Constraints on cumulative adaptation (comparing exposure effects against idealized learner models)

```{r hypothesis-table-convergence-test4-pre-exposure-PSE, results='asis'}
hyp.shrinkage <-
  get_hyp_data(
    fit_test_nested_within_condition_and_block,
    c(
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" )))

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$|\\Delta(PSE_{ideal_{-20ms}}, PSE_{actual_{-20ms}})| > 0$",
    "$|\\Delta(PSE_{ideal_{-10ms}}, PSE_{actual_{-10ms}})| > 0$",
    "$|\\Delta(PSE_{ideal_{+20ms}}, PSE_{actual_{+20ms}})| > 0$"),
  caption = "Did participants {\\em not} converge against the PSE expected from idealized learner? This table compares changes in participants' categorization function against those expected from idealized learners. This table summarizes results for the test block following the final exposure block (Test 4). For identical tests for all test blocks, see SI (\\ref{sec:convergence-test-all}).",
  col1_width = "22em")
```

Beyond the perspectives on incremental adaptation discussed so far, Figure \@ref(fig:plot-fit-PSE)C compares participants' responses against those of an idealized learner that has fully learned the exposure distributions (colored dashed lines). Figure \@ref(fig:plot-fit-PSE)C suggests that listeners in all three exposure conditions did *not* fully learn the exposure distributions. This was confirmed by the Bayesian hypothesis tests in Table \@ref(tab:hypothesis-table-convergence-test4-pre-exposure-PSE) ($|\Delta(PSE_{ideal}, PSE_{actual})| > 0$: all BFs $\ge 8000$). By itself, a failure to converge against the performance of an idealized learner would not necessarily constitute evidence against prediction 4 (*learn to convergence*). Listeners might simply not have received sufficient exposure to have learned the exposure distribution. However, as already described for the -20ms condition, participants' behavior changed little, if at all, after the first exposure block. Instead, participants seem to have prematurely converged against stable behavior long before they had fully learned the exposure distribution.

The percentage labels in Figure \@ref(fig:plot-fit-PSE)C quantify the degree to which participants adapted their PSE towards the statistics of the exposure condition: 0% would correspond to no change relative to the listeners' PSE in Test 1, and 100% would correspond to the PSE predicted for an idealized learner who has fully converged against the exposure distributions (see Methods). In the -20ms condition, changes in participants' PSE seem to converge against approximately `r d.psychometric_intercept.slope.PSE.median %>% filter(Condition.Exposure == "Shift0", Block == 7) %>% pull(prop.shift_scaled) %>% percent()` of what is expected from an idealized learner. A similar pattern of premature convergence is evident for the +20ms condition: changes in participants' PSEs seem to have leveled off by Test 4, despite the fact that participants' PSEs had shifted only about half way to the idealized learner's PSE. (For the -10ms condition, it is less clear whether participants had already converged against a PSE.) That is, in terms of the possible adaptation scenarios depicted in Figure \@ref(fig:predictions) in the introduction, it seems that our results most closely resemble the scenario shown in the right-most column of panel D.^[Figure \@ref(fig:plot-fit-PSE)C would also seem to suggest that the degree of convergence differed between exposure conditions. Two previous studies have observed similar differences, with more extreme exposure shifts eliciting *proportionally* smaller changes in PSEs than less extreme exposure shifts [@kleinschmidt-jaeger2016; @kleinschmidt2020]. For the present data, we found only anecdotal support for this pattern (see SI, \@ref(sec:shrinkage-test-all)).]

Of note, *premature convergence* negatively affected participants' recognition accuracy. As shown in Figure \@ref(fig:IO-human-accuracy) while incremental adaptation substantially improved participants' recognition accuracy compared to their pre-exposure accuracy, only participants in the -10ms condition---the exposure condition that deviates the least from listeners' prior expectations---came close to achieving the theoretical upper bound expected of an idealized learner. Listeners in the -20ms and +20ms condition appear to have stopped adapting even though further adaptation would have improved their recognition accuracy.^[While a failure to improve from, say, 90% and 95% accuracy might not seem noteworthy, it implies misunderstanding one in ten vs. one in twenty words, thus *doubling* the odds of successful recognition.]

(ref:IO-human-accuracy) Changes across blocks and conditions in participants' recognition accuracy for the unfamiliar talker's speech. For each block, we used participants' categorization functions---estimated by the psychometric mixed-effects model fit to participants' responses---to categorize all 144 exposure inputs of that exposure condition. Accuracy was calculated for the two decision rules that are most commonly assumed to underlie speech recognition as well as perceptual decision-making in other domains [for review, see @massaro-friedman1990]: Luce's choice rule (responding proportional to posterior probability of category) and the criterion choice rule (always responding with the category that has highest posterior probability). As in Figure \@ref(fig:plot-fit-PSE)C, point ranges represent the posterior medians and their 95%-CIs derived from the psychometric model. Horizontal dashed lines indicate accuracy expected from an idealized learner (an ideal observer model that has fully learned the exposure distributions) and horizontal shaded ribbons indicate the 95%-CI expected from an idealized pre-exposure listener.

```{r compute-IO-human-accuracy}
# The data on which to calculate accuracy: one full set of exposure trials from each condition
d.x <-
  d.for_analysis %>%
  filter(Phase == "exposure") %>%
  nest(data = -c(ParticipantID, Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  # get 1 set of exposure trials per condition
  slice_sample(n = 1) %>%
  unnest(data) %>%
  reframe(Item.VOT, Item.f0_Mel, vowel_duration, category) %>%
  rename(VOT = Item.VOT, f0_Mel = Item.f0_Mel, intended.category = category) %>%
  # Nest the cues into one column. Store versions of only VOT and all three cues
  # since the pre-exposure and idealized learner IOs need different inputs
  mutate(
    x.VOT = pmap(list(VOT), ~ c(...)),
    x.all = pmap(list(VOT, f0_Mel, vowel_duration), ~ c(...)),
    data_from = Condition.Exposure) %>%
  select(c(data_from, intended.category, x.VOT, x.all))

# Get accuracy for idealized learner IOs on shift conditions they were trained on, and
# Get accuracy for pre-exposure IO for all three conditions.
io.accuracy <-
  # First, we get pre-exposure accuracies
  bind_rows(
    io %>%
      select(-x) %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      crossing(evaluate_on = c("Shift0", "Shift10", "Shift40")) %>%
      left_join(
        d.x %>%
          rename(x = x.all) %>%
          select(-x.VOT) %>%
          nest(x = c(x, intended.category)),
        by = join_by(evaluate_on == data_from)),
    io %>%
      select(-x) %>%
      filter(!str_starts(Condition.Exposure, "prior")) %>%
      mutate(evaluate_on = Condition.Exposure) %>%
      left_join(
        d.x %>%
          rename(x = x.VOT) %>%
          select(-x.all) %>%
          nest(x = c(x, intended.category)),
        by = join_by(evaluate_on == data_from))) %>%
  # Get IO categorisation with both decision rules
  mutate(
    categorization.proportional =
      map2(x, io, ~
             get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>%
             filter(category == "/t/")),
    categorization.criterion =
      map2(x, io, ~
             get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "criterion") %>%
             filter(category == "/t/"))) %>%
  unnest(c(x, starts_with("categorization")), names_sep = "_") %>%
  select(!ends_with(c("observationID", "_x", "_category"))) %>%
  # For all items that are not intended to be /t/, take 1 minus the posterior of /t/
  mutate(
    categorization.proportional_response = ifelse(x_intended.category == "/t/", categorization.proportional_response, 1 - categorization.proportional_response),
    # where criterion rule responds 1 that is a /t/
    categorization.criterion_response = ifelse(categorization.criterion_response == 1, "/t/", "/d/"),
    categorization.criterion_response = ifelse(categorization.criterion_response == x_intended.category, T, F)) %>%
  group_by(Condition.Exposure, evaluate_on) %>%
  summarise(across(c(categorization.proportional_response, categorization.criterion_response), mean)) %>%
  pivot_longer(
    cols = starts_with("categorization"),
    names_to = "decision",
    values_to = "accuracy") %>%
  mutate(decision = factor(str_replace(decision, "categorization\\.(.*)_response", "\\1"), levels = c("proportional", "criterion")))  

# get human accuracy on unlabelled exposure trials
# join with predictions of the fitted psychometric model
fit_exposure <- readRDS("../models/exposure-standard-priorSD15-0.999.rds")
fit_test <- readRDS("../models/test-standard-priorSD15-0.995.rds")


human_psychometric_accuracy <-
  d.for_analysis %>%
  filter(Phase == "exposure" & Item.Labeled == F) %>%
  group_by(ParticipantID, Condition.Exposure, Block) %>%
  summarise(
    participant.accuracy = mean(Response.Correct),
    mean.accuracy = mean(participant.accuracy)) %>%
  group_by(Condition.Exposure, Block) %>%
  summarise(
    lower = quantile(mean.accuracy, probs = .025),
    median = quantile(mean.accuracy, probs = .5),
    upper = quantile(mean.accuracy, probs = .975)) %T>%
  { . ->> temp } %>% mutate(decision = factor("proportional")) %>%
  bind_rows(temp %>% mutate(decision = factor("criterion"))) %>%
  ungroup() %>%
  mutate(decision = fct_relevel(decision, "proportional")) %>%
  mutate(model = factor("human")) %>%
  full_join(
    crossing(
      Condition.Exposure = c("Shift0", "Shift10", "Shift40"),
      Block = c(1, 3, 5, 7),
      lower = NA,
      median = NA,
      upper = NA)) %>%
  add_block_labels() %>%
  full_join(
    get_pyschometric_accuracy(fit_exposure, d.for_analysis, "exposure", VOT.sd_exposure) %>%
  bind_rows(
    get_pyschometric_accuracy(fit_test, d.for_analysis, "test", VOT.sd_test)) %>%
  mutate(decision = factor(decision)) %>%
  ungroup() %>%
  mutate(decision = fct_relevel(decision, "proportional", "criterion")) %>%
  add_block_labels() %>%
    mutate(model = factor("psych"))) %>%
  group_by(decision, Condition.Exposure) %>%
  arrange(Block, .by_group = T)

rm(fit_exposure)
```

```{r IO-human-accuracy, fig.width=7.5, fig.height=base.height*2 + .75, fig.cap="(ref:IO-human-accuracy)", fig.pos="!ht"}
# store plot for plotting in SI
p.human_psych_accuracy <-
  human_psychometric_accuracy %>%
  na.omit() %>%
  mutate(model = fct_relevel(model, "psych")) %>%
  rename(evaluate_on = Condition.Exposure) %>%
  ggplot(
    aes(
      x = Block.plot_label, y = median,
      ymin = lower, ymax = upper,
      colour = evaluate_on,
      shape = model,
      group = decision)) +
  geom_hline(
    data = io.accuracy %>% filter(!str_starts(Condition.Exposure, "prior")),
    mapping = aes(yintercept = accuracy, color = evaluate_on),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4,
    show.legend = F) +
  geom_rect(
    data = io.accuracy %>%
      filter(Condition.Exposure %in% paste0("prior", c(1:5))) %>%
      group_by(evaluate_on, decision) %>%
      summarise(
        ymin = mean(accuracy) - sd(accuracy) / sqrt(length(accuracy)) * 1.96,
        ymax = mean(accuracy) + sd(accuracy) / sqrt(length(accuracy)) * 1.96),
    mapping = aes(xmin = -Inf, ymin = ymin, ymax = ymax, xmax = Inf),
    fill = "#d2d4dc",
    alpha = 0.3,
    inherit.aes = F) +
  geom_pointrange(position = position_dodge2(.5), size = .4) +
  stat_summary(
    data =
      human_psychometric_accuracy %>%
      filter(model == "psych") %>%
      rename(evaluate_on = Condition.Exposure),
    geom = "line", position = position_dodge2(.5)) +
  labs(x = "Block") +
  scale_y_continuous("Accuracy\n(of psychometric model)", breaks = c(.6, .8, 1)) +
  scale_colour_manual("Condition", values = colours.condition, labels = c("-20ms", "-10ms", "+20ms"), guide = "none") +
  scale_shape_manual("Accuracy", values = c(16, 17), labels = c("psychometric model", "human", "")) +
  coord_cartesian(ylim = c(.6, 1)) +
  facet_grid(
    decision ~ evaluate_on,
    labeller = labeller(
      decision = c("proportional" = "Luce's\ndecision rule", "criterion" = "Criterion\ndecision rule"),
      evaluate_on = c("Shift0" = "-20ms", "Shift10" = "-10ms", "Shift40" = "+20ms"))) +
  guides(shape = "none") +
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))

# plot psych model accuracy
p.human_psych_accuracy %+%
  subset(human_psychometric_accuracy %>%
           filter(model == "psych") %>%
           rename(evaluate_on = Condition.Exposure) %>%
  group_by(decision, evaluate_on) %>%
  arrange(Block, .by_group = T))
```

If confirmed, premature convergence against stable behavior despite only partial adaptation would challenge prediction 4 of existing distributional learning models. Premature convergence is also unexpected under any other model of adaptive speech perception. In the general discussion, we present an extension to distributional learning models that explains premature convergence, and highlights a striking link between adaptive changes in speech perception and second language learning. As part of that discussion, we present additional models, and entertain methodological artifacts, and analysis confounds that would offer alternative explanation of premature convergence.

## Effects of repeated testing
The final hypothesis tests investigate the effects of repeated testing. Distributional learning models predict that test stimuli, too, can form part of the input that listeners adapt to. To the extent that the information provided by test stimuli differs from that provided by exposure stimuli, these theories thus predict that repeated testing affects listeners' behavior. And, in a design like ours, with identical test stimuli across conditions, the effects of repeated testing are predicted to differ across conditions. Specifically, with sufficient repetition all conditions would be expected to converge against the distribution of the test stimuli, and thus towards identical behavior across conditions. Theories of error-based learning and ideal information integration further predict that the speed with which repeated testing changes listeners' behavior depends on the degree to which the distribution of test stimuli differs from the distribution of exposure stimuli [@davis-sohoglu2020; @kleinschmidt-jaeger2015; for relevant discussion, see also @lancia-winter2013; @xie-kurumada2024].

In line with these theories, Figure \@ref(fig:plot-fit-PSE)C shows that the effects of exposure reduced from Test 4 to Test 6, and did so primarily for the exposure conditions that differed most from the distribution of test stimuli. In Table \@ref(tab:hypothesis-table-simple-effects-block), this is evident in a reversal of the direction of the block-to-block changes for Tests 5-6, compared to Tests 1-4. For the +20ms exposure condition, these block-to-block changes went from rightward shifts in Tests 1-4 to leftward shifts in Tests 5-6 (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 + mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0", bf = T)`). For the -20ms condition, block-to-block changes went from leftward to rightward shifts (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0", bf = T)`). The only exposure condition for which no clear reversal was observed is the -10ms condition (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0", bf = T)`). As would be expected under theories of error-based learning and ideal information integration, the marginal distribution of VOT during test blocks (mean = 35.8 ms, SD = 22.2 ms) most closely resembled the exposure distribution of the -10ms condition (mean = 36.5, SD = 25.9), compared to the -20ms (mean = 26.5 ms, SD = 25.9) or +20ms condition (mean = 66.5 ms, SD = 25.9).

The effects of repeated testing replicate previous findings from lexically-guided perceptual learning paradigms [LGPL, @scharenborg-janse2013; @giovannone-theodore2021; @cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @reinisch-holt2014; @zheng-samuel2023], and extends them to distributional learning paradigms [see also @colby2018; @kleinschmidt2020]. Indeed, the effects of repeated testing can be substantial: while the effects of the +20ms condition relative to the other two exposure conditions were reduced but still credible even in Test 6 (BFs > `r get_bf(fit_test.simple_effects_condition, "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`), this was no longer the case for the effect of the -10ms condition relative to the -20ms condition (BF = `r get_bf(fit_test.simple_effects_condition, "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`; see Table \@ref(tab:hypothesis-table-simple-effects-condition)). One important methodological implication for future work is that longer test phases do not necessarily increase the statistical power to detect effects of adaptation [unless analyses account for the effects of repeated testing, as done in, e.g., @liu-jaeger2018]. <!-- TO DO: add @cummings2024 once made available online (pen in mouth paper) --> Analyses that average over all test tokens---as is still the norm---are bound to systematically underestimate the true adaptivity of human speech perception.<!-- ^[@kraljic-samuel2006 is sometimes cited as finding LGPL exposure effects even after 480 test trials over a uniform test continuum. This is, however, misleading. Kraljic and Samuel used four *different* uniform test continua over two different phonetic contrasts (/b/-/p/ and /d/-/t/). Each test session consisted of 10 randomized repetitions of 6 test trials. Kraljic and Samuel never tested (or made any claims about) whether exposure effects were still detectable during the 10th repetition. Rather they report *average* effects across the 10 repetitions (like other LGPL studies), which is perfectly compatible with the hypothesis that repeated testing reduces the effects of exposure [see @liu-jaeger2018].] -->

## Modelling the learning effects with an ideal adaptor

```{r fit-IA-inferred-VOT-f0}
d_for_ASP <-
  d.for_analysis %>%
  ungroup() %>%
  # Exclude catch trials and the final two test blocks since we expect unlearning during those blocks
  # (which is not modeled)
  filter(
    !(Phase == "exposure" & is.na(Item.ExpectedResponse.Voicing)),
    as.numeric(Block) <= 7) %>%
  # Use F0 as intended by generation script. This makes fitting more computationally feasible
  # (336 instead of 1001 unique combinations of group & test locations), and also removes the
  # potential that annotation / measurement error perturb the f0 values. It does, however,
  # make the assumption that f0 is the same across the three minimal pairs. While likely wrong,
  # this assumption better aligns with the fact that the model doesn't have a way to keep track
  # of any lexical, phonological, or phonetic context effects on VOT (and thus no way to predict
  # any potential differences between minimal pairs).
  # Create new condition variable that uniquely identifies what participants have seen at any of the
  # tests and a new block variable that identifies the type and order of blocks. Also create two
  # convenience variables, x (stimulus) and Response.Category (to make matching with the category
  # column of ideal observers/adaptors more straightforward).
  mutate(
    Condition.for_ASP = paste0(Condition.Exposure, List.ExposureBlockOrder, List.ExposureMaterials),
    Block.for_ASP = paste0(Phase, Block),
    x = pmap(.l = list(VOT, f0_Mel, vowel_duration), ~ c(...)),
    Response.Category = factor(ifelse(Response.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/")),
    Item.ExpectedResponse.Category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/"))) %>%
  slice_into_unique_exposure_test(
    condition_var = "Condition.for_ASP",
    block_var = "Block.for_ASP",
    block_order = c("test1", "exposure2", "test3", "exposure4", "test5", "exposure6", "test7")) %>%
  mutate(ExposureGroup = factor(ExposureGroup)) %>%
  select(
    ExposureGroup, Phase, ParticipantID, Block, Trial, x,
    VOT, f0_Mel, vowel_duration, Response.Category, Item.ExpectedResponse.Category)

# fit ideal adaptor with uninformative priors
# use prior roughly in middle of 5-fold CV
m_IO.VOT_f0_vowel <-
  io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%
  select(-c(mu, Sigma)) %>%
  filter(Condition.Exposure == "prior1") %>%
  mutate(Condition.Exposure = "CV_prior.mean") %>%
  right_join(
    io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%  
  unnest_wider(mu) %>%
    group_by(category) %>%
  summarise(across(c(VOT, f0_Mel, vowel_duration), mean)) %>%
  mutate(mu = pmap(.l = list(VOT, f0_Mel, vowel_duration),
    # Ensure names else it won't be recognised as an MVG IO
    ~ set_names(c(...), c("VOT", "f0_Mel", "vowel_duration")))) %>%
  select(category, mu)) %>%
  right_join(
    io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%
  select(Condition.Exposure, category, Sigma) %>%
  group_by(category) %>%
  summarise(Sigma = list(reduce(Sigma, `+`)/length(Sigma))) %>%
  group_by(category))


# fit ideal adaptor with 3 cue priors
m_IA.VOT_f0_vowelduration_informedprior <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = m_IO.VOT_f0_vowel$mu,
    Sigma_0 = map2(m_IO.VOT_f0_vowel$Sigma, m_IO.VOT_f0_vowel$Sigma_noise, `+`),
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_informative_priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15),
    file_refit = "on_change")
```

```{r summarise-PSE-predictions-of-IA}
# Get logistic model predictions of the IA's categorisation
# then prepare the data for plotting
if (file.exists("../models/d.IA_predicted_test_informedprior.rds")) {
  d.IA_predicted_test_informedprior <- readRDS("../models/d.IA_predicted_test_informedprior.rds")
} else {
  data.test <- prep_data_for_IBBU_prediction(m_IA.VOT_f0_vowelduration_informedprior)

  d.IA_predicted_test_informedprior <-
    get_IBBU_predicted_response(
      m_IA.VOT_f0_vowelduration_informedprior,
      data = data.test,
      groups = get_group_levels_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior))

  saveRDS(d.IA_predicted_test_informedprior, "../models/d.IA_predicted_test_informedprior.rds")
}

if (file.exists("../models/d.IA_predicted_PSE_informedprior.rds")) {
  d.IA_predicted_PSE_informedprior <- readRDS("../models/d.IA_predicted_PSE_informedprior.rds")
} else {
  d.IA_predicted_PSE_informedprior <-
    fit_logistic_regression_to_model_categorization(d.IA_predicted_test_informedprior) %>%
    group_by(group) %>%
    median_hdci(PSE) %>%
    # duplicate the no-exposure row; 1 for each condition
    mutate(rows = ifelse(group == "no exposure", 3, 1)) %>%
    uncount(rows) %>%
    mutate(
      Condition.Exposure = factor(c(rep("Shift0", 3), rep("Shift10", 3), rep("Shift40", 3), c("Shift0", "Shift10", "Shift40"))),
      Block = ifelse(group != "no exposure", str_replace(group, ".*(\\d)", "\\1"), group),
      Block = factor(ifelse(group == "no exposure", 1, Block))) %>%
    add_block_labels()

  saveRDS(d.IA_predicted_PSE_informedprior, "../models/d.IA_predicted_PSE_informedprior.rds")
}
```

(ref:plot-IA-human-PSE) Summary of ideal adaptor analysis, comparing shifts in listeners' PSEs against the shifts expected by an ideal adaptor [a distributional learning model that describes ideal information integration, @kleinschmidt-jaeger2015]. Point ranges and dashed horizontal lines are identical to those in Figure \@ref(fig:plot-fit-PSE)C. Colored ribbons indicate the 95%-CI for the PSEs predicted by the ideal adaptor fit to listeners' responses. The vertical bar to the right of the panel indicates the range of talker-specific PSEs an idealized listener might have experienced in previous exposure: the mean and 2.5%-to-97.5% quantile range of talker-specific PSEs derived from Bayesian ideal observers fit to all talkers in the phonetic database of isolated word productions presented in Figure \@ref(fig:prior-distributions-and-exposure-conditions)A. We return to this range when discussing possible explanations for mismatches between the ideal adaptor and participants' responses.

```{r get-talker-specific-PSEs}
# Compute distribution of talker-specific PSEs from Chodroff & Wilson (2018) database against which to compare human behavior
prior.PSE_mean <-
  d.IO_intercept.slope.PSE %>%
  filter(Condition.Exposure %in% paste0("prior", c(1:5))) %>%
  summarise(mean = mean(PSE)) %>%
  pull(mean)

# As mentioned in the result section, there are two different ways to obtain PSEs from ideal observers.
# Here we provide the code for both but continue to use the same method as in the result section. This
# is the method that introduces the same potential biases into the estimates of the ideal observers that
# *might* affect our estimates of listeners' PSE, intercept, and slope.
d.talkerPSEs <-
  # Make separate ideal observer for each talker in the database
  make_IOs_from_data(
    data = d.chodroff_wilson,
    cues = c("VOT", "f0_Mel", "vowel_duration"),
    groups = c("speechstyle", "Talker", "gender")) %>%
  nest(io = -c(speechstyle, Talker, gender)) %>%
  # Get actual PSE of IO
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  rename(PSE_actual = PSE) %>%
  select(-c(x, categorization)) %>%
  # Get PSE estimated through logistic regression
  cross_join(
    d.for_analysis %>%  
      filter(Phase == "test") %>%
      distinct(Item.VOT, Item.f0_Mel, Item.VowelDuration) %>%
      mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(...))) %>%
      select(x) %>%
      nest(x = x)) %>%
  left_join(
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = .,
      model_col = "io",
      groups = "Talker") %>%
      select(Talker, PSE) %>%
      rename(PSE_logistic = PSE)) %>%
  # For each of the two types of PSE, also create a version that is centered around
  # listeners mean PSE during Test 1 (to make it easier to compare the range of variability)
  group_by(speechstyle) %>%
  mutate(across(starts_with("PSE"), list(centered = ~ .x - (mean(.x) - prior.PSE_mean))))

# Visually investigate the distribution of PSE before aggregating them down further
# d.talkerPSEs %>%
#   select(c(speechstyle, starts_with("PSE"))) %>%
#   pivot_longer(
#     cols = starts_with("PSE"),
#     names_to = "PSE_type",
#     values_to = "PSE") %>%
#   # Filter implausible PSEs (there's one talker with a < - 100 PSE)
#   filter(PSE > 0) %>%
#   ggplot(aes(x = PSE_type, y = PSE, color = speechstyle)) +
#   geom_violin(position = pos) +
#   stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = pos) +
#   geom_point(position = "jitter", alpha = .5)

d.talkerPSEs %<>%
  summarise(
    across(
      starts_with("PSE"),
      list(
        mode = ~ modeest::mlv(.x, method = "meanshift"),
        mean = mean,
        median = median,
        lower = ~ quantile(., probs = .025),
        upper = ~ quantile(., probs = .975)),
      .names = "{.col}.{.fn}"))

# d.talkerPSEs %>%
#   pivot_longer(
#     cols = starts_with("PSE"),
#     names_to = "PSE_type",
#     values_to = "PSE") %>%
#   separate(PSE_type, into = c("PSE_type", "stat"), sep = "\\.") %>%
#   pivot_wider(names_from = stat, values_from = PSE) %>%
#   ggplot(aes(x = PSE_type, y = mean, ymin = lower, ymax = upper, color = speechstyle)) +
#   geom_pointrange(position = pos)
```

```{r plot-IA-human-PSE, fig.width=6.5, fig.height=3.5, fig.cap="(ref:plot-IA-human-PSE)", fig.pos="ht"}
pos <- position_dodge(.3)
d.psychometric_intercept.slope.PSE.median %>%
  filter(Block %in% c(1:7)) %>%
  ggplot(
    aes(
      x = Block.plot_label,
      y = PSE_unscaled,
      ymin = PSE_unscaled.lower, ymax = PSE_unscaled.upper,
      colour = Condition.Exposure,
      group = Condition.Exposure)) +
  geom_point(position = pos, size = 1.8) +
  geom_linerange(linewidth = .6, position = pos, alpha = .5) +
  stat_summary(geom = "line", position = pos) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2)),
      xmax = c(seq(1.5, 7.5, 2))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1,
    inherit.aes = F) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_ribbon(
    data = d.IA_predicted_PSE_informedprior,
    mapping = aes(x = Block.plot_label, ymin = .lower, ymax = .upper, fill = Condition.Exposure, group = Condition.Exposure),
    alpha = .1,
    position = pos,
    inherit.aes = F) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_hline(
    data = d.IO_intercept.slope.PSE %>% filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")),
    mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4) +
  annotate(
    geom = "text",
    data = d.psychometric_intercept.slope.PSE.median %>%
      mutate(true_shift = predictedPSE_unscaled - PSE_unscaled) %>%
      filter(Block == 1) %>% pull(true_shift),
      x = c(8, 8, 8),
    y = c(25, 35, 65),
      label = d.psychometric_intercept.slope.PSE.median %>%
      mutate(true_shift = paste(round(predictedPSE_unscaled - PSE_unscaled), "(100%)")) %>%
      filter(Block == 1) %>% pull(true_shift),
    colour = colours.condition,
    fontface = "bold",
    size = 2.7) +
  annotate(
    "crossbar",
    x = 7.7,
    y = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.mean,
    ymin = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.lower,
    ymax = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.upper,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  labs(y = "PSE (ms VOT)", x = "Block") +
  scale_colour_manual(
    "Condition",
    labels = c("-20ms", "-10msms", "+20ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  coord_cartesian(xlim = c(0.75, 7), clip = "off") +
  theme(
    axis.text.x = element_text(angle = 22.5, hjust = 1),
    plot.margin = unit(c(1, 3.2, 1, 1), "lines"),
    legend.position = "none")
```

For this post-hoc test, we fit a distributional learning model to listeners' data. Specifically, we used an ideal adaptor model [@kleinschmidt-jaeger2015; @kleinschmidt-jaeger2016]. The ideal adaptor defines a model of ideal information integration, updating prior beliefs about phonetic representations based on the exposure data. Like the other theoretical frameworks mentioned above, this model implements the hypothesis of model *learning*. Univariate instances of this model have previously achieved high accuracy in modeling changes in listeners categorization functions after LGPL/VGPL [for /s/-/`r linguisticsdown::cond_cmpl("ʃ")`/, @cummings-theodore2023; /b/-/d/, @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012] or DL exposure [for /b/-/p/, @kleinschmidt2020; @kleinschmidt-jaeger2016; /g/-/k/, @theodore-monto2019]. We extended the model to multivariate categories over VOT, f0, and vowel duration, and fit it to listeners' responses in our experiment (for a graphical model and details about model fitting, see SI \@ref(sec:ideal-adaptor)).

```{r compute-R2-between-ideal-adaptor-and-psychometric-model}
psych.model_prediction <-
  epred_draws(
    fit_test,
    newdata = d.for_analysis %>%
      filter(Phase == "test") %>%
      group_by(Item.VOT, Block, Condition.Exposure) %>%
      prepVars(test_mean = VOT.mean_test, levels.Condition = levels_Condition.Exposure) %>%
      filter(Block %in% c(1, 3, 5, 7)),
    seed = 928,
    re_formula = NULL,
    ndraws = 1000) %>%
  ungroup() %>%
  # Collapse all observations in Test 1 into 1 condition
  # (to parallel how the ideal adaptor treats the data)
  mutate(
    Condition.Exposure = as.character(Condition.Exposure),
    Condition = ifelse(Block == 1, "pre-exposure", Condition.Exposure),
    Condition.Exposure = factor(Condition.Exposure)) %>%
  group_by(Item.VOT, Condition.Exposure, Condition, Block) %>%
  summarise(predicted_proportion_voiceless = mean(.epred)) %>%
  mutate(predicted_proportion_voiced = 1 - predicted_proportion_voiceless) %>%
  crossing(List.ExposureBlockOrder = c("A", "B", "C")) %>%
  left_join(d.for_analysis %>%
  filter(Phase == "test", Block %in% c(1, 3, 5, 7)) %>%
  group_by(Item.VOT, List.ExposureBlockOrder, Block, Condition.Exposure) %>%
  mutate(Block = factor(Block)) %>%  
  summarise(
    proportion_voiceless = mean(Response.Voiceless),
    proportion_voiced = 1 - proportion_voiceless)) %>%
  group_by(Condition) %>%
  mutate(List = ifelse(Condition == "pre-exposure", "A", List.ExposureBlockOrder)) %>%
  group_by(Item.VOT, List, Block, Condition) %>%
  summarise(across(c(predicted_proportion_voiceless, predicted_proportion_voiced, proportion_voiceless, proportion_voiced), mean))

IA.prediction <-
  d.IA_predicted_test_informedprior %>%
  group_by(group, VOT) %>%
  summarise(across(Predicted_posterior, mean)) %>%
  mutate(ExposureGroup = ifelse(group == "prior", "no exposure", group)) %>%
  left_join(get_test_data_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior) %>%
              get_untransform_function_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior)(.)) %>%
  mutate(
    Response = `/t/` / (`/d/` + `/t/`),
    group = gsub("[ABC]A", "", group))
```

Unlike the psychometric model we used for analysis, a distributional learning model specifies how information is integrated across exposure trials and blocks. As a consequence, listeners' responses during different test blocks and across different exposure conditions jointly constrain the parameters of model (due to assumption of ideal information integration within and across exposure blocks). This substantially decreases the degrees of freedom the model has to fit listeners' responses. For example, the ideal adaptor we fit to listeners' responses in the first four test blocks has a total of 3 parameters (lapse rate, and confidence in the prior means and covariance matrices of /d/ and /t/; for details, see SI \@ref(sec:ideal-adaptor)), compared to `r sum(str_detect(rownames(fixef(fit_test)), "mu2")) - sum(str_detect(rownames(fixef(fit_test)), "Test5vs.Test4|Test6vs.Test5"))` population-level ('fixed') and `r length(get_variables(fit_test)[grep("^sd_|^cor_", get_variables(fit_test))]) - sum(str_detect(get_variables(fit_test)[grep("^sd_|^cor_", (get_variables(fit_test)))], "Block_Test6vs.Test5|Block_Test5vs.Test4"))` group-level ('random') effect parameters for the psychometric mixed-effect model fit to the same data.


```{r data-prep-IA-logodds-prediction-human-responses}
# prep data to obtain IA predicted logodds by condition and VOT up to test block 4
if (file.exists("../models/d.IA_predicted_logodds.csv")) {
  d.IA_predicted_logodds <- read_csv("../models/d.IA_predicted_logodds.csv", show_col_types = F)
} else {

data.test <-
  prep_data_for_IBBU_prediction(m_IA.VOT_f0_vowelduration_informedprior) %>%
  mutate(
    # make additional rows for test block 1 so that there is 1 for each condition
    rows = ifelse(group == "no exposure", 3, 1)) %>%
  uncount(rows) %>%
  mutate(
    Condition.Exposure = factor(c(rep("Shift0", 9), rep("Shift10", 9), rep("Shift40", 9), c("Shift0", "Shift10", "Shift40"))),
    Block = ifelse(group != "no exposure", str_replace(group, ".*(\\d)", "\\1"), group),
    Block = factor(ifelse(group == "no exposure", 1, Block)))

# get the IA predicted logodds and join with the test data frame
d.IA_predicted_logodds <-
  get_IBBU_predicted_response(
      m_IA.VOT_f0_vowelduration_informedprior,
      data = data.test,
      groups = get_group_levels_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior),
      logit = T) %>%
  group_by(VOT, Condition.Exposure, Block) %>%
  # get the modal posterior draw
  mode_hdci(Predicted_posterior) %>%
  rename(Item.VOT = VOT,
         IA_predicted_logodds = Predicted_posterior) %>%
  right_join(d.for_analysis %>%
               mutate(Block = factor(Block)),
             by = c("Item.VOT", "Condition.Exposure", "Block")) %>%
  filter(Block %in% c(1, 3, 5, 7)) %>%
  group_by(Item.VOT) %>%
  arrange(Item.VOT) %>%
  mutate(IA.logodds_prior = first(IA_predicted_logodds),
         IA.logodds_change = IA_predicted_logodds - IA.logodds_prior) %>%
  group_by(Condition.Exposure, Block) %>%
  # this separates the change in logodds by block into intercept changes, and slopes
  mutate(
     IA.logodds_changes_in_intercept_across_block_condition = mean(IA.logodds_change),
    IA.logodds_changes_in_slope_across_block_condition = IA.logodds_change - IA.logodds_changes_in_intercept_across_block_condition) %>%
  droplevels()

write_csv(d.IA_predicted_logodds, "../models/d.IA_predicted_logodds.csv")
}
```


```{r psychometric-model-block-1to4}
# fit a reduced psychometric model of human data (test blocks 1 to 4) for LOO comparability with the IA_predicted logodds model
fit_test.upto.test4 <-
  fit_model(
    d.IA_predicted_logodds,
    phase = "test",
    priorSD = 15,
    formulation = "standard-upto-test4",
    adapt_delta = .995
  )
fit_test.upto.test4 <- add_criterion(fit_test.upto.test4, criterion = "loo", moment_match = TRUE)
```


```{r fit-IA-predicted-logodds-human-responses}
# fit linear model to human responses with IA-predicted-logodds as predictor
# to assess how well IA predictions fit listeners' behaviour and how it changes from block to block
# block is treated as linear
d.IA_predicted_logodds %<>%
  mutate(Block = as.numeric(
    case_when(
      Block == "1" ~ 1,
      Block == "3" ~ 2,
      Block == "5" ~ 3,
      Block == "7" ~ 4)),
    Condition.Exposure = factor(Condition.Exposure))

# scale predictors
d.IA_predicted_logodds %<>%
  mutate(
    across(c(IA.logodds_change,
             IA.logodds_prior,
             IA.logodds_changes_in_intercept_across_block_condition,
             IA.logodds_changes_in_slope_across_block_condition,
             Block,
             VOT), ~ gs_scale(.x), .names = "{.col}_gs"))

# use new priors for the predicted logodds model since it is not a mixture but a plain logistic regression model
priors_logodds_model <-
  c(
  prior(student_t(3, 0, 2.5), class = "b"),
  prior(cauchy(0, 2.5), class = "sd"))

fit_test.IA_predicted_logodds <-
  brm(
    bf(
      Response.Voiceless ~ 1 + IA_predicted_logodds +
        (1 | ParticipantID) + (1 | Item.MinimalPair)),
    data = d.IA_predicted_logodds,
    prior = priors_logodds_model,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 4000,
    warmup = 2000,
    sample_prior = "yes",
    save_pars = save_pars(all = TRUE),
    family = bernoulli("logit"),
    control = list(adapt_delta = .999),
    file ="../models/test_IA_predicted_logodds.rds",
    file_refit = "on_change")

fit_test.IA_predicted_logodds <- add_criterion(fit_test.IA_predicted_logodds, criterion = c("loo"), moment_match = T)

fit_test.IA_predicted_logodds.r_slopes <-
  brm(
    bf(
      Response.Voiceless ~ 1 + IA_predicted_logodds +
        (1 + IA_predicted_logodds | ParticipantID) + (1 | Item.MinimalPair)),
    data = d.IA_predicted_logodds,
    prior = priors_logodds_model,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 4000,
    warmup = 2000,
    sample_prior = "yes",
    save_pars = save_pars(all = TRUE),
    family = bernoulli("logit"),
    control = list(adapt_delta = .9995),
    file ="../models/test_IA_predicted_logodds.r_slopes.rds",
    file_refit = "on_change")
```
The results are shown in Figure \@ref(fig:plot-IA-human-PSE). For the present data, the ideal adaptor model predicts the proportion of listeners' "t"-responses with a high $R^2 =$ `r round(cor(IA.prediction$Predicted_posterior, IA.prediction$Response) %>% . ^2, 3) * 100`$\%$, comparable to that of the psychometric model ($R^2 =$ `r round(cor(psych.model_prediction$predicted_proportion_voiceless, psych.model_prediction$proportion_voiceless) %>% . ^2, 3) * 100`$\%$). This suggests that the ideal adaptor provides a decent explanation for participants' behavior with substantially fewer degrees of freedom (for additional visualizations of the fit against participants' behavior, see SI, \@ref(sec:ideal-adaptor)). To more directly compare the fit of the ideal adaptor learning model to the psychometric data analysis model, we estimated the pairwise difference in the two models' expected log-predictive density.^[The (log) predictive density can be thought of as the Bayesian equivalent of the (log) data likelihood in frequentist statistics (but the Bayesian approach incorporates prior beliefs). The *expected* log predictive density (ELPD) is the mean log posterior density of the data, estimated here using leave-one-out (with moment matching) through the `add_criterion()` function of `brms` [@R-brms_a]. This ELPD estimate provides a measure of the predictive accuracy of the model, essentially estimating how predictable a held-out datapoint is if the model is fit on all other data. Differences in the ELPD of more than 4-5 standard errors are often taken to be credible, whereas differences of 2-3 standard errors should be interpreted with more caution.] This comparison suggests that the psychometric model captures participants' responses better than the ideal adaptor ($\Delta_{elpd}=$ `r loo_compare(fit_test.IA_predicted_logodds, fit_test.upto.test4)[2, 1]`; SE$_\Delta=$ `r loo_compare(fit_test.IA_predicted_logodds, fit_test.upto.test4)[2, 2]`).


Most relevant to the present purpose is a systematic deficiency of the model, evident in Figure \@ref(fig:plot-IA-human-PSE): the ideal adaptor substantially *under*-predicts changes in listeners' PSEs  during initial exposure, and *over*-predicts changes in listeners' PSE following exposure (see, in particular, the -20ms and -10ms conditions in Test 4). Why? Simply put, it is the best the model can do in order to accommodate all three of (i) listeners' pre-exposure behavior, (ii) the rapid changes in listeners' PSEs during initial exposure, and (iii) the premature flattening off of changes in listeners' PSEs. Put differently, an ideal adaptor can *either* explain the fact that listeners' PSEs changed rapidly at the start of exposure *or* that there seem to be little to no changes in listeners' PSE after the second exposure block. But it cannot explain both. (see SI \@ref(sec:ideal-adaptor) for analysis of the ideal adaptor's predictions and its relationship to human responses)

<!-- To understand this pattern, it is helpful to recognize that rational models of distributional learning describe listeners' expectations after exposure as the weighted mean of prior expectations and the observed exposure statistics [@kleinschmidt-jaeger2015]. This means that the speed with which a learner adapts at the start of exposure is expected to be predictive of how fast a learner adapts after additional exposure. Put differently, ideal adaptors constrain the rate at which PSEs can change across exposure trials. This means that an ideal adaptor can *either* explain the fact that listeners' PSEs changed rapidly at the start of exposure *or* that there seem to be little to no changes in listeners' PSE after the second exposure block. But it cannot explain both aspects of our data. It thus appears that participants indeed converged prematurely in their behavior, relative to what would be expected under the specific model of ideal information integration that we employed. -->

What about model *selection*? To assess the possibility that premature convergence is a consequence of model selection, we again used the phonetic database of /d/ and /t/ productions shown in Figure \@ref(fig:prior-distributions-and-exposure-conditions)A [@chodroff-wilson2018]. This time, we fit separate ideal observers to all talkers in that database to obtain the predicted PSEs for each of those talkers. This allowed us to estimate the range of talker-specific PSEs that a typical L1 listener of US English might have experienced throughout their life prior to our experiment. This range---indicated on the righthand-side of Figure \@ref(fig:plot-IA-human-PSE)---serves as an estimate of the range of PSEs that a listener would be expected to accommodate based on model selection alone. It provides a decent qualitative fit to the range of adaptive changes in the PSEs that listeners in our experiment accommodated.

Unlike unconstrained model learning, model selection---listeners selecting between previously learned phonetic representations---thus provides one potential explanation for premature convergence. This explanation makes a prediction that can be easily tested in future work: even substantially longer exposure to, for example, a few hundred exposure trials should still not result in convergence against the behavior of an idealized learner---at least as long as exposure is limited to a single day without intermittent sleep. A separate question for future research would be whether listeners can overcome the initial premature convergence with repeated exposure over multiple days, as hypothesized in @xie2018. Future work will also be necessary to address potential alternative explanations of premature convergence, which we discuss next. Under these alternative interpretations, the present data would *not* constitute evidence against the hypothesis that adaptive speech perception can achieve full learning (prediction 4).

### But is premature convergence real?
The first type of alternative explanation appeals to methodological confounds. For example, we considered whether the appearance of premature convergence could be a trivial result of priors we used in fitting the psychometric mixed-effects model. Following standards in the literature, we employed a weakly regularizing Student $t$ prior for all population-level effects. This prior favors small coefficient estimates, regularizing the estimated shifts in PSEs towards zero. As this regularization is particularly strong for more extreme shifts in the PSE, it is theoretically possible that our priors caused the psychometric model to 'hallucinate' premature convergence. This would make these findings artifacts of our data analysis approach, rather than findings of theoretical interest. Given how weakly regularizing the priors we employed were, this explanation struck us as unlikely: even the largest estimated shifts were well within the 95% highest density interval of the prior. Still, we decided to address this possibility more directly. We refit the psychometric model once with a substantially weaker prior (SD of Student $t$ = 5) and once with an even weaker uniform prior. In both cases, results changed only numerically and premature convergence was still observed.
```{r calculate-test-surprisal}
d.test_surprisal <-
  d.for_analysis %>%
  filter(Phase == "test") %>%
  distinct(Item.VOT) %>%
  cross_join(io %>% select(-x)) %>%
  filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")) %>%
  # For each test stimulus, calculate the sum densities of /d/ and /t/ over
  # that VOT (i.e., the density of the marginal VOT distribution)
  mutate(
    density = map2_dbl(
      Item.VOT,
      io,
      function(x, y)
        pmap(
          list(y$mu, y$Sigma, y$Sigma_noise),
          ~ dmvnorm(x, ..1, ..2 + ..3)) %>%
        reduce(`+`)),
    surprisal = -log2(density))


# # Visualize marginal density
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = density, color = Condition.Exposure)) +
#       geom_line()
#
# # Visualize surprisal
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = surprisal, color = Condition.Exposure)) +
#       geom_line()

d.test_surprisal %<>%
  group_by(Condition.Exposure) %>%
  summarise(across(surprisal, list(mean = mean, sd = sd)))
```

Other possible explanations appeal to assumptions we made for the idealized learner who has fully learned the exposure distributions, and how these assumptions mismatch the information that is available to listeners. Under this explanation, what appears as premature convergence in reality reflects adequate convergence against what would be expected from an idealized learner that has access to the same information as listeners. For example, our idealized learner models have perfectly learned the statistics of the exposure stimuli, while ignoring all test stimuli. Listeners, however, might learn even from unlabeled test stimuli [for demonstration, see @xie-kurumada2024]. Indeed, the effects of repeated testing without intermittent exposure (Test 4-6) suggests as much.

Critically, test tokens had by design identical phonetic properties across all exposure conditions. Inevitably then, the location of test tokens relative to the exposure tokens *differed* between conditions. If listeners integrate test tokens into their estimate of the talker's accent [@xie-kurumada2024], this might explain the appearance of premature convergence: at the end of Test 4, 36 out of 180 trials (20%) that participants had experienced were test tokens.^[If listeners adapt over a moving time-window (rather than over all inputs from a talker), or in other ways weight more recent information more strongly, this would further increase the relative impact of test tokens on listeners' categorization responses during test.] While it is difficult to evaluate this explanation without a specific model of how listener learn from unlabeled tokens, one consideration suggests that it is not sufficient to explain our data. To estimate how much learning test tokens alone would support in the different conditions, we calculated the surprisal of the test token under the idealized learners. For an idealized learner that has *fully* learned the exposure distribution (cf. colored dashed lines in Figure \@ref(fig:plot-fit-PSE)C), test stimuli would convey about the same amount of surprisal in the -20ms and -10ms conditions (both ${\rm E}[-\log_2 p(VOT |$ idealized learner$)] =$ `r d.test_surprisal %>% filter(Condition.Exposure == "Shift10") %>% pull(surprisal_mean) %>% round(., 1)` bits), compared to larger surprisal in the + 40 condition (`r d.test_surprisal %>% filter(Condition.Exposure == "Shift40") %>% pull(surprisal_mean) %>% round(., 1)` bits). At least based on these general considerations, learning from test tokens alone would thus predict even earlier premature convergence in the +20ms conditions, compared to the other two conditions---the opposite of what we observed. Future work can further address this question by developing and applying unsupervised adaptation models to our data [e.g., building on @harmon2019; @mcmurray2009; @olejarczuk2018; @vallabha2007; @yan-jaeger2018]. <!-- Future work could more decisively address this alternative explanation by replicating our experiment while using test tokens that are placed identically *relative to the exposure distributions* (which comes with its own analysis challenges).-->

For now, we conclude that our results *might* be identifying a previously unrecognized limitation of adaptive speech perception, and that this limitation might point to a need to revisit how we think about the processes that underlie such adaptation (in terms of model selection, rather than model learning).

<!-- Finally, other explanations appeal to mismatches between listeners and the ideal adaptor model refers to knowledge of cross-category correlations  [for a detailed list of assumptions, see appendix of @kleinschmidt-jaeger2015]. It is known, for example, that the VOT and f0 means of L1-US English /d/ and /t/ categories are correlated [@house-fairbanks1953; @lehiste-peterson1961; @ohde1984; @chodroff-wilson2018; @clayards2017]. The ideal adaptor model in Figure \@ref(fig:plot-IA-human-PSE) did not model these correlations. It is thus conceivable that prior expectations about these or other cross-category correlations were violated by our exposure distributions, keeping listeners from fully adapting. The present experiment cannot rule out this explanation. We note, however, that correlations between category means alone would seem unlikely to explain our result. First, these correlations are known to be comparatively weak across voiced and voiceless categories like /d/ and /t/ [@chodroff-wilson2018], thus carrying little information. Second, as far as we can tell, integration of these correlations into the ideal adaptor would be expected to lead to *faster* convergence against the exposure distribution, rather than to premature convergence. -->

```{r}
rm(d.psychometric_intercept.slope.PSE.draws, #fit_test.simple_effects_condition, 
   fit_test.simple_effects_block)
```
