```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Results

```{r model-fit-test-blocks, include=FALSE}
# Storing some information about the data set submitted for analysis
# (to facilitate unscaling predictions back into the original scales for plotting)
d.mean_sd_scaling <-
  d_for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")
contrast_type <- "difference"

# Simplifying model with uniform bias
fit_mix_test <-
  fit_model(
    data = d_for_analysis,
    phase = "test",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .995)

# fit difference coded model on exposure block
fit_mix_exposure <-
  fit_model(
    data = d_for_analysis,
    phase = "exposure",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .999)

# store fitted lapse rate for change modelling
fitted_lapse <- plogis(fixef(fit_mix_test)[[2]])
```

We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models, using brms [@R-brms_a] in R [@R; @RStudio]. Psychometric models account for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while commonplace in research on speech perception [but see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the present experiment, lapse rates were negligible (`r print_CI(fit_mix_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. This lapse rate compares favorably against those assumed or reported in prior work [e.g.,@kleinschmidt-jaeger2016; @kleinschmidt2020; @clayards2008].

The two psychometric models for exposure and test blocks each regressed participants' categorization responses against the full factorial interaction of VOT, block, and exposure condition, along with the maximal random effect structure (by-subject intercepts and slopes for VOT, block, and their interaction, and by-item intercept and slopes for the full factorial design; see SI, \@ref(sec:analysis-approach)). Figure \@ref(fig:plot-fit-slope-PSE) summarizes the results that we describe in more detail next. Panels A and B show participantsâ€™ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panels C and D show the slope and point of subject equality (PSE)---i.e., the point at which participants are equally likely to respond "d" and "t"---of the categorization function across blocks and conditions. These panels facilitate comparison across blocks within each exposure condition. Here we focus on the test blocks, which were identical within and across exposure conditions.^[Previous studies have estimated changes in participants' categorization responses by analyzing responses on unlabeled exposure trials [e.g., @clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019]. This approach compares responses across different VOT values (since the exposure inputs differed by exposure condition), increasing the risk that assumptions baked into the analysis approach (e.g., linearity along the acoustic-phonetic continuum) bias the results. The analysis of test blocks that are identical within and across participants avoids this issue.] Analyses of the exposure blocks are reported in the SI (\@ref(sec:exposure-analyses)), and replicate all effects found in the test blocks. 

We begin by presenting the overall effects, averaging across all test blocks. This part of our analysis matches previous work, which analyzed the *average* effect of exposure across the entire experiment ['batch tests', e.g., @clayards2008; @kleinschmidt-jaeger2016; @nixon2016; @theodore-monto2019] or during a single post-exposure test [e.g., @kleinschmidt2020]. Then we present novel analyses that address questions about the incremental adaptation---testing the predictions described in the introduction. 

```{r fit-nested-models-for-intercepts-slopes-plot}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_mix_test_nested <-
  fit_model(data = d_for_analysis,
          phase = "test",
          formulation = "nested_slope",
          priorSD = 15, 
          adapt_delta = .995)
  
fit_mix_exposure_nested <-
  fit_model(data = d_for_analysis,
            phase = "exposure",
            formulation = "nested_slope",
            priorSD = 15, 
            adapt_delta = .999)
```


```{r extract-intercepts-slopes-from-test-and-exposure-models}
get_intercepts_and_slopes <-
  . %>%
  gather_draws(`b_mu2_IpasteCondition.ExposureBlocksepEQ.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(.variable = gsub("b_mu2_IpasteCondition.ExposureBlocksepEQxShift(\\d{1,2})x(\\d{1}.*$)", "Shift\\1.\\2", .variable),
         term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept")) %>%
  separate(col = .variable, into = c("Condition.Exposure", "Block"), sep = "\\.") %>%
  mutate(Block = ifelse(str_detect(Block, "VOT"), str_replace(Block, "(\\d{1}):VOT_gs", "\\1"), Block)) %>%
  pivot_wider(names_from = term, values_from = ".value") %>%
  relocate(c(Condition.Exposure, Block, Intercept, slope, .chain, .iteration, .draw))

d.estimates <-
  full_join(
    fit_mix_test_nested %>% get_intercepts_and_slopes(),
    fit_mix_exposure_nested %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    PSE = ifelse(
      Block %in% c(2, 4, 6),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_exposure),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_test))) %>% 
  group_by(Condition.Exposure, Block) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), median = median, upper = ~ quantile(.x, probs = .975)))) 
```

```{r}
# Make ideal observers that are fit on the exposure VOTs of the three exposure conditions.
# These ideal observers can be thought of as reflecting a learner that has fully learned 
# the exposure distributions. Note that we include perceptual noise in the ideal observers
# (estimated in Kronrod et al., 2016) to make their perception more human-like.
io <-    
  make_VOT_IOs_from_exposure(
    d_for_analysis %>% 
      filter(Phase == "exposure") %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      # Subset data to a single participant per exposure condition. This is sufficient since 
      # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics 
      # (mean and SD) at the end of the experiment, which were identical across participants 
      # in each condition. 
      # 
      # Note that this approach trains the ideal observers on the *actual* VOT distribution 
      # that participants heard, not on the theoretical distributions these VOTs were sampled
      # from. This is in line with our goal to simulate behavior of an idealized participant
      # who has fully learned the exposure distributions.
      slice_sample(n = 1) %>% 
      unnest(data) %>% 
      mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")) %>% 
      rename(VOT = Item.VOT, f0 = Item.f0_Mel)) 

# Get logistic parameter estimates for a simulated ideal observer listener. Do so for both 
# exposure and test. Even though this turned out to be overkill, we did it because it was 
# theoretically possible that the procedure we use to analyze *listeners'* responses (a 
# logistic model with a linear effect of VOT) introduces a bias into the estimation. Here, 
# we thus apply a variant of that same analysis approach to responses that are sampled from
# the three ideal observer fit on the exposure data (one each per exposure condition).
# 
# Unlike for the analysis of participants' responses, there is no need to use a psychometric
# model since ideal observers have not attentional lapses (i.e., lambda = 0). 
d.io.categorization <- 
  rbind(
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      io %>% filter(Condition.Exposure != "prior"), 
      d_for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Item.VOT) %>%
        rename(x = Item.VOT), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "test") %>% 
      crossing(Block = c(1, 3, 5, 7, 8, 9)), 
    # Assess the ideal observers on the VOT locations that occur in the three different 
    # exposure conditions. Only use unlabeled exposure trials, since those are the trials
    # we assess participants' behavior on.
    get_logistic_parameters_from_model(
      io %>% filter(Condition.Exposure != "prior"), 
      d_for_analysis %>%  
        filter(Phase == "exposure", Item.Labeled == F) %>%
        group_by(Condition.Exposure) %>%
        # Since blocks differ in which VOTs are unlabeled, we include all blocks
        # (we are simulating the expected parameters across all exposure blocks)
        filter(ParticipantID == first(ParticipantID)) %>% 
        select(Item.VOT) %>% 
        rename(x = Item.VOT), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "exposure") %>% 
      crossing(Block = c(2, 4, 6))) %>%
  select(Condition.Exposure, Phase, Block, intercept_scaled, slope_scaled, PSE) %>% 
  mutate(across(c(Phase, Condition.Exposure, Block), factor)) 
```

```{r prepare-plot-intercepts-slopes}
p.across_blocks <-
  d.estimates %>% 
  ggplot(
    aes(
      x = Block, y = Intercept_median,
      ymin = Intercept_lower, ymax = Intercept_upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.55, 8.55, 1)), 
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = position_dodge(.3), size = 1) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  stat_summary(geom = "line", position = position_dodge(.3)) +
  scale_x_discrete("Block", labels = c("1" = "Test 1", "2" = "Exposure 1", "3" = "Test 2", "4" = "Exposure 2", "5" = "Test 3", "6" = "Exposure 3", "7" = "Test 4", "8" = "Test 5", "9" = "Test 6")) +
  scale_colour_manual("Condition",
    labels = c("baseline", "+10ms", "+40ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 22.5, hjust = 1)) 

p.intercept_1to7 <-
  p.across_blocks +  
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = intercept_scaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .7, 
    alpha = 0.3,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("Intercept")

p.slope_1to7 <-
  p.across_blocks +  
  aes(y = slope_median, ymin = slope_lower, ymax = slope_upper) +
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = slope_scaled, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, linewidth = .9, alpha = 0.5,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("slope (log-odds/ms VOT)")

d.true_shift <- 
  d.estimates %>% 
  # join the ideal PSEs from io predictions
  left_join(
    d.io.categorization %>% 
      dplyr::select(Phase, Condition.Exposure, Block, PSE) %>% 
      rename(PSE.io_predicted = PSE), by = c("Condition.Exposure", "Block")) %>%
  group_by(Condition.Exposure) %>% 
  mutate(
    PSE_block1 = 
      case_when(
        Condition.Exposure == "Shift0" ~ d.estimates$PSE_median[1], 
        Condition.Exposure == "Shift10" ~ d.estimates$PSE_median[10],
        Condition.Exposure == "Shift40" ~ d.estimates$PSE_median[19]),
    true_shift = PSE.io_predicted - PSE_block1,
    proportion_shift = (PSE_median - PSE_block1)/true_shift,
    proportion_shift = ifelse(Condition.Exposure != "Shift40", -(proportion_shift), proportion_shift))

# compute PSEs from talkers in Chodroff-Wilson corpus
d.talkerPSEs <- 
  get_IO_categorization(
    data = d.chodroff_wilson,
    cues = c("VOT"),
    groups = c("Talker", "gender"),
    with_noise = TRUE,
    VOTs = seq(0, 85, .5)) %>% 
  mutate(
    center_constant = mean(d.true_shift$PSE_block1) - mean(PSE),
    PSE_centered = PSE + center_constant) %>% 
  summarise(
    lower_PSE = quantile(PSE_centered, probs = .025), 
    mean_PSE = mean(PSE_centered),
    upper_PSE = quantile(PSE_centered, probs = .975)) 

p.PSE_1to7 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms VOT)") +
  aes(y = PSE_median, ymin = PSE_lower, ymax = PSE_upper) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  geom_label(
    data = d.true_shift %>% filter(Block != 1), 
    mapping = aes(label = paste(scales::percent(proportion_shift, accuracy = .1))),
    position = position_dodge(.3),
    label.size = .15,
    label.padding = unit(.18, "lines"),
    size = 2.2,
    show.legend = F) +
  geom_step(
    data = d.io.categorization,
    mapping = aes(x = Block, y = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .8, 
    alpha = 0.5, 
    direction = "mid",
    inherit.aes = F) +
  geom_text(
    data = d.true_shift %>% 
      filter(Phase == "exposure") %>% 
      group_by(Condition.Exposure, true_shift) %>% 
      summarise(),
    mapping = aes(
      x = 10.2, y = c(25, 35, 65),
      label = paste(round(true_shift), "(100%)"),
      colour = Condition.Exposure),
    colour = colours.condition,
    fontface = "bold",
    size = 3,
    inherit.aes = F) +
  annotate(
    "crossbar",
    x = 9.7, y = d.talkerPSEs$mean_PSE, ymin = d.talkerPSEs$lower_PSE, ymax = d.talkerPSEs$upper_PSE,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  coord_cartesian(xlim = c(0.75, 9), clip = "off") +
  theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"))

p.estimates_1to7 <-  
  (p.slope_1to7 | p.PSE_1to7) +
  plot_layout(guides = "collect") &
  theme(legend.position = "none", axis.text = element_text(size = 8))
```

```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
get_conditional_effects <- function(model, data, phase) {
  conditional_effects(
    x = model,
    effects = "VOT_gs:Condition.Exposure",
    conditions = make_conditions(
      data %>%
        filter(Phase == .env$phase & Item.Labeled == FALSE) %>%
        prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = "difference"),
      vars = c("Block")),
    method = "posterior_epred",
    ndraws = 500,
    re_formula = NA)
}

cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_mix_exposure, d_for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_mix_test, d_for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(1, 3, 5, 7, 8, 9), "test", "exposure"),
    Block.plot_label = factor(case_when(
      Block == 1 ~ "Test 1",
      Block == 3 ~ "Test 2",
      Block == 5 ~ "Test 3",
      Block == 7 ~ "Test 4",
      Block == 8 ~ "Test 5",
      Block == 9 ~ "Test 6",
      Block == 2 ~ "Exposure 1",
      Block == 4 ~ "Exposure 2",
      Block == 6 ~ "Exposure 3")),
    Block.plot_label = fct_relevel(Block.plot_label, c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3",  "Test 4", "Test 5", "Test 6")))

label_colour <- tibble(
  Block.plot_label = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
  Block.colour = c("grey", "white", "grey", "white", "grey", "white", "grey")) %>%
  mutate(Block.plot_label = factor(Block.plot_label, levels = Block.plot_label, ordered = T))

p.fit_1to7 <- cond_fit_test_exposure %>%
  filter(Block %in% c(1:7))  %>%
  ggplot() +
  geom_rect(
    data = label_colour,
    aes(xmin = -Inf, xmax = Inf,
        ymin = 1.05, ymax = 1.3,
        fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "white" = "white")) +
  new_scale_fill() +
  geom_linefit(data = d_for_analysis %>%
      group_by(Phase, Condition.Exposure, Block.plot_label) %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE),
      x = Item.VOT,
      y = estimate__,
      fill = NA,
      legend.position = "top",
      legend.justification = "right") +
  coord_cartesian(clip = "off", ylim = c(0, 1))

p.fit_7to9 <- cond_fit_test_exposure %>%
  filter(Block %in% c(7:9)) %>%
  ggplot() +
  geom_linefit(data = d_for_analysis %>%
      group_by(Condition.Exposure, Block) %>%
      filter(Block %in% c(7:9)),
      x = Item.VOT,
      y = estimate__,
      fill = "grey",
      legend.position = "none") +
  theme( axis.title.y = element_blank())
```

(ref:plot-fit-slope-PSE) Summary of results. **Panel A:** Changes in listeners psychometric categorization functions as a function of exposure, from Test 1 to Test 4 with all intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks since labeled trials provide no information about listeners' categorization function). Point ranges indicate the mean proportion of participants' "t"-responses and their 95% bootstrapped CI. Lines and shaded intervals show the *maximum a posteriori* (MAP) estimates and 95% posterior CIs of a Bayesian mixed-effects psychometric model fit to participants' responses. **Panel B:** Same as Panel A but for the final three test blocks without intervening exposure. Test 4 is shown as part of both Panels A and B. **Panels C \& D:** Changes across blocks in the slope and boundary (point-of-subjective-equality, PSE) of the categorization functions shown in Panels A \& B. Point ranges represent the posterior medians and their 95% CI. Dashed reference lines show the intercepts and PSEs that naive learner would be expected to converge against after sufficient exposure (an ideal observer model that has fully learned the exposure distributions). Percentage labels indicate the amount of shift as a proportion of the expected shift under an ideal observer.

\begin{landscape}

```{r plot-fit-slope-PSE, fig.width=12.5, fig.height=8, fig.cap="(ref:plot-fit-slope-PSE)"}
p.fit_1to7 + p.fit_7to9 + p.estimates_1to7 +
  plot_layout(
    design = "
AAAAAAAAA
BBB######
DDDDDDDDD
",
heights = c(1, 1, 2)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") & 
  theme(plot.tag = element_text(face = "bold")) 
```

\end{landscape}

```{r remove-plot-objects}
rm(p.across_blocks, p.estimates_1to7, p.PSE_1to7, p.slope_1to7, p.fit_1to7, p.fit_7to9)
```


<!-- TO DO: If we need to cut, this section could probably go. It's only here to 'ease in' readers. -->
## Does exposure affect participants' categorizations (averaging across all blocks)?
We first used the psychometric mixed-effects model to assess whether the exposure conditions had the expected effects across all test blocks *relative to each other*. Unsurprisingly, participants were more likely to respond "t" the longer the VOT ($`r get_bf(fit_mix_test, "mu2_VOT_gs > 0")`$). Critically, exposure affected participants' categorization responses in the expected direction. Marginalizing over all test blocks, participants in the +40 condition were less likely to respond "t" than participants in the +10 condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift40vs.Shift10 < 0")`$) or the baseline condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift40vs.Shift10 + mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). There was also evidence---albeit less decisive---that participants in the +10 condition were less likely to respond "t" than participants in the baseline condition ($`r get_bf(fit_mix_test, "mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). That is, the +10 and +40 conditions resulted in categorization functions that were shifted rightwards compared to the baseline condition, as also evident in Figures \@ref(fig:plot-fit-slope-PSE).

This replicates previous findings that exposure to changed VOT distributions changes listeners' categorization responses [for /b/-/p/: @clayards2008; @kleinschmidt2015; @kleinschmidt2020; for /g/-/k/, @theodore-monto2019]. Having established that exposure affected categorization, we turn to the questions of primary interest. Incremental changes in participants' categorization responses can be assessed from three mutually complementing perspectives. First, we compare how exposure affects listeners' categorization responses relative to other exposure conditions. This tests how early in the experiment differences between exposure conditions began to emerge. Second, we compare how exposure changes listeners' categorization responses from block to block within each condition, relative to listeners' responses prior to any exposure. Third and finally, we compare changes in listeners' responses to those expected from an ideal observer that has fully learned the exposure distributions. This investigates the degree of boundary shift listeners make at each test block relative to their expectations before informative exposure. For all three analyses, we initially focus on Tests 1-4 with intermittent exposure. We then close by briefly describing the effects of repeated testing.

## How quickly does exposure change participants' categorization functions? 
Figure \@ref(fig:plot-fit-slope-PSE)A suggests that differences between exposure conditions emerged early in the experiment: already in Test 2, listeners in the +10 condition have shifted their categorization functions rightwards relative to the baseline condition, and listeners in the +40 condition have shifted their in categorization functions even further rightwards. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-condition). Prior to any exposure, during Test 1, participants' responses did not differ across exposure condition. This result is predicted by models of adaptive speech perception under the assumptions that (a) participants in the different groups have similar prior experiences, and that (b) our sample size of is sufficiently large to yield stable estimates of listeners' categorization function.

During Test 2, after exposure to only 24 /d/ and 24 /t/ stimuli (thereof half labeled), participants' categorization responses already differed between exposure conditions (BFs > 13.7). The differences between exposure conditions that emerged at this point were all in the direction predicted by models of adaptive speech perception. This suggests that changes in listeners' categorization responses emerged *quickly*---after only a fraction of exposure trials previously tested in similar paradigms. Indeed, additional analyses reported in the SI (\@ref(sec:exposure-analyses)) found that listeners' categorization functions had already changed *during* the first exposure block, in line with Figure \@ref(fig:plot-fit-slope-PSE)A. 

The effects of the three exposure conditions continued to persist until Test 4. Table \@ref(tab:hypothesis-table-simple-effects-condition) does, however, indicate an interesting non-monotonic development in the way that listeners' categorization function changed. While the difference between the +40 condition and both the baseline and +0 condition continued to increase numerically with increasing exposure (increasingly larger magnitude of negative estimates in Tests 2-4), the same was not the case for the difference between the +10 and the baseline condition. Instead, the difference between the +10 and baseline condition reduced with increasing exposure (while maintaining its direction). This development turns out to be important in understanding incremental adaptation, and we continue to discuss it below.

```{r fit-nested-blocks-simple-effects, results='hide'}
# nested model to get simple effects of Condition embedded in block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor")
)

fit_mix_test_nested_block <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d_for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested_block-sample-priorSD15-0.995.rds")

# get tidy df of nested test simple effects within block 
simple_effects_condition <-
  tidy(fit_mix_test_nested_block, effects = "fixed") %>%
  filter(term != "theta1_(Intercept)")
```


```{r hypothesis-table-simple-effects-block, results='asis'}
# hypotheses to answer questions "when did change first emerge?" 
hyp_contrasts_nested_block <-
  c(
    "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
    "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
    "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
    "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
    "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
    "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0")

hyp_contrasts_nested_block <- hypothesis(fit_mix_test_nested_block, hyp_contrasts_nested_block, robust = T)
hyp_contrasts_nested_block <- hyp_contrasts_nested_block$hypothesis %>% dplyr::select(-Star)

# translate hypotheses into intelligible statements
hyp_contrasts_nested_block_readable <- tibble(Hypothesis = c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline", "+40 vs. +10", "+40 vs. baseline"), 5)))

table.simple_effects <- 
  make_hyp_table(hyp_contrasts_nested_block_readable, hyp_contrasts_nested_block, caption = "When did exposure begin to affect participants' categorization responses? When, if ever, were these changes undone with repeated testing? This table summarizes the simple effects of the exposure conditions for each test block. Note that righward shifts correspond to negative effects (lower intercepts in predicting the log-odds of \"t\"-responses).") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)
table.simple_effects
```


```{r hypothesis-table-simple-effects-condition, results='asis', fig.cap="TEMPORARY TABLE; TO BE REPLACED BY THE PENDING MODEL"}
fit_test_nested_condition <- read_rds("../models/Exp-AE-DLVOT-nested-condition.rds")

# hypotheses to answer questions "when did change first emerge?" 
hyp_contrasts_nested_condition <-
  c(
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > 0",
    "mu2_Condition.ExposureShift0:Block_Test3vs.Test2 > 0",
    "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
    "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 > 0",
    "mu2_Condition.ExposureShift10:Block_Test3vs.Test2 > 0",
    "mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
    "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
    "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 > 0",
    "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 > 0",
    "mu2_Condition.ExposureShift40:Block_Test4vs.Test3 > 0",
    "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 > 0")

hyp_contrasts_nested_condition <- hypothesis(fit_test_nested_condition, hyp_contrasts_nested_condition, robust = T)
hyp_contrasts_nested_condition <- hyp_contrasts_nested_condition$hypothesis %>% dplyr::select(-Star)

# translate hypotheses into intelligible statements
hyp_contrasts_nested_condition_readable <- tibble(Hypothesis = rep(c("Test 2 vs. Test 1 > 0", "Test 3 vs. Test 2 > 0", "Test 4 vs. Test 3 > 0", "Test 4 vs. Test 1 > 0"), 3))

table.simple_effects_condition <- 
  make_hyp_table(hyp_contrasts_nested_condition_readable, hyp_contrasts_nested_condition, caption = "When did exposure begin to affect participants' categorization responses? This table summarizes the simple effects of block for each condition. Note that righward shifts correspond to negative effects (lower intercepts in predicting the log-odds of \"t\"-responses).") %>%
  pack_rows("baseline ", 1, 4) %>%
  pack_rows("+10", 5, 8) %>% 
  pack_rows("+40", 9, 12)
table.simple_effects_condition
```


```{r remove-unused-objects-section3}
# remove unused objects
rm(fit_mix_test, fit_mix_test_nested, fit_mix_test_nested_block, fit_mix_exposure, fit_mix_exposure_nested, cond_fit_test_exposure)
```

## Incremental adaptation of prior expectations: block-to-block changes in listeners' categorization function
Next, we compare how exposure affected listeners' categorization responses from block to block *within* each exposure condition. To facilitate visual comparison, Figure \@ref(fig:plot-fit-slope-PSE)C & D summarize these changes for the slope and PSE, respectively. Focusing for now on Tests 1-4, this highlights three aspects of participants' behavior that were not readily apparent in the statistical comparisons we have summarized so far. First, there was little evidence of any changes in the slopes of the categorization functions (BFs < XXX; see SI, \@ref(sec:SI-slope-tests)). And whatever changes were observed across blocks were very similar across exposure conditions (BFs < XXX; SI, \@ref(sec:SI-slope-tests)). This is in line with distributional learning theories of adaptive speech perception [@kleinschmidt-jaeger2015], given that the variance of /d/ and /t/ was (a) held constant across all three exposure conditions, and (b) designed to resemble the variance of /d/ and /t/ in typical speech input.<!-- TO DO: add those tables to SI and check whether they agree with what we've written here. --> 

```{r hypothesis-table-simple-effects-block, results='asis'}
caption = "In what direction did exposure shift participants' responses compared from block to block? This table summarizes the simple effect of block for each exposure condition."
```

Second, while the PSEs for the +40 and +10 conditions were shifted rightwards compared to the baseline condition, both the +10 and the baseline condition actually shift leftwards relative to their pre-exposure starting point in Test 1. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-block). To understand this pattern, it is helpful to relate the three exposure conditions to the distribution of VOT in listeners' prior experience. Figure \@ref(fig:exposure-means-database-density) shows the category means of our exposure conditions relative to the distribution of VOT by talkers of L1-US English [based on @chodroff-wilson2018]. This comparison offers an explanation as to why the baseline condition (and to some extent the +10 condition) shift leftwards with increasing exposure, whereas the +40 condition shifts rightwards: relative to listeners' prior experience, only the +40 condition presented larger-than-expected category means, whereas the baseline condition and, to some extent, the +10 condition presented lower-than-expected category means. That is, once we take into account how our exposure conditions relate to listeners' prior experience, both the direction of changes from Test 1 to 4 *within* each exposure condition (Table \@ref(tab:hypothesis-table-simple-effects-block)), and the direction of differences *between* exposure conditions receive an explanation (Table \@ref(tab:hypothesis-table-simple-effects-condition)).

(ref:exposure-means-database-density) Placement of exposure stimuli relative to an estimate of typical phonetic distributions for `r nrow(d.chodroff_wilson)` word-initial /d/ and /t/ productions in L1-US English [based on `r length(unique(d.chodroff_wilson$Talker))` female talkers in @chodroff-wilson2018; for details, see SI \@ref(sec:SI-phonetic-data)]. The outermost contour of each category shows the 95% density quantile. Points show the category means of the exposure condition.

```{r centering}
# center exposure cues to speech corpus means
d_for_analysis %<>%
  ungroup() %>% 
  mutate(
    VOT.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(VOT = Item.VOT, Talker = "new"),
      cue = "VOT"),
    f0_Mel.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(f0_Mel = Item.f0_Mel, Talker = "new"),
      cue = "f0_Mel"),
    category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))) 
```

```{r getting-means-at-exposure-by-condition, eval=FALSE}
VOT.database_mean <- mean(d.chodroff_wilson$VOT.CCuRE)
f0_Mel.database_mean <- mean(d.chodroff_wilson$f0_Mel.CCuRE)

cond_means_exposure <- 
  d_for_analysis %>%
  filter(Phase == "exposure") %>%
  group_by(Condition.Exposure) %>%
  summarise(across(c(VOT.CCuRE, f0_Mel.CCuRE), mean, .names = "{.col}_mean")) %>% 
  mutate(
    VOT_diff_database = VOT.CCuRE_mean - VOT.database_mean,
    F0_diff_database = f0_Mel.CCuRE_mean - f0_Mel.database_mean)
```

```{r prepare-quantile-plot}
# specify quantiles for density plots
quantile_levels <- c(.05, .25, .5, .75, .95)
d_breaks <- 
  density_quantiles(
    x = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(VOT),
    y = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(f0_Mel),
    quantiles = quantile_levels)
t_breaks <- 
  density_quantiles(
    x = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(VOT),
    y = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(f0_Mel),
    quantiles = quantile_levels)

# matching breaks in each category to corresponding quantile
d_quantile <- quantile_levels
t_quantile <- quantile_levels
names(d_quantile) <- d_breaks
names(t_quantile) <- t_breaks
quantile_breaks <- tibble(quantile_levels, d_breaks, t_breaks)

p.density <-
  d.chodroff_wilson %>%
  ggplot(aes(x = VOT, y = f0_Mel, linetype = category, group = category)) +
  geom_density2d(
    data = . %>% filter(category == "/d/"),
    aes(colour = d_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = d_breaks) +
  geom_density2d(
    data = . %>% filter(category == "/t/"),
    aes(colour = t_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = t_breaks) +
  scale_y_continuous("F0 (Mel)", limits = c(200, 365)) +
  scale_x_continuous("VOT (ms)", limits = c(-12, 125), breaks = scales::breaks_width(25)) +
  scale_colour_gradient("Quantiles",
                        high = "#e6e6e6",
                        low = "#000000",
                        guide = "colourbar",
                        breaks = quantile_levels,
                        labels = scales::percent(quantile_levels)) +
  theme(legend.position = "top",
        legend.text = element_text(size = 7)) +
  #plot centered means
  new_scale_color() +
  geom_point(
    data = d_for_analysis %>%
      filter(Phase == "exposure") %>%
      group_by(Condition.Exposure, category) %>%
      summarise(across(c(VOT.CCuRE, f0_Mel.CCuRE), mean)),
    mapping = aes(x = VOT.CCuRE, y = f0_Mel.CCuRE, colour = Condition.Exposure, shape = category),
    size = 2,
    alpha = 0.8,
  inherit.aes = F,
  show.legend = T) +
  scale_colour_manual("Condition",
             labels = c("baseline", "+10ms", "+40ms"),
             values = colours.condition) +
  guides(colour = "none",
         linetype = guide_legend(title = "Category"),
         shape = guide_legend(title = "Category"))
```

```{r exposure-means-database-density, fig.width=base.width*2.5+1, fig.height=base.height*2.5, warning=FALSE,fig.cap="(ref:exposure-means-database-density)"}
p.density +
  plot_layout(ncol = 1, guides = "collect") &
  theme(legend.position = "top")

rm(p.density)
```

Third, XXX --- talk about decreasing magnitudes in shifts first and point out how this follows error-based on ideal adaptor learning.


the reason for the decrease in the difference between the +10 and baseline conditions discussed in the previous section is *not* due to a reversal of the effects in the +10 condition. Rather, both exposure conditions changed listeners' categorization function in the same direction. However, after the rapid change from the pre-exposure Test 1 to the first post-exposure Test 2, listeners' categorization responses in the baseline condition did not change as much as in the +10 condition. In fact, listeners' responses in the baseline condition changed very little at all after Test 2. This explains the reduction in the difference between the +10 and baseline conditions discussed in the previous section. It does, however, raise the question *why* listeners' responses in the baseline condition did not change further with increasing exposure. Our third and final perspective on the incremental changes induced by exposure begins to address this question.

## Are there constraints on cumulative changes in listeners' categorization functions?
Figures \@ref(fig:plot-fit-slope-PSE)C & D also compare participants' responses against those of an ideal observer that has fully learned the exposure distributions. The dashed lines represent the slopes and PSEs, respectively, that are expected from such a learner (for details, see SI \@ref(sec:SI-bias-correction)).<!-- TO DO: that section should explain the general approach and also contain the bias corrected estimates. -->
This makes it possible to assess whether---or how much---listeners have converged against the exposure distributions. We make two observations. 

First, the slopes of listeners' categorization functions in Panel C approximate those predicted by an ideal observer: many of the 95% CIs overlap with the dashed lines. This result, too, is predicted by distributional learning models of adaptive speech perception.^[Of note, we followed @xie2023 and included perceptual noise in the ideal observer [estimated for VOT in @kronrod2016]. This deviates from some earlier comparisons of human perception against ideal observers [@clayards2008]. Without the inclusion of perceptual noise, ideal observers predict much steeper categorization functions [offering a potential explanation for the mismatch between the ideal observer predictions and human categorization responses observed in @clayards2008]. This highlights the importance of considering perceptual noise when modeling human speech perception [see also @burchill2023; @chodroff2016; @feldman2009].]

Second, Panel D suggests that listeners did *not* converge against the exposure distributions. In fact, it seems that listeners in all three exposure conditions adapted less and less.
....

For the PSE, this degree of convergence is indicated by the percentage labels.


respective optimal boundaries of each condition while the labels indicate the amount of shift made at each block as a proportion of the distance between the ideal PSE and the PSE at Test 1. 

Notably, shifts were always in the right direction but none of the groups converged on the ideal boundary. We also see that while the +10 condition fell short of the ideal boundary changes in PSEs  consistently and incrementally moved towards the target up to test 4. Even so, the magnitude of shift was relatively low with the group achieving at most 30% of the maximal shift. What is most striking from the figure is the asymmetry in listener behavior between the leftward-shifted and rightward-shifted groups: when the exposure distribution is rightward shifted listeners showed a greater propensity to move their category boundaries further from initial expectations. When the exposure distribution is leftward shifted, listeners are far more conservative with their shifts and appear to be under greater constraints. This is most obvious between the baseline and +40 condition; the baseline condition is almost a mirror opposite in shift (-18ms from the PSE at Test 1) compared to the +40 condition (+20ms from the PSE at Test 1) but the maximum shift achieved by the former was just over 20% compared to 43% in the latter. 

## Effects of repeated testing over the same uniform test continuum
Finally, we briefly summarize the effects of repeated testing evident in Tables \@ref(tab:hypothesis-table-simple-effects-condition) and \@ref(tab:hypothesis-table-simple-effects-block). Recall that some theories of adaptive perception predict that uniformly distributed test tokens will reduce the effect of preceding exposure [@kleinschmidt-jaeger2015; for relevant discussion, see also @winter-lancia2013]. In line with these theories, we find that the effects of exposure reduced from Test 4 to Test 6. In Table \@ref(tab:hypothesis-table-simple-effects-block), this is evident in a reversal of the direction of the block-to-block changes for Tests 5-6, compared to Tests 1-4. For the +40 exposure condition, these block to block changes went from rightward shifts in Tests 1-4 to leftward shifts in Tests 5-6. For the other two exposure conditions, the opposite pattern from leftward to rightward shift was observed. As a consequence, exposure effects were substantially smaller in Test 6 than in Test 4 (see Table \@ref(tab:hypothesis-table-simple-effects-condition): while the effects of the +40 condition relative to the other two exposure conditions were still credible even in Test 6 (BFs > 24), this was no longer the case for the effect of the +10 condition relative to the baseline condition (BF = 1.6). This pattern of results replicates previous findings from LGPL [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @tzeng2021], and extends them to distributional learning paradigms [see also @kleinschmidt2020]. One important methodological consequence of these findings is that longer test phases do not necessarily increase the statistical power to detect effects of adaptation [unless analyses take the effects of repeated testing into account, as in the approach developed in @liu-jaeger2018]. Analyses that average across all test tokens---as remains the norm---are bound to systematically underestimate the adaptivity of human speech perception.^[@kraljic-samuel2006 is sometimes cited as finding LGPL exposure effects even after 480 test trials, and thus as an exception to the generalization we report here. This is, however, misleading. Kraljic and Samuel used four different uniform test continua over two different phonetic contrasts (/b/-/p/ and /d/-/t/). Each test session consisted of 10 randomized repetitions of 6 test trials. Kraljic and Samuel never tested (or made any claims about) whether exposure effects were still detectable during the 10th repetition. Rather they report *average* effects across the 10 repetitions (like other LGPL studies), which is perfectly compatible with the hypothesis that repeated testing reduces the effects of exposure [see @liu-jaeger2018].]
