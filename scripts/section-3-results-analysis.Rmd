```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")

# Store parameters for model fitting
d.mean_sd_scaling <-
  d.for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]
rm(d.mean_sd_scaling)

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")
```

# Results {#sec:results}
As outlined under Methods, we present two types of analyses. We first use a Bayesian mixed-effects psychometric model to estimate listeners' categorization functions prior to, during, and after the three exposure blocks. This provides a theory-agnostic way to pursue the first two goals for the present work outlined in the introduction: to provide the first tests of predictions 3 and 4, and to assess the joint effect of predictions 1-4 in a single study, using distributional exposure. Specifically, we use the psychometric models to test predictions 1-4, while accounting for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while still common in research on speech perception [for exceptions, see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the advantages of Bayesian mixed-effects psychometric models, we refer to @prins2024, though we note that we implemented a mixture model formulation of such models in the freely available statistics software \texttt{R} [@R-base], using the amazing general-purpose package \texttt{brms} for Bayesian regression modeling [@R-brms_a]. 

After we present the results of the psychometric analysis, we introduce the ideal adaptor model, and fit it to the same data. This addresses our third and final goal outlined in the introduction. Unlike the psychometric model, the ideal adaptor is a model of distributional learning. This allows us to test how large a share of the changes in listeners' perception distributional learning can explain, and whether there are any qualitative aspects of listeners' behavior that can*not* be explained well by distributional learning. 

```{r model-fit-test-blocks, include=FALSE}
fit_test <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "standard",
    priorSD = 15,
    adapt_delta = .995)

fit_exposure <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "standard",
    priorSD = 15,
    adapt_delta = .999)
```

## Bayesian mixed-effect psychometric analysis
We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models. The full model specification with additional pointers to tutorials is available in the SI (\@ref(sec:analysis-approach)). Compared to previous work [@clayards2008; @kleinschmidt-jaeger2016; @kleinschmidt2020], lapse rates were negligible (`r print_CI(fit_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. The SI reports additional analyses over the combined exposure and test data, including extensions of the psychometric models to include lapse rates that can vary by block (\@ref(sec:analysis-lapse)) and non-parametric smooths to model non-linear effects of VOT and exposure (\@ref(sec:GAMM)). All analyses replicate the findings reported here.

The psychometric models for exposure and test blocks each regressed participants' categorization responses against the full factorial interaction of VOT, block, and exposure condition, along with the maximal random effect structure (by-participant intercepts and slopes for VOT, block, and their interaction, and by-item intercept and slopes for the full factorial design; see SI, \@ref(sec:analysis-approach)). All hypothesis tests reported below are based on these models. The Bayes factors (BF) we report quantify the strength of evidence from the data for one hypothesis over an alternative hypothesis. BFs < 3 are sometimes described as indicating "anecdotal" evidence for the hypothesis; BFs between 3 and 10, "moderate" evidence; between 10 and 30, "strong" evidence; and > 30, "very strong" evidence [@jeffreys1961; @kruschke-liddell2018]. We do, however, also report the posterior probability of each hypothesis. This provides a measure of how probable the hypothesis is given the data (assuming uniform prior probabilities for both the proposed hypothesis and its alternative), which might be more intuitive to readers unfamiliar with Bayesian data analysis.

We used the psychometric model to assess incremental changes in participants’ categorization responses from four mutually complementing perspectives (all based on the same psychometric mixed-effects model). First, we test prediction 1 (*prior expectations*), comparing participants' responses for the pre-exposure test (Test 1) against the idealized pre-exposure listener model. Second, we test prediction 2a (*exposure distribution*) by comparing the effects of different exposure conditions within each block. As noted above, previous distributional learning experiments have assessed changes in listeners' categorization function after much longer exposure than our earliest tests. By comparing conditions within each block, we also test how early effects of distributional exposure can be detected, and whether these early effects already exhibit the direction expected under distributional learning. Third, we compare within each condition how exposure incrementally changes listeners' categorization responses from block to block within each condition, relative to listeners' responses prior to any exposure. By comparing these changes against the idealized pre-exposure listener model, we test predictions 1 (*prior expectations*) and 2a,b (*exposure amount & distribution*) *together*. The same tests also assess prediction 3 about the *diminishing returns* of additional exposure. Finally, we compare changes in listeners' responses to those expected from idealized learner models. This analysis has the potential to identify constraints on cumulative adaptation. The latter two perspectives---made possible by the incremental exposure-test paradigm---allow us to test predictions 3 (*diminishing returns*) and 4 (*learning to convergence*). 

Figure \@ref(fig:plot-fit-PSE) summarizes the results that we describe in more detail next. Here, we focus on the test blocks. The SI (\@ref(sec:SI-exposure-block-analysis)) reports analyses of the exposure blocks, which replicate all effects found in the test blocks. Where relevant we refer to these analyses below. Panels A & B of Figure \@ref(fig:plot-fit-PSE) show participants’ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panel C summarizes changes in listeners' PSE across blocks and conditions. This highlights how the type and amount of phonetic input affect listeners' categorization functions. 

```{r fit-nested-models-for-intercepts-slopes-plot, message=FALSE}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_test_nested_within_condition_and_block <-
  fit_model(
    data = d.for_analysis,
    phase = "test",
    formulation = "nested_within_condition_and_block",
    priorSD = 15,
    adapt_delta = .995)

fit_exposure_nested_within_condition_and_block <-
  fit_model(
    data = d.for_analysis,
    phase = "exposure",
    formulation = "nested_within_condition_and_block",
    priorSD = 15,
    adapt_delta = .999)
```

```{r fit-simple-effects-condition, results='hide'}
# Get simple effects of Condition nested under block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_condition <-
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-condition.rds")
```

```{r extract-intercepts-slopes-from-test-and-exposure-models}
d.psychometric_intercept.slope.PSE.draws <-
  full_join(
    fit_test_nested_within_condition_and_block %>% get_intercepts_and_slopes(),
    fit_exposure_nested_within_condition_and_block %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    # Adding unscaled slope here to have a more intuitive scale on which to compare VOT slopes
    # It is also one way to put the slopes for both exposure and test on the same scale (since
    # the SDs for exposure and test differ).
    # intercepts have been centered to VOT means at test therefore are comparable, intercepts for both test and exposure indicate log-odds of response at the same VOT value
    slope_unscaled = slope / (2 * ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test)),
    PSE_scaled =  -Intercept/slope,
    PSE_unscaled =
      descale(
        -Intercept/slope,
        VOT.mean_test,
        ifelse(Block %in% c(2, 4, 6), VOT.sd_exposure, VOT.sd_test)))
```

(ref:plot-fit-PSE) Summary of Bayesian mixed-effect psychometric analysis. **A)** Changes in listeners' categorization functions depending on exposure, from Test 1-4 and intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks). Point ranges indicate the mean proportion of participants' "t"-responses and their 95% bootstrapped CIs. Lines and shaded intervals show the *maximum a posteriori* (MAP) estimates and 95%-CIs of the psychometric model fit to participants' responses. **B)** Same as A) but for the final three test blocks without intervening exposure (Test 4 is repeated for ease of comparison). **Panel C:** Changes across blocks and conditions in listeners' PSE of the lapse-corrected categorization functions from A) \& B) (for changes in the slope of that function, see SI, \@ref(sec:io-bias-correction)). Point ranges represent the posterior medians and 95%-CIs derived from the psychometric model. Horizontal gray ribbon indicates the 95%-CIs of the PSEs expected from an idealized pre-exposure listener. Horizontal dashed lines indicate PSEs expected from an idealized learner. Percentage labels indicate the degree of shift in PSE by participants as a proportion of the expected shift under the idealized learners (for details, see SI, \@ref(sec:shrinkage-test-all)). 

<!-- avoiding extra blank landscape page -->
\afterpage{
  \begin{landscape}
  \thispagestyle{empty}
  
```{r}
# Create data frame for pre-exposure IO. To avoid over-fitting, we use 5-fold cross-validation.
# We use the isolated speech data from Chodroff and Wilson, and randomly cut it into 5 folds.
set.seed(920)
# d.prior <-
#   d.chodroff_wilson.isolated %>%
#   group_by(Talker, category) %>%
#   mutate(fold = sample(1:5, n(), replace = T))

# Make pre-exposure ideal observers based on Chodroff and Wilson *and* ideal observers
# that are fit on the exposure VOTs of the three exposure conditions (idealized learners).
# These ideal observers can be thought of as reflecting a learner that has fully learned
# the exposure distributions. Note that we include perceptual noise in the ideal observers
# (estimated in Kronrod et al., 2016) to make their perception more human-like.
io <-    
  bind_rows(
    # Add prior based on Chodroff & Wilson (2018). Note that this database has substantially higher
    # VOT variance than our exposure conditions, which we based on estimates from Kronrod et al.
    # (2016). Uses all three cues; duration and spectral noise taken from Kronrod et al.
    make_IOs_from_data(
      data = d.prior,
      cues = c("VOT", "f0_Mel", "vowel_duration"),
      group = "fold") %>%
      nest(io = -fold) %>%
      mutate(fold = str_c("prior", fold)) %>%
      rename(Condition.Exposure = fold) %>%
      ungroup() %>%
      # Join in the data that the pre-exposure IOs will be applied to:
      cross_join(
        d.for_analysis %>%
          filter(Phase == "test") %>%
          distinct(Condition.Exposure, Item.VOT, Item.f0_Mel, Item.VowelDuration) %>%
          mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(...))) %>%
          select(x) %>%
          nest(x = x)),
    # Add idealized learner IOs for each exposure condition
   make_IOs_from_data(
      d.for_analysis %>%
        filter(Phase == "exposure") %>%
        group_by(ParticipantID, Condition.Exposure) %>%
        nest(data = -c(ParticipantID, Condition.Exposure)) %>%
        group_by(Condition.Exposure) %>%
        # Subset data to a single participant per exposure condition. This is sufficient since
        # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics
        # (mean and SD) at the end of the experiment, which were identical across participants
        # in each condition.
        #
        # Note that this approach trains the ideal observers on the *actual* VOT distribution
        # that participants heard, not on the theoretical distributions these VOTs were sampled
        # from. This is in line with our goal to simulate behavior of an idealized participant
        # who has fully learned the exposure distributions.
        slice_sample(n = 1) %>%
        unnest(data) %>%
        mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")),
      cues = "VOT",
      groups = "Condition.Exposure") %>%
  nest(io = -c(Condition.Exposure)) %>%
      # Join in the data that the IOs will be applied to. For exposure IOs, only VOT is used for
      # categorizing stimuli (since these IOs are trained on only VOT). Here, we use the test tokens
      # to estimate PSEs for all IOs, and assume that these are constant across all exposure and
      # test blocks. In the SI, we present additional visualizations that obtain PSEs separately
      # for each block. This allows us to detect potential biases in the estimation of PSEs that
      # would result from the distribution of phonetic cues (we don't find any notable biases of
      # that form, which is why we present a simpler approach in the main text).
      left_join(
        d.for_analysis %>%  
          filter(Phase == "test") %>%
          distinct(Condition.Exposure, Item.VOT) %>%
          rename(x = Item.VOT) %>%
          nest(x = x)))

# There are two different ways to obtain PSEs from ideal observers along a specified phonetic continuum.
# One approach is to estimate the *actual* PSE (either analytically or through optimization, as implemented
# in the function get_PSE_from_IO). While this approach avoids the introduction of linearity assumptions,
# this is also its downside: we do not know *listeners' actual PSEs*, but rather estimate them in the
# psychometric mixed-effects model under the assumption of linear effects of the phonetic continuum on
# the log-odds of listeners' "t"-responses. The second approach to estimating PSEs for ideal observers
# avoids this issue by subjecting the ideal observed to the exact same estimation approach used to estimate
# listeners' PSEs. This approach also has the advantage that it allows the estimation of an intercept and
# slope (paralleling what we do for listeners' categorization function). This is the approach we take here.
#
# get_logistic_parameters_from_model() gets PSE, intercept, and slope by repeatedly sampling responses from
# the ideal observer. The function then fits an ordinary logistic regression to these responses (estimation
# of lapsing rates via a psychometric model is unnecessary since the ideal observers are *known* to have 0
# lapses). This replicates for the ideal observers any potential biases that might be introduced by the
# linearity assumption of our psychometric model.
#
# We note here that, in retrospect, our caution might be overkill since analyses in the SI suggest that the linearity
# assumption introduced only minor (non-directional) variation into the intercept, slope, and PSE estimates.
d.IO_intercept.slope.PSE <-
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io,
      model_col = "io",
      groups = "Condition.Exposure") %>%
  select(Condition.Exposure, intercept_unscaled, slope_unscaled, intercept_scaled, slope_scaled, PSE) %>%
  ungroup() %>%
  mutate(across(Condition.Exposure, factor))
```

```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_exposure, d.for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_test, d.for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(2, 4, 6), "exposure", "test")) %>%
  add_block_labels()

p.fit_1to7 <-
  cond_fit_test_exposure %>%
  filter(Block %in% c(1:7))  %>%
  ggplot() +
  geom_rect(
    data =
      tibble(
        Block.plot_label = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
        Block.colour = c("grey", "white", "grey", "white", "grey", "white", "grey")) %>%
      mutate(Block.plot_label = factor(Block.plot_label, levels = Block.plot_label, ordered = T)),
    aes(
      xmin = -Inf, xmax = Inf,
      ymin = 1.05, ymax = 1.31,
      fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "white" = "white")) +
  new_scale_fill() +
  geom_linefit(
    data = d.for_analysis %>%
      group_by(Phase, Condition.Exposure, Block.plot_label) %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE),
    x = Item.VOT,
    y = estimate__,
    fill = NA,
    legend.position = "top",
    legend.justification = "right") +
  coord_cartesian(clip = "off", ylim = c(0, 1))

p.fit_7to9 <-
  cond_fit_test_exposure %>%
  filter(Block %in% c(7:9)) %>%
  ggplot() +
  geom_linefit(
    data = d.for_analysis %>%
      group_by(Condition.Exposure, Block) %>%
      filter(Block %in% c(7:9)),
    x = Item.VOT,
    y = estimate__,
    fill = "grey",
    legend.position = "none") +
  theme(axis.title.y = element_blank())
```

```{r prepare-plot-intercepts-slopes}
# Store ideal PSEs predicted by IOs
predictedPSE_scaled_40 <- get_IO_predicted_PSE(condition = "Shift40")
predictedPSE_scaled_10 <- get_IO_predicted_PSE(condition = "Shift10")
predictedPSE_scaled_0 <- get_IO_predicted_PSE(condition = "Shift0")
predictedPSE_scaled_prior <- get_IO_predicted_PSE(condition = "prior")

# Compute proportional shifts by draw
d.psychometric_intercept.slope.PSE.median <-
  d.psychometric_intercept.slope.PSE.draws %>%
  group_by(Condition.Exposure, Block, .draw) %>%
  arrange(.draw, by_group = T) %>%
  # add the idealized learner PSEs
  mutate(
    predictedPSE_scaled =
      case_when(
        Condition.Exposure == "Shift0" ~ predictedPSE_scaled_0,
        Condition.Exposure == "Shift10" ~ predictedPSE_scaled_10,
        Condition.Exposure == "Shift40" ~ predictedPSE_scaled_40),
    predictedPSE_unscaled = descale(predictedPSE_scaled,  VOT.mean_test, VOT.sd_test)) %>%
  nest(data = -c(Condition.Exposure, .draw)) %>%
  mutate(data = map(data, ~ get_prop_shift_by_draw(.x))) %>%
  unnest(data) %>%
  group_by(Condition.Exposure, Block) %>%
  median_hdci() %>%
  add_block_labels()

pos <- position_dodge(.3)

p.across_blocks <-
  d.psychometric_intercept.slope.PSE.median %>%
  ggplot(
    aes(
      x = Block.plot_label, y = Intercept,
      ymin = Intercept.lower, ymax = Intercept.upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.6, 8.6, 1)),
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = pos, size = 1) +
  geom_linerange(linewidth = .6, position = pos, alpha = .5) +
  stat_summary(geom = "line", position = pos) +
  scale_x_discrete("Block") +
  scale_colour_manual("Condition",
    labels = c("-20ms", "-10ms", "+20ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(
    legend.position = "top",
    axis.text.x = element_text(angle = 22.5, hjust = 1))

 p.PSE_1to9 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms VOT)") +
  aes(y = PSE_unscaled, ymin = PSE_unscaled.lower, ymax = PSE_unscaled.upper) +
  geom_rect(
    data =
      d.IO_intercept.slope.PSE %>%
      filter(str_starts(Condition.Exposure, "prior")) %>%
      summarise(
        PSE.lower = mean(PSE) - sd(PSE) / sqrt(length(PSE)) * 1.96,
        PSE.upper = mean(PSE) + sd(PSE) / sqrt(length(PSE)) * 1.96),
    mapping = aes(xmin = -Inf, ymin = PSE.lower, ymax = PSE.upper, xmax = Inf),
    fill = "#d2d4dc",
    alpha = .3,
    inherit.aes = F) +
  geom_linerange(linewidth = .6, position = pos, alpha = .5) +
  geom_hline(
       data = d.IO_intercept.slope.PSE %>%
         filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")),
       mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
       linetype = 2,
       linewidth = .8,
       alpha = 0.4) +
   coord_cartesian(xlim = c(0.75, 9), clip = "off") +
   theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"), legend.position = "none")
```

```{r plot-fit-PSE, fig.width=12.5, fig.height=7, fig.cap="(ref:plot-fit-PSE)", fig.pos="htbp"}
p.fit_1to7 +
  p.fit_7to9 +
  (p.PSE_1to9 +
     geom_label(
       data = d.psychometric_intercept.slope.PSE.median %>% filter(Block != 1),
       mapping = aes(label = percent(prop.shift_unscaled), colour = Condition.Exposure),
       size = 3,
       position = position_dodge(.5)) +
     annotate(
       geom = "text",
       x = c(rep(10, 3)),
       y = c(25, 35, 65),
       label = d.psychometric_intercept.slope.PSE.median %>%
         mutate(true_shift = paste(round(predictedPSE_unscaled - PSE_unscaled), "(100%)")) %>%
         filter(Block == 1) %>% pull(true_shift),
       colour = colours.condition,
       fontface = "bold",
       size = 2.7) +
     annotate(
       geom = "text",
       x = c(rep(10, 3)),
       y = c(23, 33, 63),
       label = c(rep("idealized learner", 3)),
       colour = colours.condition,
       fontface = "bold",
       size = 2.7)) +
  plot_layout(
    design = "
AAAAAAAAA
BBBDDDDDD
###DDDDDD
",
heights = c(1.1, 1.1, 1.8)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(plot.tag = element_text(face = "bold"))
```

  \end{landscape}
}

```{r}
# Not removing the PSE panel since we replot and augment it further down
rm(fit_exposure, fit_exposure_nested_within_condition_and_block, p.fit_1to7)
```

## Prediction 1 (*prior expectations*)

```{r hypothesis-table-prediction1, results='asis'}
hyp.simple_effects_condition <-
  get_hyp_data(
    fit_test.simple_effects_condition,
    hyp.vec = c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0")) %>% 
  bind_rows(
    get_hyp_data(
      fit_test_nested_within_condition_and_block,
      hyp.vec = c(
        str_c("(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs) -", predictedPSE_scaled_prior, "= 0"),
        str_c("(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) -", predictedPSE_scaled_prior, "= 0"),
        str_c("(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs) -", predictedPSE_scaled_prior, "= 0"),
        str_c("(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x1/mu2_IpasteCondition.ExposureBlocksepEQxShift0x1:VOT_gs + 
          -mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs +
          -mu2_IpasteCondition.ExposureBlocksepEQxShift40x1/mu2_IpasteCondition.ExposureBlocksepEQxShift40x1:VOT_gs)/3 -", predictedPSE_scaled_prior, "= 0")))) 

make_hyp_table(
  fit_test.simple_effects_condition,
  hyp.simple_effects_condition,
  c("$PSE_{-10ms} = PSE_{-20ms}$",
    "$PSE_{+20ms} = PSE_{-10ms}$",
    "$PSE_{+20ms} = PSE_{-20ms}$",
    "$PSE_{-20ms} = PSE_{idealized_{prior}}$",
    "$PSE_{-10ms} = PSE_{idealized_{prior}}$",
    "$PSE_{+20ms} = PSE_{idealized_{prior}}$",
    "$PSE_{actual_{all}} = PSE_{idealized_{prior}}$"),
    caption = "Comparing participants' responses prior to exposure (Test 1) both between exposure conditions and to the idealized pre-exposure listener model. Predicted identities were tested using the Savage-Dickey density ratio (where BF > 1 indicates evidence in favor of the hypothesis that the two PSEs are identical).") %>%
  pack_rows("Comparing conditions against each other (Test block 1)", 1, 3) %>%
  pack_rows("Comparing conditions against idealized pre-exposure listener (Test block 1)", 4, 6) %>% 
  pack_rows("Comparing average PSE against idealized pre-exposure listener (Test block 1)", 7, 7)
```

Figure \@ref(fig:plot-fit-PSE)A suggests that the different participant groups had almost identical categorization functions prior to exposure. This is confirmed by the Bayesian hypothesis tests summarized in the top half of Table \@ref(tab:hypothesis-table-prediction1). During Test 1, before participants received informative exposure to the unfamiliar talker, there was moderate support for the hypothesis that participants' responses did not differ across exposure conditions. <!-- ^[The moderate BFs for these hypothesis tests are due to our use of regularizing priors, which have non-negligible density over the null [for an introduction to the Savage-Dickey method, see @wagenmakers2010]. Rather than to test the null against more plausible alternative priors, which would predictably increase the evidence for the null, we appeal to readers' intuition: the 90%-CIs of the comparisons for Test 1 are all approximately centered around zero; there is very little evidence in favor of an effect in either direction.] --> This is also reflected in the low probabilities of direction ($.5 \le p_{direction} \le .68$). This result is predicted by models of adaptive speech perception under the assumptions that (a) participants in the different groups have similar prior experiences, and that (b) our sample size is sufficiently large to yield stable estimates of listeners' categorization function. <!-- Equality of pre-exposure behavior across exposure groups is also implicitly assumed---but rarely tested---in the interpretation of most studies on adaptive speech perception [when it is tested, it often turns out that this assumption is *not* necessarily warranted, presumably due to insufficient sample sizes, cf. @kleinschmidt2020]. -->

Critically, participants' categorization functions during Test 1 closely matched the predictions of the idealized pre-exposure listener, shown as a gray band in Figure \@ref(fig:plot-fit-PSE)A. This is confirmed by Bayesian hypothesis tests summarized in last two subsections of Table \@ref(tab:hypothesis-table-prediction1), which compared participants' PSE of each condition, as well as the average PSE of all conditions, against the mean PSE of the five cross-validated idealized pre-exposure listeners (BFs $\ge$ `r get_bf(fit_test_nested_within_condition_and_block, str_c("(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x1/mu2_IpasteCondition.ExposureBlocksepEQxShift10x1:VOT_gs) -", predictedPSE_scaled_prior, "= 0"), bf = T)`) This demonstrated that a simple ideal observer trained on the phonetic distributions that a 'typical' listener might have previously experienced provides a decent model of listeners' perception prior to additional exposure [see also, e.g., @kronrod2016; @nearey-assmann2007; @tan2021; @xie2021cognition]. Notably, this is achieved with 0 degrees of freedom that are fit to listeners' responses: the ideal observers were trained solely on a phonetic database.

## Prediction 2b (*exposure distributions*): comparing exposure conditions within each test block

```{r hypothesis-table-simple-effects-condition, results='asis'}
hyp.simple_effects_condition <-
  get_hyp_data(
  fit_test.simple_effects_condition,
  hyp.vec = c(
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0")
)

make_hyp_table(
  fit_test.simple_effects_condition,
  hyp.simple_effects_condition,
  rep(c("$PSE_{-10ms} > PSE_{-20ms}$",
        "$PSE_{+20ms}  > PSE_{-10ms}$",
        "$PSE_{+20ms} > PSE_{-20ms}$"), 5),
    caption = "The simple effects of the exposure conditions for each test block. This analysis asks how early exposure starts to affect participants’ categorization responses, and when (if ever) these changes were undone with repeated testing. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses).") %>%
  pack_rows("Test block 2", 1, 3) %>%
  pack_rows("Test block 3", 4, 6) %>%
  pack_rows("Test block 4", 7, 9) %>%
  pack_rows("Test block 5 (repeated testing without additional exposure)", 10, 12) %>%
  pack_rows("Test block 6 (repeated testing without additional exposure)", 13, 15)
```

Figure \@ref(fig:plot-fit-PSE)A suggests that differences between exposure conditions emerged very rapidly after listeners received informative exposure from the unfamiliar talker. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-condition).^[The perceptual model contained in the our psychometric mixed-effects model describes the effect of VOT on the log-odds of "t"-responses as a line. The main effect of VOT is the average slope of that line across exposure conditions. The $\hat{\beta}$s for the comparisons across conditions indicate differences in the intercept of that line. Negative $\hat{\beta}$s thus indicate a *down*ward shift of that line in one condition, relative to the other. These downward shifts result in *right*ward shifts of the point of subjective equality (PSE), the VOT at which "t" and "d" responses are equally likely. This also shows in Figure \@ref(fig:plot-fit-PSE)A. In this figure, predictions are transformed into proportion "t"-responses and the downward shifts appear visually as a rightward shifts of the S-shaped categorization function (of one condition relative to another).] During Test 2, after exposure to only 24 /d/ and 24 /t/ stimuli (thereof half labeled), participants' categorization responses already differed between exposure conditions (BFs $\ge$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`). All differences between exposure conditions that emerged at Test 2 followed prediction 2b (*exposure distributions*).^[This is also evident in the fact that the probability of direction in Table \@ref(tab:hypothesis-table-simple-effects-condition) was identical to the posterior probability of the tested hypothesis for Tests 2-6: when a directional hypothesis is supported, its posterior probability (under a uniform prior) is the same as the probability of direction.] Additional analyses reported in the SI (\@ref(sec:SI-exposure-block-analysis)) found that listeners' categorization functions had already changed in the predicted direction during the first *exposure* block, in line with Figure \@ref(fig:plot-fit-PSE)A. This suggests that changes in listeners' categorization responses emerged quickly at the earliest point tested---after only a fraction of exposure trials previously tested in similar paradigms. 

The effects of the three exposure conditions persisted until Test 4, always in line with prediction 2b. Indeed, the effects of exposure numerically persisted even after no further informative exposure was provided (Tests 5-6), though the difference between the -20ms and -10ms conditions had mostly vanished by the final test block, leaving only anecdotal support (BF $\ge$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0", bf = T)`). Further comparisons of the differences between exposure conditions during Test 4-6 suggests that the differences between conditions decreased with repeated testing in the absence of further exposure. Indeed, additional hypothesis tests presented in Table \@ref(tab:hypothesis-table-simple-effects-block) in the next section confirm that repeated testing without intermittent exposure in Tests 4-6 substantially reduced the effects of exposure. This replicates similar previous findings from other types of paradigms [e.g., lexically-guided perceptual learning, @scharenborg-janse2013; @giovannone-theodore2021; @cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @zheng-samuel2023]. We leave further discussion of the effects of repeated testing to the SI (\@ref(sec:repeated-testing)) but note one important consequence for future research: analyses that average over test blocks with uninformative exposure (unlabeled inputs with uniform distributions of phonetic cues)---as is still the norm---are bound to systematically underestimate the true adaptivity of human speech perception.

Finally, similar comparisons across Tests 1-4 reveals an interesting non-monotonic development. While the difference between the +20ms condition and both the -20ms and -10ms condition continued to increase numerically with increasing exposure (increasingly larger magnitude of negative estimates in Tests 2-4), the same was not the case for the difference between the -10ms and the -20ms condition. Instead, the difference between the -10ms and -20ms condition *reduced* with increasing exposure (while maintaining its direction; from $\hat{\beta} =$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0", est = T)` to $\hat{\beta} =$ `r get_bf(fit_test.simple_effects_condition, "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0", est = T)`, see Table \@ref(tab:hypothesis-table-simple-effects-condition)). At first blush, this non-monotonicity appears to contradict prediction 2a that the magnitude of exposure effects should increase with increasing exposure. In the next section, we show that the results do, in fact, *support* prediction (2a) when listeners' prior expectations are considered. Indeed, the seemingly unexpected non-monotonicity---which would be impossible to detect without repeated testing---turns out to be important for understanding incremental adaptation.

## Predictions 1-3 together: comparing incremental changes within exposure conditions
Next, we compare how listeners' categorization responses changed from block to block *within* each exposure condition. This allows us to understand changes in listeners' categorization function relative to listeners' pre-exposure behavior, thereby assessing the joint effects of predictions 1  (*prior expectations*) and 2a,b  (*exposure amount & distributions*). To facilitate visual comparison across blocks and conditions, Figure \@ref(fig:plot-fit-PSE)C summarizes the block-to-block changes in listeners' PSE. Focusing on Tests 1-4 with intermittent exposure, this highlights several aspects of participants' behavior that were not readily apparent in the statistical comparisons presented so far.

```{r fit-simple-effects-block, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift0:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift10:VOT_gs"),
  prior(student_t(3, 0, 15), class = "b", dpar = "mu2", coef = "Condition.ExposureShift40:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_block <-
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Condition.Exposure/(Block * VOT_gs) +
      (0 + Block * VOT_gs | ParticipantID) +
      (0 + Condition.Exposure/(Block * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d.for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-block.rds")
```

```{r hypothesis-table-simple-effects-block, results='asis'}
hyp.simple_effects_block <-
  get_hyp_data(
    fit_test.simple_effects_block,
    hyp.vec =
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",                     
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",

      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 + mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",

      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",

      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 + mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0"))

make_hyp_table(
  fit_test.simple_effects_block,
  hyp.simple_effects_block,
    c(rep(c(
  # Comparing differences blocks within conditions
  "Block 1 to 2: decreased PSE",
  "Block 2 to 3: decreased PSE",
  "Block 3 to 4: decreased PSE",
  "{\\em Block 1 to 4: decreased PSE}",
  "Block 4 to 5: increased PSE",
  "Block 5 to 6: increased PSE",
  "{\\em Block 4 to 6: increased PSE}"), 2),
  "Block 1 to 2: increased PSE",
  "Block 2 to 3: increased PSE",
  "Block 3 to 4: increased PSE",
  "{\\em Block 1 to 4: increased PSE}",
  "Block 4 to 5: decreased PSE",
  "Block 5 to 6: decreased PSE",
  "{\\em Block 4 to 6: decreased PSE}"),
    caption = "Was there incremental change from test block 1 to 4? Did these changes dissipate with repeated testing from block 4 to 6? This table summarizes the simple effects of block for each exposure condition. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \`\`t\'\'-responses). Italicized rows assess incremental changes across multiple block, increasing the sensitivity to detect effects.") %>%
  pack_rows("Difference between blocks: -20ms", 1, 7) %>%
  pack_rows("Difference between blocks: -10ms", 8, 14) %>%
  pack_rows("Difference between blocks: +20ms", 15, 21)
```

First, the relative changes in participants' PSE from pre-exposure to post-exposure---i.e., from Test 1 to Test 4---all are in the direction predicted by the idealized learner models, and thus in line with the condition names (which were based on the PSEs expected from an idealized learner). Both the -10ms and the -20ms condition seem to shift *left*wards relative to their pre-exposure starting point in Test 1. Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-block) find moderate support for a leftward shift from Test 1 to 4 in both the -10ms condition (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0", bf = T)`) and the -20ms condition (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0", bf = T)`). In contrast, there was strong support that the +20ms condition shifted rightwards relative to pre-exposure again in line with the condition name (BF = `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0", bf = T)`).

Importantly, these predictions of the idealized learner models were based on the placement of our exposure conditions *relative to the phonetic distributions that a 'typical' listener would have experienced prior to our experiment* (recall Figure \@ref(fig:prior-distributions-and-exposure-conditions)A). That is, once we take into account how our exposure distributions (prediction 2b) relate to listeners' prior experience (prediction 1), both the direction of changes from Test 1 to 4 *within* each exposure condition (Table \@ref(tab:hypothesis-table-simple-effects-block)), and the direction of differences *between* exposure conditions receive an explanation (Table \@ref(tab:hypothesis-table-simple-effects-condition)). To the best of our knowledge, this is not something tested in previous distributional learning studies: to confirm or reject the joint effect of predictions 1 and 2b, one needs (1) both pre- and post-exposure tests and (2) informative baseline expectations like those provided by our idealized pre-exposure listener and idealized learners. Both have largely been lacking from previous work. 

Similarly, our next two observations are made possible only through the use of incremental testing. This includes support for both predictions 2a (*exposure amount*) and 3 (*diminishing returns*). Regarding prediction 2a, the incremental changes in participants' PSEs from Test 1 to 4 were consistently in the same direction (same sign of estimate), depending only on the exposure condition. Put differently, the shift in participants' categorization functions away from their pre-exposure PSE kept increasing with additional exposure. 

Regarding prediction 3, at least the two conditions that exhibited the largest shifts relative to pre-exposure, the -20ms and +20ms conditions, seem to show diminishing returns of additional exposure. For these two conditions, PSEs changed most from Test 1 to Test 2, followed by much smaller block-to-block changes up to Test 4 (smaller magnitude of estimates in Table \@ref(tab:hypothesis-table-simple-effects-block)). As mentioned in our Open Science statement, our experiment was not designed to test such *changes in the magnitude of the shifts* across the block within each condition. We did, however, conduct post-hoc hypothesis tests to assess the support for this pattern. These tests, reported in more detail in the SI (\@ref(sec:PSE-interaction-analysis)), found anecdotal to moderately strong evidence in support of prediction 3. For the +20ms condition, the shift from Test 1 to 2 was larger than the shift from Test 2 to 3 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < mu2_Condition.ExposureShift40:Block_Test3vs.Test2")`), which was larger than the shift from Test 3 to 4 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < mu2_Condition.ExposureShift40:Block_Test4vs.Test3")`). Comparing the change from Test 1 to 2 against the change from Test 3 to 4, there was stronger support that the speed of changes in the PSE decreased (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < mu2_Condition.ExposureShift40:Block_Test4vs.Test3")`). For the -20ms condition, the shift from Test 1 to 2 was larger than the shift from Test 2 to 3 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift0:Block_Test3vs.Test2")`), which was almost identical, but slightly smaller, than the shift from Test 3 to 4 (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > mu2_Condition.ExposureShift0:Block_Test3vs.Test2")`). Again, a comparison of the change from Test 1 to 2 against the change from Test 3 to 4, yielded the strongest support that the speed of changes in the PSE decreased (`r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift0:Block_Test4vs.Test3")`). In line with this pattern of decreasing magnitudes of changes in the PSE, there was only anecdotal evidence for all three exposure conditions that the final exposure block resulted in any additional shift in listeners' PSE (BFs $\leq$ `r get_bf(fit_test.simple_effects_block, "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0", bf = T)`, cf. Table \@ref(tab:hypothesis-table-simple-effects-block)).

```{r, include=F}
# Assessing evidence for changes in the PSEs within each condition
hypothesis(
  fit_test.simple_effects_block,
  c(
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > mu2_Condition.ExposureShift10:Block_Test3vs.Test2",
    "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3"),
  robust = T)
```

Finally, the incremental changes summarized in Table \@ref(tab:hypothesis-table-simple-effects-block) also begin to illuminate the reasons for the non-monotonic development of the -10ms and -20ms conditions relative to each other, discussed in the previous section. In particular, this non-monotonicity does *not* appear due to a reversal of the effects in either of the two exposure conditions. Rather, both exposure conditions continue to change listeners' categorization function in the same direction from Test 1 to Test 4. However, after the rapid change from the pre-exposure Test 1 to the first post-exposure Test 2, listeners' categorization responses in the -20ms condition did not change as much as in the -10ms condition. In fact, listeners' categorization function in the -20ms condition seems to have plateaued after the first exposure block. This explains the reduced difference between the -10ms and -20ms conditions discussed in the previous section. It does, however, raise the question *why* listeners' responses in the -20ms condition did not change further with increasing exposure. One explanation could be that participants in the -20ms condition did for some reason---including chance---fully learn the relevant phonetic distributions within a single block of exposure, whereas participants in the -10ms condition did not. However, the hypothesis tests we present next suggest that this was *not* the case. 

## Prediction 4 (*full convergence*): an unexpected constraint on rapid adaptation?

```{r hypothesis-table-convergence-test4-pre-exposure-PSE, results='asis'}
hyp.shrinkage <-
  get_hyp_data(
    fit_test_nested_within_condition_and_block,
    hyp.vec =
    c(
      str_c("-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, "= 0" ),
      str_c("-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, "= 0" ),
      str_c("-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, "= 0" ),      
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift0x7/mu2_IpasteCondition.ExposureBlocksepEQxShift0x7:VOT_gs  - ", predictedPSE_scaled_0, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift10x7/mu2_IpasteCondition.ExposureBlocksepEQxShift10x7:VOT_gs - ", predictedPSE_scaled_10, ") > 0" ),
      str_c("abs(-mu2_IpasteCondition.ExposureBlocksepEQxShift40x7/mu2_IpasteCondition.ExposureBlocksepEQxShift40x7:VOT_gs - ", predictedPSE_scaled_40, ") > 0" )))

make_hyp_table(
  fit_test_nested_within_condition_and_block,
  hyp.shrinkage,
  c(
    "$PSE_{idealized_{-20ms}} = PSE_{actual_{-20ms}}$",
    "$PSE_{idealized_{-10ms}} = PSE_{actual_{-10ms}}$",
    "$PSE_{idealized_{+20ms}} = PSE_{actual_{+20ms}}$",
    "$|\\Delta(PSE_{idealized_{-20ms}}, PSE_{actual_{-20ms}})| > 0$",
    "$|\\Delta(PSE_{idealized_{-10ms}}, PSE_{actual_{-10ms}})| > 0$",
    "$|\\Delta(PSE_{idealized_{+20ms}}, PSE_{actual_{+20ms}})| > 0$"),
  caption = "Did participants converge against the PSE expected from an idealized learner? This table compares changes in participants' categorization function against those expected from idealized learners. The first set of tests assess whether participants converged against the predicted PSEs of the idealized learners. The second set asseses the absolute distance between the actual and idealized PSEs. For identical hypothesis tests for all test blocks, see SI (\\ref{sec:convergence-test-all}).",
  col1_width = "20em") %>%
  pack_rows("Did participants converge against the idealized learner? (Test block 4)", 1, 3) %>%
  pack_rows("Did participants' PSEs shift less than expected from idealized learner? (Test block 4)", 4, 6) 
```

Taken together, the tests we have presented so far provide rather consistent support for the predictions of distributional learning. However, Figure \@ref(fig:plot-fit-PSE)C suggests an important caveat to this: the comparison of participants' responses against those of an idealized learner that has fully learned the exposure distributions (colored dashed lines) suggests that listeners in all three exposure conditions did *not* fully learn the exposure distributions. This is confirmed by the Bayesian hypothesis tests in Table \@ref(tab:hypothesis-table-convergence-test4-pre-exposure-PSE), which provide little support for convergence and very strong support for the hypothesis that participants reliably shifted their PSE *less* than expected from an idealized learner. By itself, a failure to converge against the performance of an idealized learner would not necessarily constitute evidence against prediction 4 (*learning to convergence*). Listeners might simply not have received sufficient exposure to have learned the exposure distribution. However, as already described for the -20ms condition, participants' behavior changed little, if at all, after the first exposure block. Instead, participants seem to have prematurely converged against stable behavior long before they had fully learned the exposure distribution.

The percentage labels in Figure \@ref(fig:plot-fit-PSE)C quantify the degree to which participants adapted their PSE towards the statistics of the exposure condition: 0% would correspond to no change relative to the listeners' PSE in Test 1, and 100% would correspond to the PSE predicted for an idealized learner who has fully converged against the exposure distributions (see Methods). In the -20ms condition, changes in participants' PSE seem to converge against approximately `r d.psychometric_intercept.slope.PSE.median %>% filter(Condition.Exposure == "Shift0", Block == 7) %>% pull(prop.shift_scaled) %>% percent()` of what is expected from an idealized learner. A similar pattern of premature convergence is evident for the +20ms condition: changes in participants' PSEs seem to have leveled off by Test 4, despite the fact that participants' PSEs had shifted only about half way to the idealized learner's PSE. (For the -10ms condition, it is less clear whether participants had already converged against a PSE.) That is, in terms of the possible adaptation scenarios discussed in the introduction, it seems that our results most closely resemble the scenario of *premature convergence*, shown in the rightmost panel of Figure \@ref(fig:predictions)D.^[Figure \@ref(fig:plot-fit-PSE)C would also seem to suggest that the degree of convergence *differed between exposure conditions. Two previous studies have observed---but not analyzed---similar patterns, with more extreme exposure shifts eliciting *proportionally* smaller changes in PSEs than less extreme exposure shifts [@kleinschmidt-jaeger2016; @kleinschmidt2020]. The SI presents additional Bayesian hypothesis tests that assess how strongly our data support this pattern. These tests find only anecdotal support (see SI, \@ref(sec:shrinkage-test-all)). Still, this is an intriguing pattern that deserves further attention in future research.]

If confirmed, premature convergence against stable behavior despite only partial adaptation would challenge prediction 4 of existing distributional learning models. Premature convergence is also unexpected under any other popular theories of adaptive speech perception (e.g., standard normalization models or changes in decision-making). It might, however, be compatible with proposals that view rapid adaption during speech perception as a form of reweighting of existing representations, rather than as a process of learning new representations [@xie2018]. We return to this idea in the general discussion, where we also discuss potential methodological artifacts that might explain the appearance of premature convergence. First, however, we present one final analysis, comparing listeners' responses to an actual distributional learning model. Beyond its primary goal, this additional comparison also provides further evidence of premature convergence.

## How much of the changes in participants' perception can distributional learning explain?
The tests we have presented so far suggest that participants' behavior qualitatively match predictions 1-3 of distributional learning models, but not prediction 4. Going beyond previous work, we found that the relative magnitude of changes in participants' behavior ordered in the expected way both across three exposure conditions and across the test blocks within each condition. These findings address the first two goals for the present study outlined in the introduction: to provide the first tests of predictions 3 and 4 during the early moments of adaptation to an unfamiliar talker, and to jointly test predictions 1-4 in a single study with *known distributional exposure*, while comparing against the predictions of idealized listeners/learners based on the relevant phonetic distributions prior to, and during, exposure.

Next, we turn to the third and final contribution we aim to make here: to assess whether distributional learning can actually explain a non-trivial share of the changes in participants' perception, as would be expected if rapid adaptive changes in speech perception are in large parts due to distributional learning. To this end, we fit an actual distributional learning model to our data [the ideal adaptor, @kleinschmidt-jaeger2015]. Like the ideal observer models that we used for the idealized listener and learners, the ideal adaptor is a Bayesian model that describes a ideal information integration. Unlike the ideal observers, however, the ideal adaptor is a distributional *learning* model. That is, while the ideal observers have perfect knowledge of the relevant phonetic distributions (e.g., the exposure distribution for the idealized learners), the ideal adaptor *infers* those distributions from the input it receives, modeling the incremental learning process that listeners are hypothesized to undergo during exposure to an unfamiliar talker. This is achieved by incrementally updating prior beliefs about the phonetic distributions of the relevant categories (here /d/ and /t/) based on the phonetic input experienced during exposure.

The model is described in detail in the SI (\@ref(sec:ideal-adaptor)), including graphical models [for an introduction, see @xie2023]. In order to model the effects of all three cues considered in the idealized models (VOT, f0, and vowel duration), we extended the model to operate over multivariate categories, and fit it to listeners' responses in our experiment. We used the \texttt{MVBeliefUpdatr} package [@R-MVBeliefUpdatr] to define and fit the model. For further detail, we refer to the SI (\@ref(sec:ideal-adaptor)). Unlike in previous work, we fixed the models prior beliefs based on the same phonetic database used for the idealized pre-exposure listener [cf. @kleinschmidt-jaeger2016]. Specifically, we used prior beliefs that had the same expected distributions of VOT, f0, and vowel duration for /d/ and /t/ as in the idealized listener model. This substantially reduces the number of parameters in the model that are fit to participants' responses: in addition to a lapse rate ($\lambda$), the ideal adaptor we fit only had two degrees of freedom. 

These two parameters ($\kappa_0$ and $\nu_0$) describe the strength of participants' prior beliefs about the category means and (co)variances, respectively. Together, $\kappa_0$ and $\nu_0$ describe the uncertainty that listeners have about the unfamiliar talker's phonetic distributions prior to exposure (or, equivalently, how relevant listeners believe their prior expectations to be for the unfamiliar talker). The same two parameters also determine the rate at which the model updates its beliefs about the exposure distributions with each new observation from that talker. This captures how prior expectations both guide and constrain distributional learning [for discussion, see @kleinschmidt-jaeger2015; @kleinschmidt2020]. Specifically, $\kappa_0$ and $\nu_0$ can be understood as pseudocounts that describe the weighting of prior expectations relative to the observed data. For example, if $\kappa_0 = 100$ then it takes 100 observations of /d/s from the unfamiliar talker until listeners' beliefs about the VOT, f0, and vowel duration values of the category means for /d/ will be a 50/50 mixture of their prior beliefs and the means observed from the unfamiliar talker.

```{r fit-IA-inferred-VOT-f0}
d_for_ASP <-
  d.for_analysis %>%
  ungroup() %>%
  # Exclude catch trials and the final two test blocks since we expect unlearning during those blocks
  # (which is not modeled)
  filter(
    !(Phase == "exposure" & is.na(Item.ExpectedResponse.Voicing)),
    as.numeric(Block) <= 7) %>%
  # Use F0 as intended by generation script. This makes fitting more computationally feasible
  # (336 instead of 1001 unique combinations of group & test locations), and also removes the
  # potential that annotation / measurement error perturb the f0 values. It does, however,
  # make the assumption that f0 is the same across the three minimal pairs. While likely wrong,
  # this assumption better aligns with the fact that the model doesn't have a way to keep track
  # of any lexical, phonological, or phonetic context effects on VOT (and thus no way to predict
  # any potential differences between minimal pairs).
  # Create new condition variable that uniquely identifies what participants have seen at any of the
  # tests and a new block variable that identifies the type and order of blocks. Also create two
  # convenience variables, x (stimulus) and Response.Category (to make matching with the category
  # column of ideal observers/adaptors more straightforward).
  mutate(
    Condition.for_ASP = paste0(Condition.Exposure, List.ExposureBlockOrder, List.ExposureMaterials),
    Block.for_ASP = paste0(Phase, Block),
    x = pmap(.l = list(VOT, f0_Mel, vowel_duration), ~ c(...)),
    Response.Category = factor(ifelse(Response.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/")),
    Item.ExpectedResponse.Category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/"))) %>%
  slice_into_unique_exposure_test(
    condition_var = "Condition.for_ASP",
    block_var = "Block.for_ASP",
    block_order = c("test1", "exposure2", "test3", "exposure4", "test5", "exposure6", "test7")) %>%
  mutate(ExposureGroup = factor(ExposureGroup)) %>%
  select(
    ExposureGroup, Phase, ParticipantID, Block, Trial, x,
    VOT, f0_Mel, vowel_duration, Response.Category, Item.ExpectedResponse.Category)

# fit ideal adaptor with informative priors
# use prior roughly in middle of 5-fold CV
m_IO.VOT_f0_vowel <-
  io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%
  select(-c(mu, Sigma)) %>%
  filter(Condition.Exposure == "prior1") %>%
  mutate(Condition.Exposure = "CV_prior.mean") %>%
  right_join(
    io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%  
  unnest_wider(mu) %>%
    group_by(category) %>%
  summarise(across(c(VOT, f0_Mel, vowel_duration), mean)) %>%
  mutate(mu = pmap(.l = list(VOT, f0_Mel, vowel_duration),
    # Ensure names else it won't be recognized as an MVG IO
    ~ set_names(c(...), c("VOT", "f0_Mel", "vowel_duration")))) %>%
  select(category, mu)) %>%
  right_join(
    io %>%
  filter(str_starts(Condition.Exposure, "prior")) %>%
  select(-x) %>%
  unnest(io) %>%
  select(Condition.Exposure, category, Sigma) %>%
  group_by(category) %>%
  summarise(Sigma = list(reduce(Sigma, `+`)/length(Sigma))) %>%
  group_by(category))

# fit ideal adaptor with 3 cue priors
m_IA.VOT_f0_vowelduration_informedprior <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = m_IO.VOT_f0_vowel$mu,
    Sigma_0 = map2(m_IO.VOT_f0_vowel$Sigma, m_IO.VOT_f0_vowel$Sigma_noise, `+`),
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_informative_priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))
```

```{r summarise-PSE-predictions-of-IA}
# Get logistic model predictions of the IA's categorization
# then prepare the data for plotting
if (file.exists("../models/d.IA_predicted_test_informedprior.rds") & !RESET_FIGURES) {
  d.IA_predicted_test_informedprior <- readRDS("../models/d.IA_predicted_test_informedprior.rds")
} else {
  data.test <- prep_data_for_IBBU_prediction(m_IA.VOT_f0_vowelduration_informedprior)

  d.IA_predicted_test_informedprior <-
    get_IBBU_predicted_response(
      m_IA.VOT_f0_vowelduration_informedprior,
      data = data.test,
      groups = get_group_levels_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior))

  saveRDS(d.IA_predicted_test_informedprior, "../models/d.IA_predicted_test_informedprior.rds")
}

if (file.exists("../models/d.IA_predicted_PSE_informedprior.rds") & !RESET_FIGURES) {
  d.IA_predicted_PSE_informedprior <- readRDS("../models/d.IA_predicted_PSE_informedprior.rds")
} else {
  d.IA_predicted_PSE_informedprior <-
    fit_logistic_regression_to_model_categorization(d.IA_predicted_test_informedprior) %>%
    group_by(group) %>%
    median_hdci(PSE) %>%
    # duplicate the no-exposure row; 1 for each condition
    mutate(rows = ifelse(group == "no exposure", 3, 1)) %>%
    uncount(rows) %>%
    mutate(
      Condition.Exposure = factor(c(rep("Shift0", 3), rep("Shift10", 3), rep("Shift40", 3), c("Shift0", "Shift10", "Shift40"))),
      Block = ifelse(group != "no exposure", str_replace(group, ".*(\\d)", "\\1"), group),
      Block = factor(ifelse(group == "no exposure", 1, Block))) %>%
    add_block_labels()

  saveRDS(d.IA_predicted_PSE_informedprior, "../models/d.IA_predicted_PSE_informedprior.rds")
}
```

```{r get-talker-specific-PSEs}
# As mentioned in the result section, there are two different ways to obtain PSEs from ideal observers.
# Here we provide the code for both but continue to use the same method as in the result section. This
# is the method that introduces the same potential biases into the estimates of the ideal observers that
# *might* affect our estimates of listeners' PSE, intercept, and slope.
# get the mean PSE of the 5-fold CV idealized pre-exposure listener PSEs
prior.PSE_mean <-
  d.IO_intercept.slope.PSE %>%
  filter(Condition.Exposure %in% paste0("prior", c(1:5))) %>%
  summarise(mean = mean(PSE)) %>%
  pull(mean)

d.talkerPSEs <-
  # Make separate ideal observer for each talker in the database
  make_IOs_from_data(
    data = d.chodroff_wilson,
    cues = c("VOT", "f0_Mel", "vowel_duration"),
    groups = c("speechstyle", "Talker", "gender")) %>%
  nest(io = -c(speechstyle, Talker, gender)) %>%
  # Get actual PSE of IO
  add_x_to_IO() %>%
  add_PSE_and_categorization_to_IO() %>%
  rename(PSE_actual = PSE) %>%
  select(-c(x, categorization)) %>%
  # Get PSE estimated through logistic regression
  cross_join(
    d.for_analysis %>%  
      filter(Phase == "test") %>%
      distinct(Item.VOT, Item.f0_Mel, Item.VowelDuration) %>%
      mutate(x = pmap(.l = list(Item.VOT, Item.f0_Mel, Item.VowelDuration), ~ c(...))) %>%
      select(x) %>%
      nest(x = x)) %>%
  left_join(
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = .,
      model_col = "io",
      groups = "Talker") %>%
      select(Talker, PSE) %>%
      rename(PSE_logistic = PSE)) %>%
  # For each of the two types of PSE, also create a version that is centered around
  # listeners mean PSE during Test 1 (to make it easier to compare the range of variability)
  group_by(speechstyle) %>%
  mutate(across(starts_with("PSE"), list(centered = ~ .x - (mean(.x) - prior.PSE_mean))))

# Visually investigate the distribution of PSE before aggregating them down further
# d.talkerPSEs %>%
#   select(c(speechstyle, starts_with("PSE"))) %>%
#   pivot_longer(
#     cols = starts_with("PSE"),
#     names_to = "PSE_type",
#     values_to = "PSE") %>%
#   # Filter implausible PSEs (there's one talker with a < - 100 PSE)
#   filter(PSE > 0) %>%
#   ggplot(aes(x = PSE_type, y = PSE, color = speechstyle)) +
#   geom_violin(position = pos) +
#   stat_summary(fun.data = mean_cl_boot, geom = "pointrange", position = pos) +
#   geom_point(position = "jitter", alpha = .5)

d.talkerPSEs %<>%
  summarise(
    across(
      starts_with("PSE"),
      list(
        mode = ~ modeest::mlv(.x, method = "meanshift"),
        mean = mean,
        median = median,
        lower = ~ quantile(., probs = .025),
        upper = ~ quantile(., probs = .975)),
      .names = "{.col}.{.fn}"))

# d.talkerPSEs %>%
#   pivot_longer(
#     cols = starts_with("PSE"),
#     names_to = "PSE_type",
#     values_to = "PSE") %>%
#   separate(PSE_type, into = c("PSE_type", "stat"), sep = "\\.") %>%
#   pivot_wider(names_from = stat, values_from = PSE) %>%
#   ggplot(aes(x = PSE_type, y = mean, ymin = lower, ymax = upper, color = speechstyle)) +
#   geom_pointrange(position = pos)
```

(ref:plot-IA-human-PSE) Summary of ideal adaptor analysis, comparing shifts in listeners' PSEs against the shifts expected by an ideal adaptor [a distributional learning model that describes ideal information integration, @kleinschmidt-jaeger2015]. Point ranges and dashed horizontal lines are identical to those in Figure \@ref(fig:plot-fit-PSE)C. Colored ribbons indicate the 95%-CI for the PSEs predicted by the ideal adaptor fit to listeners' responses. The vertical bar to the right of the panel indicates the range of talker-specific PSEs an idealized listener might have experienced in previous exposure: the mean and 2.5%-to-97.5% quantile range of talker-specific PSEs derived from Bayesian ideal observers fit to all talkers in the phonetic database of isolated word productions presented in Figure \@ref(fig:prior-distributions-and-exposure-conditions)A. We return to this range in the general discussion where we discuss possible explanations for mismatches between the ideal adaptor and participants' responses.

```{r plot-IA-human-PSE, fig.width=6.5, fig.height=3.5, fig.cap="(ref:plot-IA-human-PSE)", fig.pos="ht"}
pos <- position_dodge(.3)
d.psychometric_intercept.slope.PSE.median %>%
  filter(Block %in% c(1:7)) %>%
  ggplot(
    aes(
      x = Block.plot_label,
      y = PSE_unscaled,
      ymin = PSE_unscaled.lower, ymax = PSE_unscaled.upper,
      colour = Condition.Exposure,
      group = Condition.Exposure)) +
  geom_point(position = pos, size = 1.8) +
  geom_linerange(linewidth = .6, position = pos, alpha = .75) +
  stat_summary(geom = "line", position = pos) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2)),
      xmax = c(seq(1.5, 7.5, 2))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1,
    inherit.aes = F) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_ribbon(
    data = d.IA_predicted_PSE_informedprior,
    mapping = aes(x = Block.plot_label, ymin = .lower, ymax = .upper, fill = Condition.Exposure, group = Condition.Exposure),
    alpha = .1,
    position = pos,
    inherit.aes = F) +
  # TO DO (here and elsewhere): this should probably be an annotation
  geom_hline(
    data = d.IO_intercept.slope.PSE %>% filter(Condition.Exposure %in% c("Shift0", "Shift10", "Shift40")),
    mapping = aes(yintercept = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2,
    linewidth = .8,
    alpha = 0.4) +
  annotate(
    geom = "text",
    data = d.psychometric_intercept.slope.PSE.median %>%
      mutate(true_shift = predictedPSE_unscaled - PSE_unscaled) %>%
      filter(Block == 1) %>% pull(true_shift),
      x = c(8, 8, 8),
    y = c(25, 35, 65),
      label = d.psychometric_intercept.slope.PSE.median %>%
      mutate(true_shift = paste(round(predictedPSE_unscaled - PSE_unscaled), "(100%)")) %>%
      filter(Block == 1) %>% pull(true_shift),
    colour = colours.condition,
    fontface = "bold",
    size = 2.7) +
  annotate(
    "crossbar",
    x = 7.7,
    y = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.mean,
    ymin = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.lower,
    ymax = d.talkerPSEs[d.talkerPSEs$speechstyle == "isolated", ]$PSE_logistic.upper,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  labs(y = "PSE (ms VOT)", x = "Block") +
  scale_colour_manual(
    "Condition",
    labels = c("-20ms", "-10msms", "+20ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  coord_cartesian(xlim = c(0.75, 7), clip = "off") +
  theme(
    axis.text.x = element_text(angle = 22.5, hjust = 1),
    plot.margin = unit(c(1, 3.2, 1, 1), "lines"),
    legend.position = "none")
```

Critically, such a distributional learning model is *much* more constrained in the type of changes it can fit than the psychometric model we have used above to analyze our data. The psychometric model makes no assumptions whatsoever about the way in which listeners' perception changes during exposure, and is thus free to fit any pattern of changes in participants' responses. This is, of course, desired for the evaluation predictions 1-4: the findings so far are noteworthy precisely because the psychometric analysis could have just as well found that participants' behavior was completely unrelated to, or even directly contradicted, the predictions of distributional learning. However, psychometric analyses cannot address whether the totality of all changes in participants' perception across exposure conditions and test blocks is actually consistent with the predictions of distributional learning. For example, the psychometric analysis leaves open whether distributional learning can explain both the rapid changes at the start of exposure *and* the slower changes that occur later in exposure. In contrast, a distributional learning model specifies how information is integrated across exposure trials and blocks. As a consequence, listeners' responses during different test blocks and across different exposure conditions jointly constrain the parameters of model (e.g., for the ideal adaptor, due to assumption of ideal information integration within and across exposure blocks). To appreciate just how different the ideal adaptor is from the psychometric analyses we have presented so far consider that the latter used `r sum(str_detect(rownames(fixef(fit_test)), "mu2")) - sum(str_detect(rownames(fixef(fit_test)), "Test5vs.Test4|Test6vs.Test5"))` population-level ('fixed') and `r length(get_variables(fit_test)[grep("^sd_|^cor_", get_variables(fit_test))]) - sum(str_detect(get_variables(fit_test)[grep("^sd_|^cor_", (get_variables(fit_test)))], "Block_Test6vs.Test5|Block_Test5vs.Test4"))` group-level ('random') effect parameters, whereas the ideal adaptor uses a total of 3 parameters! It is thus possible that a distributional learning model does *not* provide a good explanation for the quantitative changes in listeners' perception even when all qualitative predictions of distributional learning are met.

```{r compute-R2-between-ideal-adaptor-and-psychometric-model}
psych.model_prediction <-
  epred_draws(
    fit_test,
    newdata = d.for_analysis %>%
      filter(Phase == "test") %>%
      group_by(Item.VOT, Block, Condition.Exposure) %>%
      prepVars(test_mean = VOT.mean_test, levels.Condition = levels_Condition.Exposure) %>%
      filter(Block %in% c(1, 3, 5, 7)),
    seed = 928,
    re_formula = NULL,
    ndraws = 1000) %>%
  ungroup() %>%
  # Collapse all observations in Test 1 into 1 condition
  # (to parallel how the ideal adaptor treats the data)
  mutate(
    Condition.Exposure = as.character(Condition.Exposure),
    Condition = ifelse(Block == 1, "pre-exposure", Condition.Exposure),
    Condition.Exposure = factor(Condition.Exposure)) %>%
  group_by(Item.VOT, Condition.Exposure, Condition, Block) %>%
  summarise(predicted_proportion_voiceless = mean(.epred)) %>%
  mutate(predicted_proportion_voiced = 1 - predicted_proportion_voiceless) %>%
  crossing(List.ExposureBlockOrder = c("A", "B", "C")) %>%
  left_join(d.for_analysis %>%
  filter(Phase == "test", Block %in% c(1, 3, 5, 7)) %>%
  group_by(Item.VOT, List.ExposureBlockOrder, Block, Condition.Exposure) %>%
  mutate(Block = factor(Block)) %>%  
  summarise(
    proportion_voiceless = mean(Response.Voiceless),
    proportion_voiced = 1 - proportion_voiceless)) %>%
  group_by(Condition) %>%
  mutate(List = ifelse(Condition == "pre-exposure", "A", List.ExposureBlockOrder)) %>%
  group_by(Item.VOT, List, Block, Condition) %>%
  summarise(across(c(predicted_proportion_voiceless, predicted_proportion_voiced, proportion_voiceless, proportion_voiced), mean))

IA.prediction <-
  d.IA_predicted_test_informedprior %>%
  group_by(group, VOT) %>%
  summarise(across(Predicted_posterior, mean)) %>%
  mutate(ExposureGroup = ifelse(group == "prior", "no exposure", group)) %>%
  left_join(get_test_data_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior) %>%
              get_untransform_function_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior)(.)) %>%
  mutate(
    Response = `/t/` / (`/d/` + `/t/`),
    group = gsub("[ABC]A", "", group))
```

```{r data-prep-IA-logodds-prediction-human-responses}
# Prep data to obtain IA predicted logodds by condition and VOT up to test block 4
if (file.exists("../models/d.IA_predicted_logodds.csv") & !RESET_MODELS) {
  d.IA_predicted_logodds <- read_csv("../models/d.IA_predicted_logodds.csv", show_col_types = F)
} else {
  
  data.test_for_IA_predicted_logodds <-
    prep_data_for_IBBU_prediction(m_IA.VOT_f0_vowelduration_informedprior) %>%
    mutate(
      # Make additional rows for test block 1 so that there is 1 for each condition
      rows = ifelse(group == "no exposure", 3, 1)) %>%
    uncount(rows) %>%
    mutate(
      Condition.Exposure = factor(c(rep("Shift0", 9), rep("Shift10", 9), rep("Shift40", 9), c("Shift0", "Shift10", "Shift40"))),
      Block = ifelse(group != "no exposure", str_replace(group, ".*(\\d)", "\\1"), group),
      Block = factor(ifelse(group == "no exposure", 1, Block)))
  
  # Get the IA predicted logodds and join with the test data frame
  d.IA_predicted_logodds <-
    get_IBBU_predicted_response(
      m_IA.VOT_f0_vowelduration_informedprior,
      data = data.test_for_IA_predicted_logodds,
      groups = get_group_levels_from_stanfit(m_IA.VOT_f0_vowelduration_informedprior),
      logit = T) %>%
    group_by(VOT, Condition.Exposure, Block) %>%
    # get the modal posterior draw
    mode_hdci(Predicted_posterior) %>%
    rename(
      Item.VOT = VOT, 
      IA_predicted_logodds = Predicted_posterior) %>%
    right_join(
      d.for_analysis %>% filter(Block %in% c(1, 3, 5, 7)) %>% mutate(Block = factor(Block)),
      by = c("Item.VOT", "Condition.Exposure", "Block")) %>%
    group_by(Item.VOT) %>%
    mutate(
      IA_predicted_logodds.prior = mean(ifelse(Block == 1, IA_predicted_logodds, NA), na.rm = T),
      IA_predicted_logodds.change = IA_predicted_logodds - IA_predicted_logodds.prior) %>%
    # Separate the change in logodds by block into intercept changes, and slopes
    group_by(Condition.Exposure, Block) %>%
    mutate(
      IA_predicted_logodds.changes_in_intercept_across_block_condition = mean(IA_predicted_logodds.change),
      IA_predicted_logodds.changes_in_slope_across_block_condition = IA_predicted_logodds.change - IA_predicted_logodds.changes_in_intercept_across_block_condition) %>%
    droplevels() %>%
    ungroup()
  
  write_csv(d.IA_predicted_logodds, "../models/d.IA_predicted_logodds.csv")
}
```

```{r psychometric-model-block-1to4}
# Fit a reduced psychometric model of human data (test blocks 1 to 4) for LOO comparability with the IA_predicted logodds model
fit_test.upto.test4 <-
  fit_model(
    d.IA_predicted_logodds,
    phase = "test",
    priorSD = 15,
    formulation = "standard-upto-test4",
    adapt_delta = .995
  )
fit_test.upto.test4 <- add_criterion(fit_test.upto.test4, criterion = "loo", moment_match = TRUE)
```

```{r fit-IA-predicted-logodds-human-responses}
# Fit linear model to human responses with IA-predicted-logodds as predictor
# to assess how well IA predictions fit listeners' behaviour and how it changes from block to block
# block is treated as linear
d.IA_predicted_logodds %<>%
  mutate(Block = as.numeric(
    case_when(
      Block == "1" ~ 1,
      Block == "3" ~ 2,
      Block == "5" ~ 3,
      Block == "7" ~ 4)),
    Condition.Exposure = factor(Condition.Exposure))

# Scale predictors
d.IA_predicted_logodds %<>%
  ungroup() %>% 
  mutate(
    across(
      c(
        IA_predicted_logodds,
        IA_predicted_logodds.prior,
        IA_predicted_logodds.change,
        IA_predicted_logodds.changes_in_intercept_across_block_condition,
        IA_predicted_logodds.changes_in_slope_across_block_condition,
        Block,
        VOT), 
      ~ gs_scale(.x), 
      .names = "{.col}_gs"))

# Use new priors for the predicted logodds model since it is not a mixture but a plain logistic regression model
priors_logodds_model <-
  c(
    prior(student_t(3, 0, 2.5), class = "b"),
    prior(cauchy(0, 2.5), class = "sd"))

fit_test.IA_predicted_logodds <-
  brm(
    bf(
      Response.Voiceless ~ 0 + IA_predicted_logodds +
        (1 | ParticipantID) + (1 | Item.MinimalPair)),
    data = d.IA_predicted_logodds,
    prior = priors_logodds_model,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 4000,
    warmup = 2000,
    sample_prior = "yes",
    save_pars = save_pars(all = TRUE),
    family = bernoulli("logit"),
    control = list(adapt_delta = .999, max_treedepth = 15),
    file ="../models/test_IA_predicted_logodds.rds",
    file_refit = "on_change")

fit_test.IA_predicted_logodds <- add_criterion(fit_test.IA_predicted_logodds, criterion = c("loo"), moment_match = T)
# loo_compare(fit_test.IA_predicted_logodds, fit_test.upto.test4)

## with random slopes
fit_test.IA_predicted_logodds_with_RLs <-
  brm(
    bf(
      Response.Voiceless ~ 0 + IA_predicted_logodds +
        (1 + IA_predicted_logodds | ParticipantID) + (1 | Item.MinimalPair)),
    data = d.IA_predicted_logodds,
    prior = priors_logodds_model,
    cores = 4,
    chains = 4,
    init = 0,
    iter = 4000,
    warmup = 2000,
    sample_prior = "yes",
    save_pars = save_pars(all = TRUE),
    family = bernoulli("logit"),
    control = list(adapt_delta = .9995, max_treedepth = 15),
    file ="../models/test_IA_predicted_logodds_with_RLs.rds",
    file_refit = "on_change")

fit_test.IA_predicted_logodds_with_RLs <- add_criterion(fit_test.IA_predicted_logodds_with_RLs, criterion = c("loo"), moment_match = T)
# loo_compare(fit_test.IA_predicted_logodds_with_RLs, fit_test.upto.test4)
```

The predictions of the fitted ideal adaptor model are shown as shaded intervals in Figure \@ref(fig:plot-IA-human-PSE).  For the present data, the ideal adaptor model predicts the proportion of listeners' "t"-responses with a high $R^2 =$ `r round(cor(IA.prediction$Predicted_posterior, IA.prediction$Response) %>% . ^2, 3) * 100`$\%$, comparable to that of the psychometric model ($R^2 =$ `r round(cor(psych.model_prediction$predicted_proportion_voiceless, psych.model_prediction$proportion_voiceless) %>% . ^2, 3) * 100`$\%$). This demonstrates that the ideal adaptor can explain a non-trivial share of participants' changing behavior (for additional visualizations of the fit against participants' behavior, see SI, \@ref(sec:ideal-adaptor)). It does so substantially fewer degrees of freedom than the psychometric model. 

To further compare the ideal adaptor learning model to the psychometric data analysis model, we conducted two Bayesian model comparisons. We first compared the Bayesian psychometric mixed-effects model against a Bayesian mixed-effects logistic regression model that only used the maximum a posteriori (MAP) predictions of the ideal adaptor as a predictor (MAP-predicted log-odds of a "t"-response), along with by-participant and by-item random intercepts.^[We used mixed-effect logistics regression rather than a psychometric model with lapses since the ideal adaptor's predictions already include the effects of the ideal adaptor's lapse rate.] This model assumes that the ideal adaptor's predictions provide an equally good fit to all participants, but allows for participant- and item-specific response-biases. Comparison of the two models suggests that the psychometric model provides a better fit to participants' responses, capturing information beyond the predictions of the ideal adaptor ($\Delta_{elpd}=$ `r loo_compare(fit_test.IA_predicted_logodds, fit_test.upto.test4)[2, 1]`; SE$_\Delta=$ `r loo_compare(fit_test.IA_predicted_logodds, fit_test.upto.test4)[2, 2]`).^[The (log) predictive density can be thought of as the Bayesian equivalent of the (log) data likelihood in frequentist statistics (but the Bayesian approach incorporates prior beliefs). The *expected* log predictive density (ELPD) is the mean log posterior density of the data, estimated here using leave-one-out (with moment matching) through the `add_criterion()` function of `brms` [@R-brms_a]. This ELPD estimate provides a measure of the predictive accuracy of the model, essentially estimating how predictable a held-out data point is if the model is fit on all other data. Differences in the ELPD of more than 4-5 standard errors are often taken to be credible, whereas differences of 2-3 standard errors should be interpreted with more caution.] This result held, though somewhat less decisively, even when participants were allowed to vary in their sensitivity to the ideal adaptor's predictions (by including random by-participant slopes for the ideal adaptor's prediction, $\Delta_{elpd}=$ `r loo_compare(fit_test.IA_predicted_logodds_with_RLs, fit_test.upto.test4)[2, 1]`; SE$_\Delta=$ `r loo_compare(fit_test.IA_predicted_logodds_with_RLs, fit_test.upto.test4)[2, 2]`). 

In short, the ideal adaptor provides an impressively good quantitative model of participants' responses, with very few degrees of freedom. However, there is also some evidence that it does not capture all of the information captured by the psychometric analysis. *What* is this additional information that the psychometric model captures? As would be expected from a distributional learning model, the ideal adaptor displays predictions 1-4. Its PSE is initially fully determined by its prior beliefs (prediction 1), and then changes monotonically within increasing exposure (prediction 2a) in the direction expected from the exposure distribution (prediction 2b). These incremental changes initially proceed quickly, and then slow down (prediction 3); however, since convergence against the phonetic distributions of the unfamiliar talker has not yet occurred, the ideal adaptor is still clearly learning even after the last exposure block (see slope of shaded intervals in Figure \@ref(fig:plot-IA-human-PSE)). While predictions 1-3 were also met by participants, we saw above that prediction 4 does not seem to be met. Could this mismatch between the ideal adaptor and our participants be part of the information the ideal adaptor is lacking, compared to the psychometric model?

A look at the posterior distribution of the ideal adaptor's fitted parameters, summarized in Table \@ref(tab:ideal-adaptor-parameter-summary), provides some support for this idea. The inferred lapse rate estimates overlap with those of the psychometric model (`r print_CI(fit_test, "theta1_Intercept")`, see above). The large inferred values for $\kappa_{0}$ and $\nu_{0}$ are the result of two factors: (1) a good fit between the prior $\mu_{c}$ and $\Sigma_c$ we obtained from the @chodroff-wilson2018 database against listeners' pre-exposure responses, and (2) the fact that, after exposure to 72 /d/s and 72 /t/s, listeners' categorization functions have shifted by only 20-50% towards the shift expected from an idealized learner (recall Figure \@ref(fig:plot-fit-PSE)). Recall that the ideal adaptor---like any standard distributional learning model---assumes that listeners keep integrating information until they have converged against the distributions in the input from the unfamiliar talker. *Because of this assumption*, the best fit the ideal adaptor can achieve *has* to assume that adaptation has not yet completed, since listeners have not yet converged against the idealized learner state. 

```{r ideal-adaptor-parameter-summary, results='asis'}
summarize.NIW_ideal_adaptor_stanfit <- function(
    x,
    pars = c("kappa", "nu", "lapse_rate"),
    groups = c(
      "prior",
      "Cond Shift0AA_Up to test3", "Cond Shift0AA_Up to test5", "Cond Shift0AA_Up to test7",
      "Cond Shift10AA_Up to test3", "Cond Shift10AA_Up to test5", "Cond Shift10AA_Up to test7",
      "Cond Shift40AA_Up to test3", "Cond Shift40AA_Up to test5", "Cond Shift40AA_Up to test7")
) {
  add_ibbu_stanfit_draws(x, groups = groups, summarize = F) %>%
    unnest_cue_information_in_model() %>%
    gather_variables(exclude = c(".chain", ".iteration", ".draw", ".row", "cue", "cue2", "group", "category")) %>%
    ungroup() %>%
    # + Remove double mentions of variables that are constant across groups, categories, and cues
    #   (currently: lapse_rate)
    # + Remove double mentions of variables that are constant across cues
    #   (currently: kappa and nu)
    # + Remove double mentions of variables that are constant across cue2
    #   (currently: m)
    mutate(
      across(
        c(group, category),
        ~ ifelse(.variable == "lapse_rate", NA_character_, as.character(.x))),
      across(
        c(cue, cue2),
        ~ ifelse(.variable %in% c("kappa", "nu", "lapse_rate"), NA_character_, as.character(.x))),
      across(
        c(cue2),
        ~ ifelse(.variable == "m", NA, .x))) %>%
    distinct() %>%
    group_by(group, category, cue, cue2, .variable) %>%
    median_hdci() %>%
    mutate(
      group = factor(group, levels = groups),
      .variable = factor(.variable, levels = pars)) %>%
    # Format and sort output
    relocate(group, category, .variable, cue, cue2, .value, everything()) %>%
    arrange(match(.$.variable, levels(.$.variable)), match(.$group, levels(.$group)), category, cue, cue2) %>%
    mutate(
      .value = round(ifelse(.variable == "lapse_rate", .value * 100, .value), 1),
      .lower = round(ifelse(.variable == "lapse_rate", .lower * 100, .lower), 1),
      .upper = round(ifelse(.variable == "lapse_rate", .upper * 100, .upper), 1),
      !! sym(unique(paste0(unique(.$.width * 100), "\\%-HDCI"))) := paste0("[", .lower, ", ", .upper, "]")) %>%
    rename(
      Parameter = .variable,
      Estimate = .value) %>%
    select(-c(.width, .point, .interval, .lower, .upper)) %>% 
    # Simplifying for present paper
    ungroup() %>%
    filter(Parameter %in% pars & (group == "prior" | is.na(group))) %>%
    select(-c(group, category, cue, cue2)) %>%
    distinct() %>%
    mutate(
      Parameter = 
        case_when(
          Parameter == "kappa" ~ "strength of prior beliefs about $\\mu_c$ ($\\kappa_{0}$)",
          Parameter == "nu" ~ "strength of prior beliefs about $\\Sigma_c$ ($\\nu_{0}$)",
          Parameter == "lapse_rate" ~ "lapse rate ($\\lambda$)")) %>%
    kable(
      caption = "Summary of the ideal adaptor model's parameters fit to participants' behavior.",
      format = "latex",
      booktabs = TRUE,
      escape = FALSE) 
}

summarize.NIW_ideal_adaptor_stanfit(m_IA.VOT_f0_vowelduration_informedprior) 
```

```{r}
# model with interaction of block and change in IA-predicted logodds 
fit_test.IA_predicted_logodds.prior.change.interaction_with_RLs <-
  update(
    fit_test.IA_predicted_logodds_with_RLs,
    formula. =  Response.Voiceless ~ 1 + IA_predicted_logodds.prior_gs + (Block_gs * IA_predicted_logodds.change_gs) + 
      (1 + IA_predicted_logodds.prior_gs + (Block_gs * IA_predicted_logodds.change_gs) | ParticipantID) + (1 | Item.MinimalPair),
    newdata = d.IA_predicted_logodds,
    file ="../models/test_IA_predicted_logodds.prior.change.interaction.rds")

fit_test.IA_predicted_logodds.prior.change.interaction_with_RLs <- add_criterion(fit_test.IA_predicted_logodds.prior.change.interaction_with_RLs, criterion = c("loo"), moment_match = T)

# loo_compare(fit_test.IA_predicted_logodds.prior.change.interaction_with_RLs, fit_test.upto.test4)
```

Critically, this assumption comes at a cost, as evident in Figure \@ref(fig:plot-IA-human-PSE): the ideal adaptor *under*-predicts the rapid changes in listeners' PSEs  during initial exposure, while *over*-predicting the slower changes in listeners' PSE following exposure (see, in particular, the -20ms and -10ms conditions in Test 4). This pattern was confirmed by an additional mixed-effects logistic regression that allowed the predicted log-odds of the ideal adaptor to interact with a linear effect of test block (1-4), while keeping the by-participant and by-item random intercepts (for details, see SI, \@ref(sec:ideal-adaptor-fit)). This analysis found a credible negative interaction between the ideal adaptor's predictions and test block (<!-- TO DO: report beta etc here -->): with increasing exposure, changes in the ideal adaptor's categorization function become increasingly less predictive of participants' actual behavior.

Simply put, constrained by the assumptions of distributional learning, the ideal adaptor cannot fit all three of (i) listeners' pre-exposure behavior, (ii) the rapid changes in listeners' PSEs during initial exposure, and (iii) the premature flattening off of changes in listeners' PSEs. Rather, an ideal adaptor can *either* explain the fact that listeners' PSEs changed rapidly at the start of exposure *or* that there seem to be little to no changes in listeners' PSE after the second exposure block. But it cannot explain both. 

```{r}
rm(d.psychometric_intercept.slope.PSE.draws, fit_test.simple_effects_block)
```
