```{r, include=FALSE, message=FALSE}
if (!exists("PREAMBLE_LOADED")) source("preamble.R")
```

# Results

```{r model-fit-test-blocks, include=FALSE}
# Storing some information about the data set submitted for analysis
# (to facilitate unscaling predictions back into the original scales for plotting)
d.mean_sd_scaling <-
  d_for_analysis %>%
  group_by(Phase) %>%
  filter(Item.Labeled == FALSE) %>%
  summarise(across(Item.VOT, list(mean = mean, sd = sd)))

VOT.mean_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[1]
VOT.sd_exposure <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[1]
VOT.mean_test <- (d.mean_sd_scaling %>% pull(Item.VOT_mean))[2]
VOT.sd_test <- (d.mean_sd_scaling %>% pull(Item.VOT_sd))[2]

levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")
contrast_type <- "difference"

# Fit exposure and test psychometric mixed-effects models
fit_test <-
  fit_model(
    data = d_for_analysis,
    phase = "test",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .995)

fit_exposure <-
  fit_model(
    data = d_for_analysis,
    phase = "exposure",
    formulation = "standard",
    priorSD = 15, 
    adapt_delta = .999)
```

We analyzed participants' categorization responses during exposure and test blocks in two separate Bayesian mixed-effects psychometric models, using brms [@R-brms_a] in R [@R; @RStudio].^[Fitting the models separately avoids questions about how differences in the VOT distribution during exposure blocks might affect the analysis of test blocks. For the test analyses, it also removes any potential collinearity between effects of exposure and effects of VOT.] Psychometric models account for attentional lapses while estimating participants' categorization functions. Failing to account for attentional lapses---while commonplace in research on speech perception [but see @clayards2008; @kleinschmidt-jaeger2016]---can lead to biased estimates of categorization boundaries [@prins2011; @wichmann-hill2001]. For the present experiment, lapse rates were negligible (`r print_CI(fit_test, "theta1_Intercept")`), and all results replicate in simple mixed-effects logistic regressions [@jaeger2008]. This lapse rate compares favorably against those assumed or reported in prior work [e.g.,@kleinschmidt-jaeger2016; @kleinschmidt2020; @clayards2008].

The psychometric models for exposure and test blocks each regressed participants' categorization responses against the full factorial interaction of VOT, block, and exposure condition, along with the maximal random effect structure (by-subject intercepts and slopes for VOT, block, and their interaction, and by-item intercept and slopes for the full factorial design; see SI, \@ref(sec:analysis-approach)). All hypothesis tests reported below are based on these models. Figure \@ref(fig:plot-fit-slope-PSE) summarizes the results that we describe in more detail next. Panels A and B show participantsâ€™ categorization responses during exposure and test blocks, along with the categorization function estimated from those responses via the mixed-effects psychometric models. These panels facilitate comparison between exposure conditions within each block. Panels C and D show the slope and point of subject equality (PSE)---i.e., the point at which participants are equally likely to respond "d" and "t"---of the categorization function across blocks and conditions. These panels facilitate comparison across blocks within each exposure condition. Here we focus on the test blocks, which were identical within and across exposure conditions. Analyses of the exposure blocks are reported in the SI (\@ref(sec:exposure-analyses)), and replicate all effects found in the test blocks. 

We begin by presenting the overall effects, averaging across all test blocks. This part of our analysis resembles previous work, which analyzed the *average* effect of exposure across the entire experiment ['batch tests', e.g., @clayards2008; @kleinschmidt-jaeger2016; @nixon2016; @theodore-monto2019]. <!-- Unlike those previous studies, however, we compare responses over test trials that are physically identical across conditions and placed throughout the experiment. This reduces the risk that assumptions baked into the analysis approach (e.g., linearity along the acoustic-phonetic continuum) bias the results. --> Then we the address the questions about incremental adaptation that motivated our experiment---testing the predictions described in the introduction. 

```{r fit-nested-models-for-intercepts-slopes-plot}
# Refit the same model, formulated as nesting intercepts and slopes within each unique combination of
# exposure condition and block. This makes it easier to extract the intercept, slope, and PSE for the
# large result figure (Panels C and D).
fit_test_nested <-
  fit_model(
    data = d_for_analysis,
    phase = "test",
    formulation = "nested_slope",
    priorSD = 15, 
    adapt_delta = .995)

fit_exposure_nested <-
  fit_model(
    data = d_for_analysis,
    phase = "exposure",
    formulation = "nested_slope",
    priorSD = 15, 
    adapt_delta = .999)
```

```{r extract-intercepts-slopes-from-test-and-exposure-models}
d.estimates <-
  full_join(
    fit_test_nested %>% get_intercepts_and_slopes(),
    fit_exposure_nested %>% get_intercepts_and_slopes()) %>%
  mutate(
    Block = factor(Block),
    PSE = ifelse(
      Block %in% c(2, 4, 6),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_exposure),
      descale(-Intercept/slope, VOT.mean_test, VOT.sd_test))) %>% 
  group_by(Condition.Exposure, Block) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), median = median, upper = ~ quantile(.x, probs = .975)))) 
```

```{r}
# Make ideal observers that are fit on the exposure VOTs of the three exposure conditions.
# These ideal observers can be thought of as reflecting a learner that has fully learned 
# the exposure distributions. Note that we include perceptual noise in the ideal observers
# (estimated in Kronrod et al., 2016) to make their perception more human-like.
io <-    
  make_VOT_IOs_from_exposure(
    d_for_analysis %>% 
      filter(Phase == "exposure") %>% 
      group_by(ParticipantID, Condition.Exposure) %>% 
      nest(data = -c(ParticipantID, Condition.Exposure)) %>% 
      group_by(Condition.Exposure) %>% 
      # Subset data to a single participant per exposure condition. This is sufficient since 
      # the ideal observer's /d/ and /t/ categories only depend on the sufficient statistics 
      # (mean and SD) at the end of the experiment, which were identical across participants 
      # in each condition. 
      # 
      # Note that this approach trains the ideal observers on the *actual* VOT distribution 
      # that participants heard, not on the theoretical distributions these VOTs were sampled
      # from. This is in line with our goal to simulate behavior of an idealized participant
      # who has fully learned the exposure distributions.
      slice_sample(n = 1) %>% 
      unnest(data) %>% 
      mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/")) %>% 
      rename(VOT = Item.VOT, f0 = Item.f0_Mel)) %>%
  # Add prior based on Chodroff & Wilson (2018). Note that this database has substantially higher 
  # VOT variance than our exposure conditions, which we based on estimates from Kronrod et al. 
  # (2016).
  bind_rows(
    d.chodroff_wilson %>%
      make_MVG_ideal_observer_from_data(
        cues = c("VOT"),
        Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
      mutate(Condition.Exposure = "prior") %>%
      nest(io = -c(Condition.Exposure)))

# Get logistic parameter estimates for a simulated ideal observer listener. Do so for both 
# exposure and test. Even though this turned out to be overkill, we did it because it was 
# theoretically possible that the procedure we use to analyze *listeners'* responses (a 
# logistic model with a linear effect of VOT) introduces a bias into the estimation. Here, 
# we thus apply a variant of that same analysis approach to responses that are sampled from
# the three ideal observer fit on the exposure data (one each per exposure condition).
# 
# Unlike for the analysis of participants' responses, there is no need to use a psychometric
# model since ideal observers have not attentional lapses (i.e., lambda = 0). 
d.io.categorization <- 
  rbind(
    # Assess the ideal observers at the exact same locations used during test blocks
    get_logistic_parameters_from_model(
      model = io, 
      x = d_for_analysis %>%  
        filter(Phase == "test") %>%
        distinct(Condition.Exposure, Item.VOT) %>%
        rename(x = Item.VOT), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "test") %>% 
      crossing(Block = c(1, 3, 5, 7, 8, 9)), 
    # Assess the ideal observers on the VOT locations that occur in the three different 
    # exposure conditions. Only use unlabeled exposure trials, since those are the trials
    # we assess participants' behavior on.
    get_logistic_parameters_from_model(
      io, 
      d_for_analysis %>%  
        filter(Phase == "exposure", Item.Labeled == F) %>%
        group_by(Condition.Exposure) %>%
        # Since blocks differ in which VOTs are unlabeled, we include all blocks
        # (we are simulating the expected parameters across all exposure blocks)
        filter(ParticipantID == first(ParticipantID)) %>% 
        select(Item.VOT) %>% 
        rename(x = Item.VOT), 
      model_col = "io", 
      groups = "Condition.Exposure") %>% 
      mutate(Phase = "exposure") %>% 
      crossing(Block = c(2, 4, 6))) %>%
  select(Condition.Exposure, Phase, Block, intercept_scaled, slope_scaled, PSE) %>% 
  mutate(across(c(Phase, Condition.Exposure, Block), factor)) 
```

```{r prepare-plot-intercepts-slopes}
p.across_blocks <-
  d.estimates %>% 
  ggplot(
    aes(
      x = Block, y = Intercept_median,
      ymin = Intercept_lower, ymax = Intercept_upper,
      colour = Condition.Exposure, group = Condition.Exposure)) +
  geom_rect(
    data = tibble(
      xmin = c(seq(0.5, 6.5, 2), seq(7.55, 8.55, 1)), 
      xmax = c(seq(1.5, 7.5, 2), seq(8.5, 9.5, 1))),
    mapping = aes(xmin = xmin, xmax = xmax),
    ymin = -Inf, ymax = Inf, fill = "grey", alpha = .1, inherit.aes = F) +
  geom_point(position = position_dodge(.3), size = 1) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  stat_summary(geom = "line", position = position_dodge(.3)) +
  scale_x_discrete("Block", labels = c("1" = "Test 1", "2" = "Exposure 1", "3" = "Test 2", "4" = "Exposure 2", "5" = "Test 3", "6" = "Exposure 3", "7" = "Test 4", "8" = "Test 5", "9" = "Test 6")) +
  scale_colour_manual("Condition",
    labels = c("baseline", "+10ms", "+40ms", "prior"),
    values = c(colours.condition, "darkgray"),
    aesthetics = "color") +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 22.5, hjust = 1)) 

p.intercept_1to7 <-
  p.across_blocks +  
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = intercept_scaled, colour = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .7, 
    alpha = 0.3,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("Intercept")

p.slope_1to7 <-
  p.across_blocks +  
  aes(y = slope_median, ymin = slope_lower, ymax = slope_upper) +
  geom_step(
    data = d.io.categorization,
    aes(x = Block, y = slope_scaled, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, linewidth = .9, alpha = 0.5,
    direction = "mid",
    inherit.aes = F) +
  scale_y_continuous("slope (log-odds/ms VOT)")

d.true_shift <- 
  d.estimates %>% 
  # Join the PSEs from ideal observers
  left_join(
    d.io.categorization %>% 
      dplyr::select(Phase, Condition.Exposure, Block, PSE) %>% 
      rename(PSE.io_predicted = PSE), by = c("Condition.Exposure", "Block")) %>%
  group_by(Condition.Exposure) %>% 
  mutate(
    PSE_block1 = 
      case_when(
        Condition.Exposure == "Shift0" ~ d.estimates$PSE_median[1], 
        Condition.Exposure == "Shift10" ~ d.estimates$PSE_median[10],
        Condition.Exposure == "Shift40" ~ d.estimates$PSE_median[19]),
    true_shift = PSE.io_predicted - PSE_block1,
    proportion_shift = (PSE_median - PSE_block1)/true_shift)

# Compute distribution of talker-specific PSEs from Chodroff & Wilson (2018) corpus
d.talkerPSEs <- 
  get_IO_categorization(
    data = d.chodroff_wilson,
    cues = c("VOT"),
    groups = c("Talker", "gender"),
    with_noise = TRUE,
    VOTs = seq(0, 85, .5)) %>% 
  mutate(
    center_constant = mean(d.true_shift$PSE_block1) - mean(PSE),
    PSE_centered = PSE + center_constant) %>% 
  summarise(
    lower_PSE = quantile(PSE_centered, probs = .025), 
    mean_PSE = mean(PSE_centered),
    upper_PSE = quantile(PSE_centered, probs = .975)) 

p.PSE_1to7 <-     
  p.across_blocks +  
  scale_y_continuous("PSE (ms VOT)") +
  aes(y = PSE_median, ymin = PSE_lower, ymax = PSE_upper) +
  geom_linerange(linewidth = .6, position = position_dodge(.3), alpha = .5) +
  geom_label(
    data = d.true_shift %>% filter(Block != 1), 
    mapping = aes(label = paste(scales::percent(proportion_shift, accuracy = .1))),
    position = position_dodge(.3),
    label.size = .15,
    label.padding = unit(.18, "lines"),
    size = 2.2,
    show.legend = F) +
  geom_step(
    data = d.io.categorization,
    mapping = aes(x = Block, y = PSE, color = Condition.Exposure, group = Condition.Exposure),
    linetype = 2, 
    linewidth = .8, 
    alpha = 0.5, 
    direction = "mid",
    inherit.aes = F) +
  geom_text(
    data = d.true_shift %>% 
      filter(Phase == "exposure") %>% 
      group_by(Condition.Exposure, true_shift) %>% 
      summarise(),
    mapping = aes(
      x = 10.2, y = c(25, 35, 65),
      label = paste(round(true_shift), "(100%)"),
      colour = Condition.Exposure),
    colour = colours.condition,
    fontface = "bold",
    size = 3,
    inherit.aes = F) +
  annotate(
    "crossbar",
    x = 9.7, y = d.talkerPSEs$mean_PSE, ymin = d.talkerPSEs$lower_PSE, ymax = d.talkerPSEs$upper_PSE,
    colour = "black",
    width = 0.1,
    alpha = .6) +
  coord_cartesian(xlim = c(0.75, 9), clip = "off") +
  theme(plot.margin = unit(c(1, 3.2, 1, 1), "lines"))

p.estimates_1to7 <-  
  (p.slope_1to7 | p.PSE_1to7) +
  plot_layout(guides = "collect") &
  theme(legend.position = "none", axis.text = element_text(size = 8))
```

```{r plot-test-exposure-fit, fig.width=11, fig.height=3, message=FALSE}
cond_fit_test_exposure <-
  tibble(
    full_join(
      get_conditional_effects(fit_exposure, d_for_analysis, "exposure") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_exposure)),
      get_conditional_effects(fit_test, d_for_analysis, "test") %>%
        .[[1]] %>%
        mutate(Item.VOT = descale(VOT_gs, VOT.mean_test, VOT.sd_test)))) %>%
  arrange(as.numeric(as.character(Block))) %>%
  mutate(
    Phase = ifelse(Block %in% c(1, 3, 5, 7, 8, 9), "test", "exposure"),
    Block.plot_label = factor(case_when(
      Block == 1 ~ "Test 1",
      Block == 3 ~ "Test 2",
      Block == 5 ~ "Test 3",
      Block == 7 ~ "Test 4",
      Block == 8 ~ "Test 5",
      Block == 9 ~ "Test 6",
      Block == 2 ~ "Exposure 1",
      Block == 4 ~ "Exposure 2",
      Block == 6 ~ "Exposure 3")),
    Block.plot_label = fct_relevel(Block.plot_label, c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3",  "Test 4", "Test 5", "Test 6")))

label_colour <- tibble(
  Block.plot_label = c("Test 1", "Exposure 1", "Test 2", "Exposure 2", "Test 3", "Exposure 3", "Test 4"),
  Block.colour = c("grey", "white", "grey", "white", "grey", "white", "grey")) %>%
  mutate(Block.plot_label = factor(Block.plot_label, levels = Block.plot_label, ordered = T))

p.fit_1to7 <- cond_fit_test_exposure %>%
  filter(Block %in% c(1:7))  %>%
  ggplot() +
  geom_rect(
    data = label_colour,
    aes(xmin = -Inf, xmax = Inf,
        ymin = 1.05, ymax = 1.3,
        fill = Block.colour), show.legend = F) +
  scale_fill_manual(values = c("grey" = "grey", "white" = "white")) +
  new_scale_fill() +
  geom_linefit(data = d_for_analysis %>%
      group_by(Phase, Condition.Exposure, Block.plot_label) %>%
      filter(Block %in% c(1:7), Item.Labeled == FALSE),
      x = Item.VOT,
      y = estimate__,
      fill = NA,
      legend.position = "top",
      legend.justification = "right") +
  coord_cartesian(clip = "off", ylim = c(0, 1))

p.fit_7to9 <- cond_fit_test_exposure %>%
  filter(Block %in% c(7:9)) %>%
  ggplot() +
  geom_linefit(data = d_for_analysis %>%
      group_by(Condition.Exposure, Block) %>%
      filter(Block %in% c(7:9)),
      x = Item.VOT,
      y = estimate__,
      fill = "grey",
      legend.position = "none") +
  theme( axis.title.y = element_blank())
```

(ref:plot-fit-slope-PSE) Summary of results. **Panel A:** Changes in listeners psychometric categorization functions as a function of exposure, from Test 1 to Test 4 with all intervening exposure blocks (only unlabeled trials were included in the analysis of exposure blocks since labeled trials provide no information about listeners' categorization function). Point ranges indicate the mean proportion of participants' "t"-responses and their 95% bootstrapped CI. Lines and shaded intervals show the *maximum a posteriori* (MAP) estimates and 95% posterior CIs of a Bayesian mixed-effects psychometric model fit to participants' responses. **Panel B:** Same as Panel A but for the final three test blocks without intervening exposure. Test 4 is shown as part of both Panels A and B. **Panels C \& D:** Changes across blocks in the slope and boundary (point-of-subjective-equality, PSE) of the categorization functions shown in Panels A \& B. Point ranges represent the posterior medians and their 95% CI. Dashed reference lines show the intercepts and PSEs that naive learner would be expected to converge against after sufficient exposure (an ideal observer model that has fully learned the exposure distributions). Percentage labels indicate the amount of shift as a proportion of the expected shift under an ideal observer.

\begin{landscape}

```{r plot-fit-slope-PSE, fig.width=12.5, fig.height=8, fig.cap="(ref:plot-fit-slope-PSE)"}
p.fit_1to7 + p.fit_7to9 + p.estimates_1to7 +
  plot_layout(
    design = "
AAAAAAAAA
BBB######
DDDDDDDDD
",
heights = c(1, 1, 2)) +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") & 
  theme(plot.tag = element_text(face = "bold")) 
```

\end{landscape}

```{r}
rm(p.across_blocks, p.estimates_1to7, p.PSE_1to7, p.slope_1to7, p.fit_1to7, p.fit_7to9)
```

<!-- TO DO: If we need to cut, this section could probably go. It's only here to 'ease in' readers. -->
## Replication of previous findings (comparing exposure conditions averaging over test blocks)
Unsurprisingly, participants were more likely to respond "t" the longer the VOT ($`r get_bf(fit_test, "mu2_VOT_gs > 0")`$). Critically, exposure affected participants' categorization responses in the expected direction. Marginalizing over all test blocks, participants in the +40 condition were less likely to respond "t" than participants in the +10 condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift40vs.Shift10 < 0")`$) or the baseline condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift40vs.Shift10 + mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). There was also evidence---albeit less decisive---that participants in the +10 condition were less likely to respond "t" than participants in the baseline condition ($`r get_bf(fit_test, "mu2_Condition.Exposure_Shift10vs.Shift0 < 0")`$). That is, the +10 and +40 conditions resulted in categorization functions that were shifted rightwards compared to the baseline condition, as also evident in Figures \@ref(fig:plot-fit-slope-PSE).

This conceptually replicates previous findings that exposure to changed VOT distributions changes listeners' categorization responses [for /b/-/p/: @clayards2008; @kleinschmidt2015; @kleinschmidt2020; for /g/-/k/, @theodore-monto2019]. Next, we turn to the questions of primary interest. Incremental changes in participants' categorization responses can be assessed from three mutually complementing perspectives. First, we compare how exposure affects listeners' categorization responses *relative to other exposure conditions*. This tests how early in the experiment differences between exposure conditions begin to emerge. Second, we compare how exposure changes listeners' categorization responses from block to block within each condition, relative to listeners' responses prior to any exposure. Third, we compare changes in listeners' responses to those expected from an ideal observer that has fully learned the exposure distributions. This analysis can identify constraints on cumulative adaptation. For all three analyses, we initially focus on Tests 1-4 with intermittent exposure. 

Following that, we analyze the effects of testing and, in particular, repeated testing during Tests 4-6. Though research typically interprets tests as passive windows into the effects of exposure, test stimuli *also* constitute part of the exposure input listeners' receive. As we discuss below, this has both methodological and theoretical consequences.

```{r}
rm(fit_test, fit_test_nested, fit_exposure, fit_exposure_nested, cond_fit_test_exposure)
```

## How quickly does exposure affect listeners' categorization responses? (comparing exposure conditions within each block)
Figure \@ref(fig:plot-fit-slope-PSE)A suggests that differences between exposure conditions emerged early in the experiment: already in Test 2, listeners in the +10 condition have shifted their categorization functions rightwards relative to the baseline condition, and listeners in the +40 condition have shifted their in categorization functions even further rightwards. This is confirmed by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-condition). Prior to any exposure, during Test 1, participants' responses did not differ across exposure condition. This result is predicted by models of adaptive speech perception under the assumptions that (a) participants in the different groups have similar prior experiences, and that (b) our sample size of is sufficiently large to yield stable estimates of listeners' categorization function.

During Test 2, after exposure to only 24 /d/ and 24 /t/ stimuli (thereof half labeled), participants' categorization responses already differed between exposure conditions (BFs > 13.7). The differences between exposure conditions that emerged at this point were all in the direction predicted by models of adaptive speech perception. Additional analyses reported in the SI (\@ref(sec:exposure-analyses)) found that listeners' categorization functions had already changed *during* the first exposure block, in line with Figure \@ref(fig:plot-fit-slope-PSE)A. This suggests that changes in listeners' categorization responses emerged *quickly* at the earliest point tested---after only a fraction of exposure trials previously tested in similar paradigms. 

The effects of the three exposure conditions continued to persist until Test 4. Table \@ref(tab:hypothesis-table-simple-effects-condition) does, however, indicate an interesting non-monotonic development in the way that listeners' categorization function changed. While the difference between the +40 condition and both the baseline and +0 condition continued to increase numerically with increasing exposure (increasingly larger magnitude of negative estimates in Tests 2-4), the same was not the case for the difference between the +10 and the baseline condition. Instead, the difference between the +10 and baseline condition reduced with increasing exposure (while maintaining its direction). This development turns out to be potentially important in understanding incremental adaptation, and we continue to discuss it below.

```{r fit-simple-effects-condition, results='hide'}
# Get simple effects of Condition nested under block
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),  
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block1:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block3:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block5:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block7:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block8:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Block9:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_test.simple_effects_condition <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Block / (Condition.Exposure * VOT_gs) +
      (0 + Block / VOT_gs | ParticipantID) +
      (0 + Block / (Condition.Exposure * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d_for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-condition.rds")
```

```{r hypothesis-table-simple-effects-condition, results='asis'}
hyp.simple_effects_condition <- 
  hypothesis(
    fit_test.simple_effects_condition, 
    c(
      "mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 = 0",
      "mu2_Block1:Condition.Exposure_Shift40vs.Shift10 + mu2_Block1:Condition.Exposure_Shift10vs.Shift0 = 0",
      "mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block3:Condition.Exposure_Shift40vs.Shift10 + mu2_Block3:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block5:Condition.Exposure_Shift40vs.Shift10 + mu2_Block5:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block7:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block8:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift10vs.Shift0 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 < 0",
      "mu2_Block9:Condition.Exposure_Shift40vs.Shift10 + mu2_Block7:Condition.Exposure_Shift10vs.Shift0 < 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
  hyp.simple_effects_condition, 
  c("+10 vs. baseline = 0", "+40 vs. +10 = 0", "+40 vs. baseline = 0", rep(c("+10 vs. baseline", "+40 vs. +10", "+40 vs. baseline"), 5)), 
    caption = "When did exposure begin to affect participants' categorization responses? When, if ever, were these changes undone with repeated testing? This table summarizes the simple effects of the exposure conditions for each test block. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \"t\"-responses).") %>% 
  pack_rows("Test block 1 (pre-exposure)", 1, 3) %>%
  pack_rows("Test block 2", 4, 6) %>% 
  pack_rows("Test block 3", 7, 9) %>% 
  pack_rows("Test block 4", 10, 12) %>% 
  pack_rows("Test block 5 (repeated testing without additional exposure)", 13, 15) %>% 
  pack_rows("Test block 6 (repeated testing without additional exposure)", 16, 18)

rm(fit_test.simple_effects_condition)
```

## Incremental adaptation from prior expectations (comparing block-to-block changes within exposure conditions)
Next, we compare how exposure affected listeners' categorization responses from block to block *within* each exposure condition. To facilitate visual comparison, Figure \@ref(fig:plot-fit-slope-PSE)C & D summarize these changes for the slope and PSE, respectively. Focusing for now on Tests 1-4, this highlights four aspects of participants' behavior that were not readily apparent in the statistical comparisons we have summarized so far. 

First, Panel C highlights the relative lack of changes in the slope of listeners categorization function. Slope changes, or lack thereof, have received comparatively little attention in previous work [but see @clayards2008; @theodore-monto2019] but they form part of the empirical facts that theories of speech perception need to account for. Compared to the changes in PSEs in Panel D, changes in the slope of listeners' categorization functions in Panel C were similar across exposure conditions (BFs < XXX; SI, \@ref(sec:SI-slope-tests)). Slopes also changed little relative to listeners' categorization responses in Test 1 (BFs < XXX; see SI, \@ref(sec:SI-slope-tests)). Both of these findings are in line with distributional learning theories of adaptive speech perception [@kleinschmidt-jaeger2015], given that the variance of /d/ and /t/ was (a) held constant across all three exposure conditions, and (b) designed to resemble the variance of /d/ and /t/ in typical speech input.<!-- TO DO: THIS NEEDS TO BE TESTED PROBABLY. add those tables to SI and CHECK whether they agree with what we've written here. --> 

```{r fit-simple-effects-block, results='hide'}
my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift0:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift10:VOT_gs"),
  set_prior("student_t(3, 0, 15)", dpar = "mu2", coef = "Condition.ExposureShift40:VOT_gs"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor")
)

fit_test.simple_effects_block <- 
  brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 0 + Condition.Exposure/(Block * VOT_gs) + 
      (0 + Block * VOT_gs | ParticipantID) + 
      (0 + Condition.Exposure/(Block * VOT_gs) | Item.MinimalPair),
    theta1 ~ 1),
    data = d_for_analysis %>%
      filter(Phase == "test") %>%
      prepVars(levels.Condition = levels_Condition.Exposure, contrast_type = contrast_type),
    priors = my_priors,
    cores = 4,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    sample_prior = "yes",
    control = list(adapt_delta = .995),
    file ="../models/test-nested-block.rds")
```

```{r hypothesis-table-simple-effects-block, results='asis', fig.cap=""}
hyp.simple_effects_block <- 
  hypothesis(
    fit_test.simple_effects_block, 
    c(
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift0:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test2vs.Test1 + mu2_Condition.ExposureShift0:Block_Test3vs.Test2 + mu2_Condition.ExposureShift0:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift0:Block_Test5vs.Test4 + mu2_Condition.ExposureShift0:Block_Test6vs.Test5 < 0",
      
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 > 0",
      "mu2_Condition.ExposureShift10:Block_Test3vs.Test2 > 0",
      "mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",
      "mu2_Condition.ExposureShift10:Block_Test2vs.Test1 + mu2_Condition.ExposureShift10:Block_Test3vs.Test2 + mu2_Condition.ExposureShift10:Block_Test4vs.Test3 > 0",

      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 < 0",
      "mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      "mu2_Condition.ExposureShift10:Block_Test5vs.Test4 + mu2_Condition.ExposureShift10:Block_Test6vs.Test5 < 0",
      
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 < 0",
      "mu2_Condition.ExposureShift40:Block_Test3vs.Test2 < 0",
      "mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
      "mu2_Condition.ExposureShift40:Block_Test2vs.Test1 + mu2_Condition.ExposureShift40:Block_Test3vs.Test2 + mu2_Condition.ExposureShift40:Block_Test4vs.Test3 < 0",
    
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 > 0",
      "mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0",
      "mu2_Condition.ExposureShift40:Block_Test5vs.Test4 + mu2_Condition.ExposureShift40:Block_Test6vs.Test5 > 0"),
    robust = T) %>%
  .$hypothesis %>% 
  dplyr::select(-Star)

make_hyp_table(
    hyp.simple_effects_block,
    c(rep(c(
  # Comparing differences blocks within conditions
  "Block 1 to 2: decreased PSE",
  "Block 2 to 3: decreased PSE",
  "Block 3 to 4: decreased PSE",
  "{\\em Block 1 to 4: decreased PSE}",
  "Block 4 to 5: increased PSE",
  "Block 5 to 6: increased PSE",
  "{\\em Block 4 to 6: increased PSE}"), 2),
  "Block 1 to 2: increased PSE",
  "Block 2 to 3: increased PSE",
  "Block 3 to 4: increased PSE",
  "{\\em Block 1 to 4: increased PSE}",
  "Block 4 to 5: decreased PSE",
  "Block 5 to 6: decreased PSE",
  "{\\em Block 4 to 6: decreased PSE}"),
    caption = "Was there incremental change from test block 1 to 4? Did these changes dissipate with repeated testing from block 4 to 6? This table summarizes the simple effects of block for each exposure condition. Note that rightward shifts of the categorization function (and its PSE) correspond to negative estimates (lower intercepts in predicting the log-odds of \"t\"-responses).") %>% 
  pack_rows("Difference between blocks: baseline", 1, 7) %>%
  pack_rows("Difference between blocks: +10", 8, 14) %>%
  pack_rows("Difference between blocks: +40", 15, 21) 

rm(fit_test.simple_effects_block)
```

Second, while the PSEs for the +40 and +10 conditions were shifted rightwards compared to the baseline condition, both the +10 and the baseline condition seem to shift *left*wards relative to their pre-exposure starting point in Test 1. This is supported by Bayesian hypothesis tests summarized in Table \@ref(tab:hypothesis-table-simple-effects-block). The evidence for the leftward shifts is quite weak for the +10 condition (BF = 3.5 for changes from Test 1 to 4), for which the PSE changes comparatively little across tests, but it is stronger for the baseline condition (BF = 7.6). In contrast, the +40 condition is clearly shifted rightwards relative to pre-exposure (BF = 45.2). To understand this pattern, it is helpful to relate the three exposure conditions to the distribution of VOT in listeners' prior experience. Figure \@ref(fig:exposure-means-database-density) shows the category means of our exposure conditions relative to the distribution of VOT by talkers of L1-US English [based on @chodroff-wilson2018]. This comparison offers an explanation as to why the baseline condition (and to some extent the +10 condition) shift leftwards with increasing exposure, whereas the +40 condition shifts rightwards: relative to listeners' prior experience, only the +40 condition presented larger-than-expected category means, whereas the baseline condition and, to some extent, the +10 condition presented lower-than-expected category means. That is, once we take into account how our exposure conditions relate to listeners' prior experience, both the direction of changes from Test 1 to 4 *within* each exposure condition (Table \@ref(tab:hypothesis-table-simple-effects-block)), and the direction of differences *between* exposure conditions receive an explanation (Table \@ref(tab:hypothesis-table-simple-effects-condition)).

(ref:exposure-means-database-density) Placement of exposure stimuli relative to an estimate of typical phonetic distributions for `r nrow(d.chodroff_wilson)` word-initial /d/ and /t/ productions in L1-US English [based on `r length(unique(d.chodroff_wilson$Talker))` female talkers in @chodroff-wilson2018; for details, see SI \@ref(sec:SI-phonetic-data)]. The outermost contour of each category shows the 95% density quantile. Points show the category means of the exposure condition.

```{r centering}
# center exposure cues to speech corpus means
d_for_analysis %<>%
  ungroup() %>% 
  mutate(
    VOT.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(VOT = Item.VOT, Talker = "new"),
      cue = "VOT"),
    f0_Mel.CCuRE = remove_speechrate_effect_from_cue(
      data = d.chodroff_wilson, 
      newdata = d_for_analysis %>% 
        ungroup() %>% 
        mutate(f0_Mel = Item.f0_Mel, Talker = "new"),
      cue = "f0_Mel"),
    category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))) 
```

```{r getting-means-at-exposure-by-condition, eval=FALSE}
VOT.database_mean <- mean(d.chodroff_wilson$VOT.CCuRE)
f0_Mel.database_mean <- mean(d.chodroff_wilson$f0_Mel.CCuRE)

cond_means_exposure <- 
  d_for_analysis %>%
  filter(Phase == "exposure") %>%
  group_by(Condition.Exposure) %>%
  summarise(across(c(VOT.CCuRE, f0_Mel.CCuRE), mean, .names = "{.col}_mean")) %>% 
  mutate(
    VOT_diff_database = VOT.CCuRE_mean - VOT.database_mean,
    F0_diff_database = f0_Mel.CCuRE_mean - f0_Mel.database_mean)
```

```{r prepare-quantile-plot}
# specify quantiles for density plots
quantile_levels <- c(.05, .25, .5, .75, .95)
d_breaks <- 
  density_quantiles(
    x = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(VOT),
    y = d.chodroff_wilson %>% filter(category == "/d/") %>% pull(f0_Mel),
    quantiles = quantile_levels)
t_breaks <- 
  density_quantiles(
    x = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(VOT),
    y = d.chodroff_wilson %>% filter(category == "/t/") %>% pull(f0_Mel),
    quantiles = quantile_levels)

# matching breaks in each category to corresponding quantile
d_quantile <- quantile_levels
t_quantile <- quantile_levels
names(d_quantile) <- d_breaks
names(t_quantile) <- t_breaks
quantile_breaks <- tibble(quantile_levels, d_breaks, t_breaks)

p.density <-
  d.chodroff_wilson %>%
  ggplot(aes(x = VOT, y = f0_Mel, linetype = category, group = category)) +
  geom_density2d(
    data = . %>% filter(category == "/d/"),
    aes(colour = d_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = d_breaks) +
  geom_density2d(
    data = . %>% filter(category == "/t/"),
    aes(colour = t_quantile[as.character(after_stat(level))]),
    contour_var = "density",
    breaks = t_breaks) +
  scale_y_continuous("F0 (Mel)", limits = c(200, 365)) +
  scale_x_continuous("VOT (ms)", limits = c(-12, 125), breaks = scales::breaks_width(25)) +
  scale_colour_gradient("Quantiles",
                        high = "#e6e6e6",
                        low = "#000000",
                        guide = "colourbar",
                        breaks = quantile_levels,
                        labels = scales::percent(quantile_levels)) +
  theme(legend.position = "top",
        legend.text = element_text(size = 7)) +
  #plot centered means
  new_scale_color() +
  geom_point(
    data = d_for_analysis %>%
      filter(Phase == "exposure") %>%
      group_by(Condition.Exposure, category) %>%
      summarise(across(c(VOT.CCuRE, f0_Mel.CCuRE), mean)),
    mapping = aes(x = VOT.CCuRE, y = f0_Mel.CCuRE, colour = Condition.Exposure, shape = category),
    size = 2,
    alpha = 0.8,
  inherit.aes = F,
  show.legend = T) +
  scale_colour_manual("Condition",
             labels = c("baseline", "+10ms", "+40ms"),
             values = colours.condition) +
  guides(colour = "none",
         linetype = guide_legend(title = "Category"),
         shape = guide_legend(title = "Category"))
```

```{r exposure-means-database-density, fig.width=base.width*2.5+1, fig.height=base.height*2.5, warning=FALSE,fig.cap="(ref:exposure-means-database-density)"}
p.density +
  plot_layout(ncol = 1, guides = "collect") &
  theme(legend.position = "top")

rm(p.density)
```

Third, the estimates in Table \@ref(tab:hypothesis-table-simple-effects-block) suggest that listeners' PSEs changed most from Test 1 to Test 2, and then changed less and less with additional exposure up to Test 4 (smaller magnitude of estimates compared to earlier test blocks). This is particularly pronounced for the two conditions that shifted the most relative to pre-exposure, the baseline condition and the +40 condition. This pattern is predicted by models of adaptive speech perception that are sensitive to the prediction error experienced while processing speech. This includes models that assume error-based learning [@sohoglu-davis2016; see also discussion in @davis-sohoglu2020; @harmon2019] as well as Bayesian belief-updating models [@kleinschmidt-jaeger2015; for demonstration, see @jaeger2019].

Fourth, Panel D also begins to illuminate the reasons for the non-monotonic development of the +10 and baseline conditions relative to each other, discussed in the previous section. In particular, this non-monotonicity does *not* appear due to a reversal of the effects in either of the two exposure conditions. Rather, both exposure conditions continue to change listeners' categorization function in the same direction from Test 1 to Test 4. However, after the rapid change from the pre-exposure Test 1 to the first post-exposure Test 2, listeners' categorization responses in the baseline condition did not change as much as in the +10 condition. Additional Bayesian hypothesis tests reported in the SI (\@ref(sec:hypothesis-test-test-interactions)) suggest that these differences in the incremental effects of the two conditions are credible (BF = XXX). This explains the reduction in the difference between the +10 and baseline conditions discussed in the previous section. It does, however, raise the question *why* listeners' responses in the baseline condition did not change further with increasing exposure. Our third and final perspective on the incremental changes induced by exposure begins to address this question.

## Constraints on cumulative adaptation (comparing exposure effects against idealized learner models)
Figure \@ref(fig:plot-fit-slope-PSE)C-D also compare participants' responses against those of an idealized learner that has fully learned the exposure distributions. Specifically, we fit Bayesian ideal observers against the labeled VOT distributions of each exposure condition. Following @xie2023, we included perceptual noise in the ideal observer [estimated for VOT in @kronrod2016]. The dashed lines represent the slopes and PSEs, respectively, that are expected from these models (for details, see SI \@ref(sec:SI-bias-correction)).<!-- TO DO: that section should explain the general approach and also contain the bias corrected estimates. -->
This makes it possible to assess whether---or how much---listeners have converged against the exposure distributions. We make two observations. 

First, the slopes of listeners' categorization functions in Panel C approximate those predicted by the idealized learner models: many of the 95% CIs overlap with the dashed lines.^[Without the inclusion of perceptual noise, ideal observers predict much steeper categorization functions [see also @feldman2009; @kronrod2016]. This offers a potential explanation for the mismatch between the ideal observer predictions and human categorization responses when perceptual noise is not considered [@clayards2008].] 

Second, Panel D suggests that listeners did *not* converge against the exposure distributions. The percentage labels in Panel D quantify the degree to which listeners adapted their PSE towards the statistics of the exposure condition: 0% would correspond to no change relative to the listeners' PSE in Test 1, and 100% would correspond to complete convergence against the PSE predicted for an idealized learner. This highlights a striking asymmetry between the condition resulting in rightward shifts of the categorization function (+40), and the conditions resulting in leftward shifts (baseline and +10). On the one hand, the predicted PSEs of an idealized learner for the +40 and baseline conditions are shifted approximately by about the same amount relative to listeners' pre-exposure PSE in Test 1. However, the degree to which listeners converged against these predicted PSEs differed substantially between the two conditions, with cumulative adaptation proceeding almost twice as far in the rightward-shifted +40 condition (in Test 4: `r d.true_shift %>% filter(Condition.Exposure == "Shift40" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()` towards idealized PSE) compared to the leftward-shifted baseline condition (`r d.true_shift %>% filter(Condition.Exposure == "Shift0" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()`). Comparing within just the leftward-shifted conditions, we find that relative shift is smaller for the baseline condition, compared to the +10 condition (`r d.true_shift %>% filter(Condition.Exposure == "Shift10" & Block == 7) %>% summarise(proportion_shift = percent(proportion_shift)) %>% pull()`). 

<!-- TO DO: considering pointing out that these constraints are *not* predicted by IBBU models, though it's still not clear whether that's actually the case -->

## Effects of testing

```{r calculate-test-surprisal}
d.test_surprisal <-
  d_for_analysis %>% 
  filter(Phase == "test") %>% 
  distinct(Item.VOT) %>%
  cross_join(io) %>%
  # For each test stimulus, calculate the sum of densities of /d/ and /t/ over 
  # that VOT (i.e., the density of the marginal VOT distribution)
  mutate(
    density = map2_dbl(
      Item.VOT, 
      io, 
      function(x, y) 
        pmap(
          list(y$mu, y$Sigma, y$Sigma_noise),
          ~ dmvnorm(x, ..1, ..2 + ..3)) %>%
        reduce(`+`)),
    surprisal = -log(density))

# # Visualize marginal density
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = density, color = Condition.Exposure)) +
#       geom_line()
# 
# # Visualize surprisal
# d.test_surprisal %>%
#   ggplot(aes(x = Item.VOT, y = surprisal, color = Condition.Exposure)) +
#       geom_line()

d.test_surprisal %<>%
  group_by(Condition.Exposure) %>%
  summarise(surprisal = mean(surprisal))
```

Finally, we briefly summarize the effects of testing. Some models of adaptive perception predict that exposure to uniformly distributed test tokens will reduce the effect of preceding exposure [@kleinschmidt-jaeger2015; for relevant discussion, see also @lancia-winter2013]. In line with these theories, there is evidence that the effects of exposure reduced from Test 4 to Test 6 (see Tables \@ref(tab:hypothesis-table-simple-effects-condition) and \@ref(tab:hypothesis-table-simple-effects-block)). In Table \@ref(tab:hypothesis-table-simple-effects-block), this is evident in a reversal of the direction of the block-to-block changes for Tests 5-6, compared to Tests 1-4. For the +40 exposure condition, these block to block changes went from rightward shifts in Tests 1-4 to leftward shifts in Tests 5-6 (BF = 10.4). For the baseline condition, block to block changes went from leftward to rightward shifts (BF = 7.3). The only exposure condition for which no clear reversal was observed is the +10 condition (BF = 1.3). Two factors are likely to have contributed to this. First, this condition exhibited the smallest exposure effects, limiting the power to detect a reversal in that effect. Second, the +10 condition is also the condition, for which the marginal distribution of VOT during test blocks (mean = 35.8 ms, SD = 22.2 ms) most closely resembled the distribution during exposure (mean = 36.5, SD = 25.9), compared to the baseline (mean = 26.5 ms) or +40 condition (mean = 66.5 ms; SDs were identical across conditions).^[This does not entail that test trials were more expected in the +10 condition, so that listeners experienced smaller prediction errors. For example, for an ideal observer that has fully learned the exposure distribution (cf. dashed lines in Figure \@ref(fig:plot-fit-slope-PSE))C-D), test stimuli conveyed about the same amount of surprisal in the baseline and +10 conditions (mean surprisal = `r d.test_surprisal %>% filter(Condition.Exposure == "Shift10") %>% pull(surprisal) %>% round(., 1)` bits), compared to much larger surprisal in the + 40 condition (`r d.test_surprisal %>% filter(Condition.Exposure == "Shift40") %>% pull(surprisal) %>% round(., 1)` bits).]

As a consequence of repeated testing, exposure effects were substantially smaller in Test 6 than in Test 4 (see Table \@ref(tab:hypothesis-table-simple-effects-condition): while the effects of the +40 condition relative to the other two exposure conditions were still credible even in Test 6 (BFs > 24), this was no longer the case for the effect of the +10 condition relative to the baseline condition (BF = 1.6). This pattern of results replicates previous findings from LGPL [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @tzeng2021], and extends them to distributional learning paradigms [see also @kleinschmidt2020]. One important methodological consequence is that longer test phases do not necessarily increase the statistical power to detect effects of adaptation [unless analyses take the effects of repeated testing into account, as in the approach developed in @liu-jaeger2018]. Analyses that average across all test tokens---as remains the norm---are bound to systematically underestimate the adaptivity of human speech perception.<!-- ^[@kraljic-samuel2006 is sometimes cited as finding LGPL exposure effects even after 480 test trials over a uniform test continuum. This is, however, misleading. Kraljic and Samuel used four *different* uniform test continua over two different phonetic contrasts (/b/-/p/ and /d/-/t/). Each test session consisted of 10 randomized repetitions of 6 test trials. Kraljic and Samuel never tested (or made any claims about) whether exposure effects were still detectable during the 10th repetition. Rather they report *average* effects across the 10 repetitions (like other LGPL studies), which is perfectly compatible with the hypothesis that repeated testing reduces the effects of exposure [see @liu-jaeger2018].] -->

Indeed, there is some evidence that even 12 test trials---directly following 24 labeled and 24 unlabeled exposure inputs---were sufficient to affect listeners' perception. Consider the 'zigzag' pattern in Figure \@ref(fig:plot-fit-slope-PSE))C: the slopes of listeners' categorization functions were smaller during test than during exposure blocks (BF = XXX). <!-- TO DO: can we run a model that contains exposure and test block and runs: Response ~ Block.Type[effect coded] * VOT + Random effects and everything else identical to your main analysis? No exposure condition. The BF of the interaction term is the evidence we need. --> Analyses presented in the SI (\@ref(sec:XXX)) suggest that these differences are not due to biases in our analysis (such as the linearity assumption for VOT, or the effects of weakly regularizing prior given differences in the amount of data for exposure and test blocks). Whatever the reasons for these differences, they highlight that even short tests can affect listeners' responses.


