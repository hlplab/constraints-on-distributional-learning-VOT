---
title: "cut out stuff"
author: "T. Florian Jaeger"
date: "2023-05-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cuts from introduction to be re-intregated into discussion

## CAN THIS NOW BE CUT?
Previous work provides initial support for predictions (2a) and (2b). For example, recent LGPL/VGPL studies provide evidence in support of prediction (2a)---that the *amount* of phonetic evidence during exposure gradiently affects the magnitude of shifts in listeners' categorization boundary [@cummings-theodore2023; @liu-jaeger2018; @liu-jaeger2019; @vroomen2007]. In such paradigms, listeners are exposed to natural recordings of one phonetic category (e.g., /s/) and shifted instances of a second category that are manipulated to be perceptually more similar to the first category (e.g., /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/). Both the typical and the shifted sound instances are lexically or visually labeled by their context. For example, in an LGPL study, the lexical context will disambiguate the intended category of both the typical sounds (e.g., "dino*s*aur") and the shifted sounds (e.g., "medi*sh*ine"). Studies like this typically compare two groups of listeners that differ only in which of the two sounds was shifted. For example, one group of listeners might be exposed to 20 typical /s/ and 20 shifted /s/-like /`r linguisticsdown::cond_cmpl("ʃ")`/, mixed with 160 filler words that do not contain either sound. The other group of listeners might be exposed to 20 typical /`r linguisticsdown::cond_cmpl("ʃ")`/ and 20 shifted /`r linguisticsdown::cond_cmpl("ʃ")`/-like /s/, mixed with the same 160 filler words. Following exposure, the two groups of listeners will categorize sounds along an unlabeled test continuum (e.g., "asi" to "ashi") differently. Specifically, listeners in each group will categorize more sounds along the continuum as belonging to the category that was shifted during exposure [e.g., @norris2003; @eisner-mcqueen2005; @kraljic-samuel2005]. 

In a particularly informative study, Cummings and Theodore compared shifts in categorization function between groups of listeners after exposure to 1, 4, 10, or 20 lexically labeled shifted /s/ or /`r linguisticsdown::cond_cmpl("ʃ")`/ tokens (each matched by an equal number of unshifted tokens from the opposite category). Shifts in listeners' categorization functions increased with the number of exposure to tokens, in line with prediction (2a) of distributional learning models. @vroomen2007 found similarly increasing shifts in categorization functions *within* participants, comparing the effects of 1, 2, 4, ..., 32 exposures to visually labeled shifted tokens [see also @kleinschmidt-jaeger2012]. However, LGPL/VGPL paradigms---at least as used traditionally---limit experimenters' control over the phonetic properties of the exposure stimuli: consistent with the goals of those studies, shifted sound instances are selected to be perceptually ambiguous between two categories, rather than to exhibit specific phonetic distributions. This limits the extent to which such paradigms can inform predictions (1) and (2b) about the effects of phonetic distributions in prior and recent experience. To the extent that LGPL/VGPL research has assessed the effects of phonetic properties, this has thus largely been limited to qualitative post-hoc analyses [@drouin2016; @kraljic-samuel2007; @tzeng2021; for quantitative tests, see @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. 

Support for prediction (2b) has thus primarily come from DL studies. In an important early study, @clayards2008 exposed two different groups of US English listeners to instances of "b" and "p" that differed in their distribution along the voice onset time continuum (VOT). VOT is the primary phonetic cue to word-initial stops in US English: the voiced category (e.g. /b/) is produced with lower VOT than the voiceless category (e.g., /p/). Clayards and colleagues held the VOT means of /b/ and /p/ constant between the two exposure groups, but manipulated whether both /b/ and /p/ had wide or narrow variance along VOT. Exposure was unlabeled: on any trial, listeners saw pictures of, e.g., bees and peas on the screen while hearing a synthesized recording along the *bees*-*peas* continuum (obtained by manipulating VOT). Listeners' task was to click on the picture corresponding to the word they heard. If listeners adapt by learning how /b/ and /p/ are distributed along VOT, listeners in the wide variance group were predicted to exhibit a more shallow categorization function than the narrow variance group. This is precisely what Clayards and colleagues found, providing support for prediction (2b) that the distribution of phonetic cues in the exposure input causes changes in listeners' behavior in ways consistent with distributional learning theories [see also @nixon2016; @theodore-monto2019].

Other studies have exposed different groups of listeners to distributions that were shifted in one or the other direction along a phonetic continuum---very much like the three exposure conditions in Figure \@ref(fig:block-design-figure) [@chladkova2017; @colby2018; @idemaru-holt2011; @idemaru-holt2020; @kleinschmidt2020]. For example, @kleinschmidt-jaeger2016 used the stimuli developed by Clayards to expose five different groups of listeners to VOT distributions for /b/ and /p/ that were shifted to different degrees. The five different exposure conditions were each shifted by 10msecs in VOT relative to the other, but held constant the distance between the /b/ and /p/ mean <!-- (always 40ms) --> and the variance of /b/ and /p/. <!-- (both always 8.3ms$^2$).--> All groups of listeners were exposed to 222 trials of exposure input. Unlike the present study, Kleinschmidt and Jaeger did not include a pre-test or incremental intermittent testing. Instead, the effect of exposure was estimated by estimating listeners' categorization functions over the last third of the 222 trials. This revealed that listeners' categorization functions were shifted in the direction of the exposure input, and that the magnitude of this shift increased with the degree of exposure. The fact that all five exposure conditions elicited the predicted effect relative to each other constitutes particularly strong support consistent with prediction (2b) that the distribution of phonetic cues in the exposure input affects the magnitude of adaptive changes in listeners' categorization functions. 

Together with more recent findings from adaptation to natural accents [@hitczenko-feldman2016; @tan2021; @xie2021cognition], these findings suggests that the *outcome* of adaptation is qualitatively compatible with predictions (2a) and (2b) of distributional learning models [e.g., exemplar theory, @johnson1997; ideal adaptors, @kleinschmidt-jaeger2015].^[A related line of work has used distributional learning and training paradigms to study the acquisition of *novel* sound contrasts [e.g., @maye2002; @mcclelland1999; @pajak-levy2012; @pisoni1982]. These studies, too, have observed learning behavior qualitatively compatible with distributional learning models [for review, see @pajak2016].] Previous studies have, however, relied on tests that averaged over, and/or followed, hundreds of exposure trials. This leaves open *how adaptation incrementally unfolds* throughout the earliest moments of exposure---i.e., whether listeners' categorization behavior indeed changes in ways predicted by models of adaptive speech perception: developing from expectations based on previously experienced phonetic distributions (prediction 1) to increasing integration of the phonetic distributions observed during exposure to the unfamiliar talker (predictions 2a,b). Previous studies also focused on one prediction at a time, leaving open how the effect of prior expectations and the statistics of the unfamiliar input *jointly* explain adaptation, or even how the amount and distribution of exposure inputs *jointly* explain adaptation. 

Finally, we are not aware of any previous attempts to address predictions (3) and (4): without incremental testing, it is difficult to assess whether there are hard limits on adaptivity or simply 'how far the learner has gotten' with the exposure input they have received so far [for discussion, see @cummings-theodore2023; @kleinschmidt-jaeger2016; @kleinschmidt2020]. For the same reasons, it is difficult to assess whether the build-up of adaptation follows the predictions of error-based learning or ideal information integration (prediction 4). 

The incremental exposure-test paradigm in Figure \@ref(fig:block-design-figure) begins to address these knowledge gaps. <!-- The experiment starts with a test block that assesses listeners' state prior to informative exposure---often assumed, but not tested, to be identical across exposure conditions [see also @colby2018; @xie2021cognition]. Additional intermittent tests---opaque to participants---then assess incremental changes up to the first 144 informative exposure trials. This lets us assess how the joint effects of exposure amount and exposure distribution---corresponding to predictions (2a) and (2b)---unfold *incrementally*, and whether this unfolding follows the predictions of error-based learning and ideal information integration (prediction 4). By comparing the direction of adaptation not only across conditions, but also relative to the distribution of phonetic cues in listeners' prior experience, we begin to assess prediction (1). Finally, by comparing adaptation against the behavior expected under idealized learners, we address prediction (3). We test predictions (1)-(4) in a Bayesian mixed-effects psychometric model (a mixture model extension of generalized linear mixed-effect models). Such models are commonly used in research on psychophysics to correct for attentional lapses and responses biases [see @prins2019bayesian], but remain underutilized within research on speech perception. --> 
In addition to our primary goals, we took several modest steps towards addressing concerns about ecological validity that might limit the generalizability of DL results. This includes concerns about the ecological validity of both the stimuli and their distribution in the experiment [see discussion in @baese-berk2018]. For example, previous distributional learning studies have often used highly unnatural, 'robotic'-sounding, speech. Beyond raising questions about what types of expectations listeners apply to such speech, these stimuli also failed to exhibit naturally occurring covariation between phonetic cues that listeners are known to expect [see, e.g., @idemaru-holt2011; @schertz2016]. Similarly, LGPL/VGPL studies have often used perceptually ambiguous stimuli obtained by 'acoustic blending'---mixing recordings of two words (e.g., "sin" and "shin") at different relative intensity. This, too, can create acoustic properties that are rarely, if ever, observed in human speech (Rachel Theodore, p.c.). We instead developed stimuli that both sound natural and exhibit the type of phonetic covariation that listeners expect from everyday speech perception. We return to these and additional steps we took to increase the ecological validity of the phonetic *distributions* under Methods.

## Other cuts

```{r}
colours.voicing <- c("#7EC8E3", "#990033")
colours.sex <- c("#c1502e", "#2F9FC2")


    Condition.Exposure = case_when(
    Condition.Exposure == "Shift0" ~ "baseline",
    Condition.Exposure == "Shift10" ~ "+10ms",
    Condition.Exposure == "Shift40" ~ "+40ms"), 
    Condition.Exposure = fct_relevel(
          Condition.Exposure, c("baseline", "+10ms", "+40ms")),


############################################################################
# function to plot talker productions in experiment 1 (section 2.3)
############################################################################

plot_talker_UVGs <- function (data_production, data_perception, noise = FALSE) {
  plot <- data_production %>%
    mutate(x = list(VOT = seq(-100, 130, .5)),
           x = map(x, ~ as_tibble(.x) %>% rename("VOT (ms)" = value))) %>%
    unnest(io) %>%
    mutate(
      gaussian = pmap(
        list(x, gender, category, mu, Sigma, Sigma_noise),
        ~ geom_function(
          data = ..1,
          aes(x = `VOT (ms)`,
              linetype = ..3, colour = ..2),
          fun = function(x) dnorm(x, mean = ..4[[1]][[1]], sd = if (noise == T) sqrt(..5[[1]][[1]]) + sqrt(..6[[1]][[1]]) else sqrt(..5[[1]][[1]])), alpha = .2)))

  plot %>%
    ggplot() +
    plot$gaussian +
    scale_colour_manual("Talker sex", values = colours.sex, labels = c("Female", "Male")) +
    scale_linetype_discrete("Category") +
    scale_y_continuous("Density") +
    geom_rug(
      data = data_perception %>%
        ungroup() %>%
        distinct(Item.VOT),
      mapping = aes(x = Item.VOT),
      colour = "black",
      alpha = .6,
      inherit.aes = F) +
    guides(colour = "none")
}



plot_talker_MVGs <- function(
  data_production,
  prod_means = c(chodroff.mean_VOT, chodroff.mean_f0_Mel),
  cues,
  data_perception = d.test.excluded,
  percept_means = c(VOT.mean_exp1, f0.mean_exp1),
  centered = F
) {
  plot <- data_production %>%
    unnest(io) %>%
    select(-c(x, PSE, categorization, line)) %>%
    mutate(ellipse_points = pmap(
      list(mu, Sigma, Sigma_noise),
      ~ get_bivariate_normal_ellipse(..1, Sigma = ..2 + ..3))) %>%
    group_by(Talker) %>%
    mutate(ellipse = pmap(
      list(gender, category, ellipse_points),
      ~ geom_path(data = ..3, mapping = aes(x = ..3[[1]], y = ..3[[2]], colour = ..1, linetype = ..2), alpha = .1)))

  plot %>%
    ggplot() +
    plot$ellipse +
    scale_x_continuous("VOT (ms)", breaks = seq(-100, 150, 50)) +
    scale_y_continuous("F0 (Mel)") +
    scale_colour_manual("Talker sex", values = colours.sex, labels = c("Female", "Male")) +
    scale_linetype_discrete("Category") +
    geom_point(
      data = if (centered == T) data_perception %>%
        ungroup() %>%
        distinct(Item.VOT, Item.F0_Mel) %>%
        mutate(Item.VOT = Item.VOT + (prod_means[1] - percept_means[1]),
               Item.F0_Mel = Item.F0_Mel + (prod_means[2] - percept_means[2])) else
                 data_perception %>%
        ungroup() %>%
        distinct(Item.VOT, Item.F0_Mel),
      aes(x = Item.VOT, y = Item.F0_Mel),
      shape = 4,
      size = .8,
      alpha = .2,
      inherit.aes = F) +
    geom_abline(intercept = if (centered == T) normMel(245.46968) + (prod_means[2] - percept_means[2]) else normMel(245.46968),
               slope = 0.03827,
               linetype = 2,
               alpha = .3) +
    guides(colour = "none", category = "none")
}
```

# NOTES for structure of intro
+ strong test of predictions about effects of exposure
  + linking back to exposure distributions in the input (unlike all other work except for KJ16)
  + control over phonetic distributions (unlike LGPL, VGPL, AA)  
  + can fit categorization function (unlike VGPL; DSL); phonetic distribution during test and the way we analyze our data
    + (identical test stimuli across and within subject: theory-free testing)
    + (Bayesian mixed-effects psychometric model to avoid bias in estimation of cat fun)    

+ strong test of incrementality (also highlighted in xie2023)
  + pre-exposure test (unlike most other work)
  + test short / early moments of exposure (unlike DL)
  + test incremental accumulation (unlike DL, LGPL, AA) --> amount of evidence

+ move towards increased ecolological validity
  + ecological validity of stimuli (unlike some of DL, VGPL)
  + ecological validity of distributions (unlike DL; unlike LGPL/VGPL not always maximally ambiguous tokens)

## old intro draft

The predominant paradigms in research on adaptive speech perception are, however, not well-suited to address this question. As @cummings-theodore2023 summarize, "most research [...] has focused on identifying the conditions that are necessary for adaptation to occur" and "consistent with [this goal], outcomes [...] are most often considered as a binary result---does any learning occur, or not?" As a consequence, much remains unknown about how exposure comes to affect perception. It is unclear, for example, whether adaptive changes accumulate depending on both the amount of speech input and its statistical properties in the way predicted by the most explicit theoretical frameworks [e.g., the ideal adaptor, @kleinschmidt-jaeger2015; C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]. 

Typical paradigms manipulate exposure between listeners, and then assess the effects of exposure on subsequent test stimuli that are identical for all groups [for review, see @baese-berk2018; @schertz-clare2020]. These types of paradigms have provided evidence that adaptation to an unfamiliar talker can be rapid. For example, a thought-provoking finding by @clarke2004 suggests that exposure to eighteen sentences from an L2-accented talker---less than two minutes of speech---can be sufficient to facilitate significantly faster processing of that speech. This finding has since been replicated and extended to show that equally short exposure can facilitate recognition that is both faster and more accurate [@xie2018; for related results, see also @bradlow2023; @xie2017; @xie2021jep]. Other work has traded the ecological validity of natural L2 accents against increased control over the phonetic properties of exposure and test stimuli---a critical step towards stronger tests, as competing hypotheses about the mechanisms underlying adaptive speech perception require strong linking hypotheses mapping the acoustic input onto listeners' responses [@martin2023; @xie2023]. One such paradigm is lexically- or visually-guided perceptual recalibration [@bertelson2003; @norris2003; @kraljic-samuel2005], in which listeners are exposed to phonetically manipulated instances of a sound (e.g., making the "s" in "embassy" sound almost like an "sh"), mixed with many filler words without that sound. Following such exposure, listeners are known to shift their categorization function, so as to categorize more tokens along the "s"-"sh" continuum as "s". Recent work within those paradigms has found that as little as four phonetically shifted instances of a sound category can be sufficient to significantly alter listeners' categorization boundary [@liu-jaeger2018; @liu-jaeger2019; @cummings-theodore2023; @vroomen2007]. The same studies have found that exposure seems to accumulate, leading to larger boundary shifts for listeners who were exposed to more instances of the shifted sound [up to a point, @liu-jaeger2018; @vroomen2007; see also @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012]. Findings like these suggest that even rapid adaptation can be cumulative, rather than being an all or nothing process.

There are, however, important limitations to what perceptual recalibration paradigms can tell us about incremental adaptation. As is typical for such paradigms, all of the above experiments exposed listeners to shifted pronunciations that were always lexically or visually labeled stimuli (e.g., embedding the "sh"-like "s" in the word "embassy", which effectively labels it as an "s"). Such labeling is known to facilitate adaptation [@burchill2018; @burchill2023]---indeed, if shifted pronunciations are embedded in minimal pair or nonce-word context, listeners do no longer shift their categorization boundary [@norris2003; @REF-theodore?]. In everyday speech perception, however, listeners often have uncertainty about the word they are hearing, and must either use contextual information to label the input or adapt from unlabeled input. Perceptual recalibration paradigms, at least as used traditionally, also limit experimenters' control over the phonetic properties of the exposure stimuli: shifted sound instances are selected to sound ambiguous between, e.g., "s" and "sh", not based on their phonetic properties. To the extent that researchers have aimed to understand the consequences of those phonetic properties on the degree of boundary shift following exposure, this has involved post-hoc analyses [@drouin2016; @tzeng2021?]. It is thus an open question to what extent the boundary shifts observed in such experiments reflect not only the quantity, but also the distribution of phonetic properties, during exposure [as would be expected under, e.g., the ideal adaptor framework].

The present work thus employs a novel repeated-exposure-test paradigm that explicitly control the distribution of phonetic properties during exposure.  [clayards, bejjanki; kj16, k20; see also theodore-monto2019]




## Maryann's most recent intro
Recent reviews have identified distributional learning of marginal cue statistics ['normalization', @apfelbaum-mcmurray2015; @mcmurray-jongman2011; @magnuson-nusbaum2007] or the statistics of cue-to-category mappings as an important mechanism affording this adaptivity ['representational learning', @clayards2008; @idemaru-holt2011; @kleinschmidt-jaeger2015; @davis-sohoglu2020; for review, @schertz-clare2020; @xie2023]. This hypothesis has gained considerable influence over the past decade, with findings that changes in listener perception are qualitatively predicted by the statistics of exposure stimuli [@bejjanki2011; @clayards2008; @idemaru-holt2020; @kleinschmidt2012; @munson2011; @nixon2016; @theodore-monto2019; @tan2021; for important caveats, see @harmon2019].

Viewing speech perception as an adaptive process has been pivotal in our understanding of how human listeners overcome the lack of invariance problem; a problem fully appreciated when one begins to map out the variability of acoustic-phonetic cues that point to a single linguistic category [e.g. @delattre1955; @peterson-barney1952; @newman2001]; compounded when talker sex, age, social class, dialect and a host of other contexts are factored into consideration. Listeners' aptitude at speech comprehension however, belie this challenge. Given the uncertainty involved it is not surprising models of spoken word recognition that allow for probabilistic outcomes have left a lasting impression [@mcllelland-elman1986; @vitevitch-luce; @norris-mcqueen2008].

Over the past 20 years there have been prolific investigations into how and when listeners adjust their phonological categories after hearing acoustically manipulated speech sounds. These manipulations take place at the margins of linguistic categories where perception can be heavily influenced by the contexts in which they are presented [@norris2003; @mcqueen2006]. A sound that is ambiguous between /s/ and /sh/ presented in the utterance *contradiction* would bias its interpretation as /sh/ since *contradicson* is not a word. Repeated exposure to the sound in such biasing word contexts reliably elicits a shift in perception along the /s/-/sh/ continuum in subsequent testing -- those having heard the sound in /sh/-biasing words tend to give more /sh/ responses; vice-versa for those who were exposed to it in /s/-contexts. This perceptual recalibration of less prototypical category members has also been induced under audio-visual manipulations [@bertelson2003; @vroomen2007]. The paradigm has been exploited to its fullest to investigate, among other things, the sustainability of perceptual changes [@eisner-mcqueen2006; @kraljic-samuel2005], its generalizability to members of the same phonological class [@kraljic-samuel2006], and its generalizability to other talkers [@kraljic-samuel2007; @reinisch-holt2014].

In general, these findings are compatible with exemplar and other probabilistic updating frameworks that link the distributions of cues to changes in category mappings hence perceptual recalibration findings can to an extent inform general understanding of talker adaptation. But the mechanisms that underlie the perceptual changes observed are still not well understood and therefore remain a point of debate. Some positions remain less specified than others. For instance the proposal that listeners expand their categories when confronted with unfamiliar accents or that they "relax their criteria" for category membership (@zheng-samuel2020; @schmale2012; @floccia2006; @bent2016). While it is possible that apparent perceptual shifts post-exposure can be explained by processes independent of distributional learning [@clarke-davidson2008; see @xie2023 for simulations] what is needed are better specified hypotheses coupled with stronger predictions and tests to weigh the evidence [@schertz-clare2020; @xie2023; @bent-baese-berk2021].

Analytic frameworks that facilitate modelling of perceptual processes conditioned on different assumptions offer a way forward. If robust speech recognition involves learning from the input under varying contexts in a rational manner, it has to account for the implicit assumptions that listeners seem to bring to any speech perception task  with regard to cue-category mappings, and be able to explain how they reconcile these assumptions with recent input. Theories that explicitly bring this to bear include the influential exemplar models [@johnson1996; @pierrehumbert2001; @apfelbaum-mcmurray2015], Bayesian inference models [@feldman2009; @kronrod2016; @kleinschmidt-jaeger2015; @hitczenko-feldman2016], and error-driven learning [@harmon2019].

In a recent example @cummings-theodore2023 working within the ideal adaptor framework, predicted that perceptual recalibration could have graded effects. This logic follows from the general premise that adaptation is the outcome of weighted updates of listener prior expectations of cue-category mappings with the statistics of talker input. By manipulating the number of times an ambiguous sound between /s/ and /sh/ was heard between participants and within each biasing context (1, 4, 10 or 20 occurences) they showed that the size of the putative perceptual recalibration effect correlated with the frequency of the ambiguous tokens. Model simulations qualitatively predicted behavioral results and provided strong evidence of a mechanism that is sensitive to cue statistics. This result corroborates earlier modelling efforts of @kleinschmidt-jaeger2011 which demonstrated that incremental bayesian belief-updating is a possible mechanism behind what has been believed to be dichotomous perceptual phenomena -- selective adaptation and perceptual recalibration.

The present study was devised in similar spirit to past studies guided by an understanding of language as inference and learning under uncertain conditions [@fine2010; @kleinschmidt-jaeger2016; @kleinschmidt-jaeger2011; @clayards2008]. In particular we aim to subject the hypothesis that talker adaptation results from distributional learning with incremental belief updating to a stronger test. While studies of perceptual recalibration that demonstrate graded learning effects based on the quantity of evidence support this hypothesis, there are limitations to the paradigm that preclude deeper investigation. Talker-specific learning involves inferring the means and variances of her cue-category mappings. This task is made more difficult for talkers with extreme cue shifts that fall beyond the prior expectations of listeners because an entire remapping of the cue space is required [@sumner2011]. In perceptual recalibration listeners are presented with maximally informative instances of the same ambiguous acoustic-phonetic token essentially providing ideal but very unnatural circumstances for learning to occur. However even this has a limit -- exposure to a certain number of critical trials (about 20 trials in lexical context studies [@cummings-theodore2022; @tzeng2021]; 64 trials in audio-visual context studies[@vroomen2007]) -- do not bring additive learning effects.

Here we build on the pioneering work of @clayards2008; @kleinschmidt-jaeger2016; @theodore-monto2019; @kleinschmidt2020 with some design innovations that we believe affords a productive test of the core claims of an ideal adaptor account of speech perception. In @kleinschmidt-jaeger2016 L1-US English listeners heard recordings of /b/-/p/ minimal pair words like *beach* and *peach* that were acoustically manipulated. Separate groups of listeners were exposed to different distributions of voice onset times (VOTs)---the primary cue distinguishing word-initial voicing ---that were shifted by up to +30 ms, relative to what one might expect from a 'typical' talker (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)A). In line with the distributional learning hypothesis, listeners' category boundary or point of subjective equality (PSE)---i.e., the VOT for which listeners are equally likely to respond "b" or "p"---shifted in the same direction as the exposure distribution (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)B). @kleinschmidt-jaeger2016 and closely related work have been able to show perceptual shifts move qualitatively in the direction of the manipulated distributions but so far none of them were designed to test incremental adaptation. We propose to fill that gap with a novel test-exposure-test design. In doing so we aim to estimate listeners prior expectations about the category mappings for our test talker before they receive further informative exposure and to document how quickly, from the onset of exposure, does the distributional learning effect emerge. The latter point is something that remains opaque in previous work because of the lack of test blocks. Given the substantial evidence that adaptation is rapid (e.g. under 5 mins in L2 accent adaptation; 4-10 trials in perceptual recalibration) listeners may show learning effects very early on in distributional learning as well. On the other hand, given the comparatively more naturalistic task of inferring talker distributions over a range of cues, learning effects may take longer to show.   

In experimental work researchers often have to consider the generalizability of their results which leads to questions about ecological validity. There is a trade-off between ecological validity of the experimental design and the desired degree of control over the variables. Questions about ecological validity of prior work in distributional learning pertain to two features. First, the stimuli which were generated with a synthesiser, had an obvious machine-like quality[@kleinschmidt-jaeger2016; @clayards2008]. Second, the pairs of distributions of voiced and voiceless categories were always identical in their variances [see also @theodore-monto2019] which adds to the artificiality of the experiment. In our description of methods below we show how we can begin to improve on these features through the stimuli and the setting of exposure conditions.


## Previous intro



<!--
However, Kleinschmidt and Jaeger also observed a previously undocumented property of these adaptive changes: shifts in the exposure distribution had less than proportional (sublinear) effect on shifts in PSE (Figure \@ref(fig:kleinschmidt-jaeger-2016-replotted)C). While this finding is broadly compatible with the hypothesis of distributional learning, it points to important not well-understood constraints on adaptive speech perception.-->


(ref:kleinschmidt-jaeger-2016-replotted) Design and results of @kleinschmidt-jaeger2016 replotted. **Panel A:** Different groups of participants were exposed to different shifts in the mean VOT of /b/ and /p/. **Panel B:** categorization functions fitted to the last 1/6th of all trials depending on the exposure condition (shift in VOT means of /b/ and /p/). For reference, the black dashed line shows the categorization function of the 0-shift condition. The colored dashed lines shows the categorization function expected for an ideal observer that has fully learned the exposure distributions. **Panel C:** Mean and 95% CI of participants' points of subjective equality (PSEs), relative to the PSE of the ideal observers.

```{r kleinschmidt-jaeger-2016-refitted}
# load K&J2016 data and filter to all semi-supervised rows
d.KJ16 <-
  supunsup_clean %>%
  filter(supCond == "mixed" | supCond == "supervised") %>%
  # rename variables so that they match the naming conventions employed in the remainder
  # of this paper.
  rename(
    ParticipantID = subject,
    Item.VOT = vot,
    Condition.Exposure = bvotCond,
    Response.Voiceless = respP,
    Item.MinimalPair = wordClass,
    category = respCategory) %>%
  mutate(
    Condition.Exposure = factor(case_when(
      Condition.Exposure == 0 ~ "+0ms",
      Condition.Exposure == 10 ~ "+10ms",
      Condition.Exposure == 20 ~ "+20ms",
      Condition.Exposure == 30 ~ "+30ms")),  
    Condition.Exposure = fct_relevel(
      Condition.Exposure, c("+0ms", "+10ms", "+20ms", "+30ms")))

d.KJ16_unlabeled <-
  d.KJ16 %>%
  arrange(trial) %>%
  filter(labeled == "unlabeled") %>%
  group_by(ParticipantID) %>%
  # filter to final 6th of total trials
  slice_tail(n = 37)

VOT.mean_d.KJ16_unlabeled <- mean(d.KJ16_unlabeled$Item.VOT)
VOT.sd_d.KJ16_unlabeled <- sd(d.KJ16_unlabeled$Item.VOT)
d.KJ16_unlabeled %<>% mutate(VOT_gs = (Item.VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled))

contrasts(d.KJ16_unlabeled$Condition.Exposure) <-
  cbind("10vs0" = c(-3/4, 1/4, 1/4, 1/4),
        "20vs10" = c(-1/2, -1/2, 1/2, 1/2),
        "30vs20" = c(-1/4, -1/4, -1/4, 3/4))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(cauchy(0, 2.5), class = "sd", dpar = "mu2"),
  prior(lkj(1), class = "cor"))

fit_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~ 1 + VOT_gs * Condition.Exposure +
        (1 + VOT_gs | ParticipantID) +
        (1 + VOT_gs * Condition.Exposure | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .99),
    file = "../models/KJ16-semisupervised-GLMM.rds")

# fit nested model to extract slopes and intercepts
fit_nested_KJ16 <-
  brm(
    bf(
      Response.Voiceless ~ 1,
      mu1 ~ 0 + offset(0),
      mu2 ~  0 + (Condition.Exposure) / VOT_gs +
        (0 + VOT_gs | ParticipantID) +
        (0 + (Condition.Exposure) / VOT_gs | Item.MinimalPair),
      theta1 ~ 1),
    data = d.KJ16_unlabeled,
    cores = chains,
    chains = chains,
    init = 0,
    iter = 4000,
    warmup = 2000,
    family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
    priors = my_priors,
    control = list(adapt_delta = .995),
    file = "../models/KJ16-semisupervised-nested-GLMM.rds")

cond_fit_KJ16 <- fit_KJ16 %>%
  conditional_effects(
  effects = "VOT_gs:Condition.Exposure",
  method = "posterior_epred",
  ndraws = 500,
  re_formula = NA) %>%
  .[[1]] %>%
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled))

d.KJ16_PSE <-
  fit_nested_KJ16 %>%
  gather_draws(`b_mu2_Condition.Exposure.*`, regex = TRUE, ndraws = 8000) %>%
  mutate(
    term = ifelse(str_detect(.variable, "VOT_gs"), "slope", "Intercept"),
    .variable = gsub("b_mu2_Condition.ExposureP(\\d{1,2}ms).*$", "\\1", .variable)) %>%
  pivot_wider(names_from = term, values_from = ".value") %>%
  rename(Condition.Exposure = .variable) %>%
  relocate(c(Condition.Exposure, Intercept, slope, .chain, .iteration, .draw)) %>%
  mutate(
    PSE = descale(-Intercept/slope, VOT.mean_d.KJ16_unlabeled, VOT.sd_d.KJ16_unlabeled),
    Condition.Exposure = paste0("+", Condition.Exposure)) %>%
  group_by(Condition.Exposure) %>%
  summarise(
    across(
      c(Intercept, slope, PSE),
      list(lower = ~ quantile(.x, probs = .025), mean = mean, upper = ~ quantile(.x, probs = .975))))
```


```{r io-categorization-kleinschmidt-jaeger-2016}
# Test points by condition
x <-
  d.KJ16_unlabeled %>%
  group_by(Condition.Exposure) %>%
  distinct(Item.VOT) %>%
  rename(x = Item.VOT)

# get io categorizations of the test points by condition
io.d.KJ16 <-
  make_MVG_ideal_observer_from_data(
  d.KJ16 %>%
    rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
  nest(io = -c(Condition.Exposure)) %>%
  left_join(x) %>%
  mutate(x = map(x, ~ c(.x))) %>%
  nest(x = x) %>%
  mutate(categorization = map2(
    x, io,
    ~ get_categorization_from_MVG_ideal_observer(x = .x$x, model = .y, decision_rule = "proportional") %>%
    mutate(VOT = map(x, ~ .x[1]) %>% unlist()))) %>%
  unnest(cols = categorization, names_repair = "unique") %>%
  pivot_wider(names_from = category, values_from = response, names_prefix = "response_") %>%
    mutate(n_b = round(`response_b` * 10^12), n_p = 10^12 - n_b) %>%
  group_by(Condition.Exposure) %>%
  nest(data = -c(Condition.Exposure)) %>%
  mutate(
    model_unscaled = map(
      data, ~ glm(
      cbind(n_p, n_b) ~ 1 + VOT,
      family = binomial,
      data = .x)),
    intercept_unscaled = map_dbl(model_unscaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_unscaled = map_dbl(
      model_unscaled, ~ tidy(.x)[2, 2] %>% pull()),
    model_scaled = map(data, ~ glm(
      cbind(n_p, n_b) ~ 1 + I((VOT - VOT.mean_d.KJ16_unlabeled) / (2 * VOT.sd_d.KJ16_unlabeled)),
      family = binomial,
      data = .x)),
    intercept_scaled = map_dbl(model_scaled, ~ tidy(.x)[1, 2] %>% pull()),
    slope_scaled = map_dbl(model_scaled, ~ tidy(.x)[2, 2] %>% pull()),
    PSE = -intercept_unscaled/slope_unscaled)

# get io with lapse-accounted categorization
io.d.KJ16.lapse_rate <-
  make_MVG_ideal_observer_from_data(
    data = d.KJ16 %>%
      rename(VOT = Item.VOT) %>%
      group_by(ParticipantID, Condition.Exposure) %>%
      nest(data = -c(ParticipantID, Condition.Exposure)) %>%
      group_by(Condition.Exposure) %>%
      slice_sample(n = 1) %>%  
      unnest(data),
    group = "Condition.Exposure",
    cues = c("VOT"),
    Sigma_noise = matrix(80, dimnames = list("VOT", "VOT")),
    lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = -c(Condition.Exposure)) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist()))) %>%
  unnest(categorization, names_repair = "unique")

# get io of typical talker and its categorization (+0 condition)
d.typical_talker <-
  io.d.KJ16[[2]][[1]][1, 1] %>%
  unnest(io) %>%
  mutate(lapse_rate = plogis(fixef(fit_KJ16)[[2]])) %>%
  nest(io = everything()) %>%
  crossing(x = seq(-10, 70, .5)) %>%
  nest(x = x) %>%
  mutate(
    categorization =
    map2(x, io, ~
           get_categorization_from_MVG_ideal_observer(
             x = .x$x, model = .y, decision_rule = "proportional") %>%
           filter(category == "p") %>%
           mutate(VOT = map(x, ~.x[1]) %>% unlist())),
    line = map(
      categorization,
      ~ geom_line(
          data = .x,
          mapping = aes(x = VOT, y = response),
          linetype = 2,
          linewidth = 0.6,
          alpha = .8,
          colour = "black")))
```




```{r kleinschmidt-jaeger-2016-replotted, fig.height=base.height*3.5, fig.width=base.width*5.5, fig.cap="(ref:kleinschmidt-jaeger-2016-replotted)"}
# make histograms of exposure distributions
p.KJ16.histogram <-
  d.KJ16 %>%
  group_by(Condition.Exposure) %>%
  slice_head(n = 222) %>%
  ggplot(aes(x = Item.VOT,
             fill = paste(Condition.Exposure, trueCat))) +
  geom_histogram(binwidth = 5) +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous(breaks = c(0, 25, 50)) +
  scale_fill_manual(
    "Category",
    values = c("+0ms b" = "#f8766d",
               "+0ms p" = "#fdd1ce",
               "+10ms b" = "#7cae00",
               "+10ms p" = "#d4ff66",
               "+20ms b" = "#00bfc4",
               "+20ms p" = "#99fcff",
               "+30ms b" = "#c77cff",
               "+30ms p" = "#e9ccff"),
    aesthetics = "fill",
    labels = c("/b/", "/p/", "", "", "", "", "", "")) +
   guides(
    fill = guide_legend(
      override.aes = list(
        fill = c("#383838", "#C0C0C0", NA, NA, NA, NA, NA, NA),
        values = c("b", "p", NA, NA, NA, NA, NA, NA)), nrow = 1)) +
  facet_wrap(~ Condition.Exposure, nrow = 1) +
  theme(legend.justification = "left") +
  remove_x_guides

p.KJ16.fit <-
  cond_fit_KJ16 %>%
  rename(Condition = Condition.Exposure) %>%
  ggplot() +
  geom_ribbon(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    ymin = lower__, ymax = upper__, fill = Condition),
    alpha = .1,
    show.legend = F) +
  geom_line(aes(
    x = Item.VOT,
    y = estimate__,
    group = Condition,
    color = Condition),
    linewidth = .7,
    alpha = 0.6,
    show.legend = F) +
  stat_summary(
    data = d.KJ16_unlabeled %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    fun.data = mean_cl_boot,
    mapping = aes(
      x = Item.VOT,
      y = Response.Voiceless,
      group = Condition,
      colour = Condition),
    geom = "pointrange",
    size = 0.15,
    show.legend = F) +
  geom_line(
    data = io.d.KJ16.lapse_rate %>%
      rename(Condition = Condition.Exposure) %>%
      group_by(Condition),
    mapping = aes(x = VOT, y = response, group = Condition, colour = Condition),
    linetype = 2,
    linewidth = 1,
    alpha = .6,
    inherit.aes = F,
    show.legend = F) +
  d.typical_talker$line +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 50)) +
  scale_y_continuous("Proportion \"p\"-responses", breaks = c(0, .5, 1)) +
  facet_wrap(~ Condition, nrow = 1)

p.KJ16.PSE <-
  d.KJ16_PSE %>%
  left_join(io.d.KJ16) %>%
  rename(PSE.io = PSE) %>%
  ggplot(aes(y = PSE_mean, x = Condition.Exposure, colour = Condition.Exposure)) +
  geom_hline(
    yintercept = c(20, 30, 40, 50),
    linewidth = 1.5,
    alpha = .4,
    linetype = 2,
    colour = scales::hue_pal()(4)) +
  geom_linerange(
    aes(ymin = PSE_lower, ymax = PSE_upper), size = 1, alpha = .8, show.legend = F) +
  geom_label(size = 4, show.legend = F, aes(label = paste(round((PSE_mean - 20) / (c(20, 30, 40, 50) - 20) * 100, 1), "%"))) +
  scale_x_discrete("Condition") +
  scale_y_continuous("PSE") +
  theme(axis.text.x = element_text(angle = 22.5, hjust = .8))

layout <- "
AAAA#
BBBBC"

p.KJ16.histogram + p.KJ16.fit + p.KJ16.PSE +
  plot_layout(design = layout,
              guides = "collect") +
  plot_annotation(tag_levels = 'A', tag_suffix = ")") &
  theme(legend.position = "top",
        plot.tag = element_text(face = "bold"))
```




```{r remove-unused-objects-section1}
rm(d.KJ16, d.KJ16_unlabeled, d.KJ16_PSE, cond_fit_KJ16, fit_KJ16, fit_nested_KJ16, p.KJ16.PSE)
```

For example, influential models of adaptive speech perception predict proportional, rather than sublinear, shifts (for proof, see SI \@ref(sec:ibbu-proof)). This is the case both for incremental Bayesian belief-updating model [@kleinschmidt-jaeger2011] and general purpose normalization accounts [@mcmurray-jongman2011]---models that have been found to explain listeners' behavior well in experiments with less substantial changes in exposure. There are, however, proposals that can accommodate this finding. Some proposals distinguish between two types of mechanisms that might underlie representational changes, <!--- link representational change and distributional learning --> *model learning* and *model selection* [@xie2018, p. 229]. The former refers to the learning of a new category representations---for example, learning a new generative model for the talker [@kleinschmidt-jaeger2015, Part II] or storage of new talker-specific exemplars [@johnson1997; @sumner2011]. Xie and colleagues hypothesized that this process might be much slower than is often assumed in the literature, potentially requiring multiple days of exposure and memory consolidation during sleep [see also @fenn2013; @tamminen2012; @xie2018sleep]. Rapid adaptation that occurs within minutes of exposure might instead be achieved by selecting between *existing* talker-specific representations that were learned from previous speech input---e.g., previously learned talker-specific generative models [see mixture model in @kleinschmidt-jaeger2015, p. 180-181] or previously stored exemplars from other talkers [@johnson1997]. Model learning and model selection both offer explanations for the sublinear effects observed in @kleinschmidt-jaeger2016. But they suggest different predictions for the evolution of this effect over the course of exposure.

Under the hypothesis of model learning, sublinear shifts in PSEs can be explained by assuming a hierarchical prior over talker-specific generative models [$p(\Theta)$ in @kleinschmidt-jaeger2015, p. 180]. This prior would 'shrink' adaptation towards listeners' priors---similar to the effect of random by-subject or by-item effects in generalized linear mixed-effect models, which shrink group-level effect estimates towards the population mean of the data [@baayen2008]. Critically, as long as these priors attribute non-zero probability to even extreme shifts (e.g., the type of Gaussian prior used in mixed-effects models), this predicts listeners' PSEs will continue to change with increasing exposure until they have converged against the PSE that is ideal for the exposure statistics. In contrast, the hypothesis of model selection predicts that rapid adaptation is more strictly constrained by previous experience: listeners can only adapt their categorization functions up to a point that corresponds to (a mixture of) previously learned talker-specific generative models. This would imply that at least the earliest moments of adaptation are subject to a hard limit (Figure \@ref(fig:prediction)): exposure helps listeners to adapt their interpretation to more closely aligned with the statistics of the input, but only to a certain point.

(ref:prediction) Contrasting predictions of model learning and model selection hypotheses about the incremental effects of exposure on listeners' categorization function. Both hypothesis predict incremental adaptation towards the statistics of the input, as well as constraints on this adaptation. The two hypotheses differ, however, in that model selection predicts a hard limit on how far listeners' can adapt during initial encounters with an unfamiliar talker.

```{r prediction, fig.height=base.height+1/4, fig.width=base.width*2, fig.cap="(ref:prediction)"}
k <- 10^-1

crossing(
  Exposure = 0:100,
  Shift = c(20, 40),
  Hypothesis = c("model learning", "model selection")) %>%
  mutate(
    PSE = 25 + ifelse(Hypothesis == "model learning", Shift, pmin(Shift, 25)) *
      (1 - exp(-k * Exposure))) %>%
  ggplot(aes(x = Exposure, y = PSE, color = factor(Shift))) +
  geom_hline(
    data = crossing(Shift = c(0, 20, 40), Hypothesis = c("model learning", "model selection")),
    aes(yintercept = Shift + 25, color = factor(Shift)), linetype = 2) +
  geom_line(alpha = .5) +
  scale_y_continuous("PSE (in ms VOT)") +
  scale_color_manual(breaks = c("0", "20", "40"), values = c("gray", "green", "blue")) +
  facet_wrap(~ Hypothesis) +
  guides(color = "none") +
  theme(panel.grid = element_blank())
```

The present study employs a novel incremental exposure-test paradigm to address two questions. We test whether the sublinear effects of exposure observed in recent work replicate for exposure that (somewhat) more closely resembles the type of speech input listeners receive on a daily basis. And, we evaluate the predictions of the model learning and selection hypotheses against human perception. We take this question to be of interest beyond the specific hypotheses we contrast: whether there are hard limits to the benefits of exposure to unfamiliar speech patterns ultimately has consequences for education and medical treatment.

# Other intros

END OF INTRO: 
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu/FILLIN-FULL-URL). 
+ ALL OTHER OSF STATEMENTS.


Prior to creating the three exposure conditions for the experiment, we conducted a norming experiment (N = XXX participants) to assess US-L1 listeners' perception of our stimuli and to determine a baseline categorisation boundary for this talker. While it is normal and acceptable practice to set the baseline by taking population estimates of mean values from past studies on stops, we reasoned that such estimates were highly variable and therefore aimed to obtained a more accurate estimation of how L1-US English listeners perceived the speech of our talker. To anticipate the outcome, we eventually discovered that the classification boundary from norming underestimated the boundary fitted to our participants' classification in the initial test block. This placed our baseline and baseline +10ms shift exposure conditions slightly leftwards of participants' initial perceptual boundary. This finding, however does not impinge on the conclusions drawn from this study [<!--CALL IT baseline, +10 AND +40-->]

The other purpose of the norming experiment was to detect possible anomalous features present in our stimuli (for e.g. if it would elicit unusual categorisation behaviour or whether certain minimal-pairs had an exaggerated effect on categorisation). For the norming experiment the VOT continua employed 24 VOT steps ranging from -100ms VOT to +130ms (-100, -50, -10, 5  `r paste0(seq(15, 90, 5), collapse = ", ")`, `r paste0(seq(100, 130, 10), collapse = ", ")`). VOT tokens in the lower and upper ends were distributed over larger increments because stimuli in those ranges were expected to elicit floor and ceiling effects, respectively. We found VOT to have the expected effect on the proportion of "t"-responses, i.e. higher VOTs elicited greater "t"-responses and that the word-pairs did not differ substantially from each other. The results and analysis of the norming experiment are reported in full in section \@ref(sec:XX).  




To construct the baseline exposure distribution we first computed the point of subjective equality (PSE) from the perceptual component of the fitted psychometric function of listener responses in the norming experiment. The PSE corresponds to the VOT duration that was perceived as most ambiguous across all participants during norming (i.e. the stimulus that on average, elicited equal chance of being categorised as /d/ or /t/) thus marking the categorical boundary. From a distributional perspective the PSE is where the likelihoods of both categories intersect and have equal probability density (we assumed Gaussian distributions and equal prior probability for each category). To limit the infinite combinations of category likelihoods that could intersect at this value, we set the variances of the /d/ (80ms) and /t/ (270ms <!--/t/ variance lowered from original 398ms estimate because of dip-tip pair variance limitations--> categories based on parameter estimates (@Kurumada_Xie_Jaeger_2022) obtained from the production database of word-initial stops in @chodroff2017structure. To each variance value we added 80ms following (@kronrod2016unified) to account for variability due to perceptual noise since these likelihoods were estimated from perceptual data. We took an additional degree of freedom of setting the *distance between the means* of the categories at 46ms; this too was based on the mean  for /d/ and /t/ estimated from the production database. The means of both categories were then obtained through a grid-search process to find the likelihood distributions that crossed at 25ms VOT (see XX of SI for further detail on this procedure).


The distributional make up was determined through a process of sampling tokens from a discretised normal distribution with values rounded to the nearest multiple of 5 integer (available through the `extraDistr` package in R). 


In addition, participants' categorization during the early phase of the experiment were scrutinised for their slope orientation and their proportion of "t"-responses at the least ambiguous locations of the VOT continuum. The early phase of the experiment was defined as the first 36 trials and the least ambiguous locations were defined as -20ms below the empirical mean of the /d/ category and +20ms above the empirical mean of the /t/ category. These means were obtained from the production data estimates by @Kurumada_Xie_Jaeger_2022.


# Results leftovers



----
Additional hypothesis tests in Table \@ref(tab:hypothesis-table-interactions) show that the change from Test 1 to 2 was largest (BF = 57.82), followed by the change from Test 2 to 3 (BF = 10), with only minimal changes from Test 3 to 4 (BF = 1.68). Qualitatively paralleling the changes across blocks for the +40 condition, the change in the difference between the +10 and baseline conditions was largest from Test 1 to 2 (BF = 5.42), and then somewhat decreased from Test 2 to Test 4 (BFs < 1). 
-----



# Predictions of existing models of adaptive speech perception {#sec:proof}
We show that two of the most influential approaches to adaptive speech perception do not predict the sublinear (less than proportional) shifts in categorisation functions observed both in the present study and in in previous work [@kleinschmidt-jaeger2016; @kleinschmidt2020]. We focus on these two models since they are the only widely-used accounts of adaptive speech perception that are (1) not limited to specific types of contrasts (unlike, e.g., vowel normalization accounts) and (2) sufficiently specific to make concrete predictions [for review of existing models, see @xie2023]. We only briefly discuss why a third type of account---exemplar models of speech perception---*likely* would also fail to predict the sublinear effects.

We emphasize that our discussion deliberately focuses on *models*, not theories. The finding of sublinear effects of exposure can be accommodated in all three theories that underlie the three models discussed here (Bayesian inference in the ideal adaptor framework, similarity-based inference in exemplar theory, or normalization relative to expectations). Yet, the fact that none of the existing *models* predicts this effect highlights that this property of adaptive speech perception was only recently discovered, and is as-of-yet not well understood.

## Incremental Bayesian belief-updating [@kleinschmidt-jaeger2011, @kleinschmidt-jaeger2012, @kleinschmidt-jaeger2015, @kleinschmidt-jaeger2016; @kleinschmidt2020]
The only distributional learning model that has been more repeatedly tested against adaptive speech perception---incremental Bayesian belief-updating [@kleinschmidt-jaeger2011]---predicts proportional, rather than sublinear, shifts. This model had previously been found to closely predict the cumulative effects of exposure in perceptual recalibration to audio-visually [@kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012] or lexically labeled speech [@cummings-theodore2023], as well as the type of exposure to unlabelled minimal pair words employed by Kleinschmidt and Jaeger [@theodore-monto2019]. However, all of these studies employed comparatively small changes in cue distributions, and lacked the design necessary to detect deviation from proportionality (we return to this point below). The findings presented in @kleinschmidt-jaeger2016 would seem to reject this specific distributional learning model [though not necessarily the theory it is derived from, @kleinschmidt-jaeger2015; for discussion of the relation between theory and model, see also @kleinschmidt2020]  <!-- TO DO: make sure we do -->

```{r, eval=FALSE}
# TO DO: This needs to be adjusted (if we keep it) so that the function doesn't just assume that the only cue is VOT 
# (currently parts of the function make that assumption but other parts don't)

get_IBBU_updates <- function(
    prior = NULL,
    exposure = NULL,
    test = NULL,
    condition = NULL,
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72,
    updates_to_keep = c(0, 48, 96, 144),
    d.chodroff_wilson = d.chodroff_wilson.isolated,
    seed = 1234
) {
  set.seed(seed)
  Sigma_noise <- diag(c(80, 878)[1:length(cues)], nrow = length(cues))
  dimnames(Sigma_noise) <- list(cues, cues)

  if (is.null(prior)) {
    prior <- d.chodroff_wilson %>%
      make_NIW_ideal_adaptor_from_data(
        cues = cues, kappa = kappa, nu = nu, Sigma_noise = Sigma_noise)
  }
  message("NIW made")
  
  if (is.null(exposure)) {
    message("When exposure is NULL, condition is interpreted as the VOT shift.")
    exposure <-
      lapply(
        X = as.list(1:3),
        FUN = function(x) d.chodroff_wilson %>%
          # Sample exposure from VOT distribution regardless of which cues are used
          # for belief-updating, since that's what we did in the experiment.
          make_MVG_ideal_observer_from_data(
            cues = "VOT", Sigma_noise = matrix(80, dimnames = list("VOT", "VOT"))) %>%
          sample_MVG_data_from_model(Ns = 24, randomize.order = T)) %>%
      reduce(rbind) %>%
      mutate(
        Trial = 1:144,
        Condition.Exposure = paste0("True", .env$condition),
        VOT = VOT + as.numeric(gsub("Shift", "", .env$condition)),
        f0 = predict_f0(VOT, Mel = F),
        f0_Mel = predict_f0(VOT, Mel = T)) %>%
      relocate(Condition.Exposure, Trial, category, VOT, f0)
  }

  idealized_ios <-
    make_VOT_IOs_from_exposure(exposure) %>%
    cross_join(
      exposure %>%  
        transmute(x = pmap(list(VOT), ~ c(...))) %>%
        nest(x = x)) %>%
    get_logistic_parameters_from_model(
      model = ., 
      model_col = "io",
      groups = "Condition.Exposure")

  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.order = "Trial",
    noise_treatment = "no_noise",
    lapse_treatment = "no_lapses",
    method = "label-certain") %>%
    filter(observation.n %in% updates_to_keep) %>%
    nest(ia = -observation.n) %>%
    mutate(
      Condition.Exposure = unique(exposure$Condition.Exposure),
      observation.bin = as.numeric(factor(observation.n, levels = sort(observation.n))),
      observation.alpha = observation.bin / max(observation.bin),
      cf = map(ia,
               ~ get_categorization_function_from_NIW_ideal_adaptor(.x, noise_treatment = "no_noise")),
      sf = map2(cf, observation.alpha,
                ~ stat_function(
                  fun = function(x) {
                    as.numeric(.x(
                      map(x, function(y) c(y, predict_f0(y, Mel = T))),
                      target_category = 2)) },
                  alpha = .y,
                  color = if (condition %in% c("Shift0", "Shift10", "Shift40")) colours.condition[condition] else "black"))) %>%
    select(-c(observation.bin, observation.alpha)) %>%
    relocate(Condition.Exposure, observation.n, ia, cf, sf) %>%
    mutate(x = list(.env$test)) %>%
    # Convert updated beliefs into estimated PSE and slope
    get_logistic_parameters_from_model(
      model = ., 
      model_col = "ia",
      groups = c("Condition.Exposure", "observation.n", "cf", "sf")) %>%
    # Join in information about idealized prior and posterior
    # (the latter based on ideal observer that has fully learned the exposure distribution).
    # Then calculate for each update step the proportion of updating relative to those
    # idealized priors and posteriors.
    left_join(
      idealized_ios %>%
        select(Condition.Exposure, slope_scaled, PSE) %>%
        rename(idealized_posterior_slope_scaled = slope_scaled, idealized_posterior_PSE = PSE)) %>%
    cross_join(
      idealized_ios %>%
        ungroup() %>%
        filter(Condition.Exposure == "prior") %>%
        select(slope_scaled, PSE) %>%
        rename(idealized_prior_slope_scaled = slope_scaled, idealized_prior_PSE = PSE)) %>%
    mutate(
      proportion_of_idealized_PSE = (PSE - idealized_prior_PSE) / (idealized_posterior_PSE - idealized_prior_PSE),
      proportion_of_idealized_slope_scaled = (slope_scaled - idealized_prior_slope_scaled) / (idealized_posterior_slope_scaled - idealized_prior_slope_scaled))
}

get_IBBU_updates_for_experiment <- function(
    data = d.for_analysis,
    condition,
    test = NULL,
    cues = c("VOT", "f0_Mel"),
    kappa = 72, nu = 72
) {
  exposure <-
    data %>%
    ungroup() %>%
    filter(
      Condition.Exposure == condition,
      Phase == "exposure") %>%
    filter(ParticipantID == first(ParticipantID)) %>%
    mutate(category = ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"))

  get_IBBU_updates(
    exposure = exposure,
    test = test,
    condition = condition,
    cues = cues, kappa = kappa, nu = nu)
}

x <-
  d.for_analysis %>%
  ungroup() %>%
  filter(Phase == "test") %>%
  distinct(Item.VOT, Item.f0_Mel) %>%
  arrange(Item.VOT) %>%
  mutate(x = map2(Item.VOT, Item.f0_Mel, ~ c(.x, .y))) %>%
  pull(x)

# Plot updates for our three exposure condition
d.updates <-  
  bind_rows(
    get_IBBU_updates_for_experiment(condition = "Shift0", test = x, cues = c("VOT")),
    get_IBBU_updates_for_experiment(condition = "Shift10", test = x, cues = c("VOT")),
    get_IBBU_updates_for_experiment(condition = "Shift40", test = x, cues = c("VOT")))

# d.exposure %>%
#   distinct(VOT) %>%
#   ggplot(aes(x = VOT)) +
#   d.updates$sf +
#   scale_x_continuous(limits = c(10, 90)) +
#   geom_label(
#     data = d.updates %>% filter(observation.n == 144),
#     aes(
#       x = PSE,
#       color = Condition.Exposure,
#       label = percent(proportion_of_idealized_PSE)),x

#     y = .5)
```




```{r, get-updates-nu-equal-kappa, eval=FALSE}
x <-
  tibble(VOT = seq(0, 100)) %>%
  mutate(x = map(VOT, ~ c(.x, predict_f0(.x, Mel = T)))) %>%
  pull(x)

# Plot updates for true rightward shifts of various magnitudes
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x),
            get_IBBU_updates(condition = 20, test = x),
            get_IBBU_updates(condition = 30, test = x),
            get_IBBU_updates(condition = 40, test = x))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r get-updates-nu-larger-kappa, eval=FALSE}
d.updates <-  
  bind_rows(get_IBBU_updates(condition = 10, test = x, nu = 10000),
            get_IBBU_updates(condition = 20, test = x, nu = 10000),
            get_IBBU_updates(condition = 30, test = x, nu = 10000),
            get_IBBU_updates(condition = 40, test = x, nu = 10000))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```


```{r get-updates-nu-smaller-kappa, eval=FALSE}
d.updates <-  
  bind_rows(
    get_IBBU_updates(condition = 10, test = x, kappa = 10000),
    get_IBBU_updates(condition = 20, test = x, kappa = 10000),
    get_IBBU_updates(condition = 30, test = x, kappa = 10000),
    get_IBBU_updates(condition = 40, test = x, kappa = 10000))

tibble(VOT = seq(10, 90)) %>%
  ggplot(aes(x = VOT)) +
  d.updates[d.updates$observation.n == 144,]$sf +
  scale_x_continuous() +
  geom_label(
    data = d.updates %>% filter(observation.n == 144),
    aes(
      x = PSE,
      color = Condition.Exposure,
      label = percent(proportion_of_idealized_PSE)),
    y = .5)
```

```{r, eval=FALSE}
m <- readRDS("../models/IBBU-Tan-Jaeger2022-Experiment2-scaled.RDS")
summary(m)
```

## Centering cues relative to expecations [C-CuRE, @apfelbaum-mcmurray2015; @mcmurray-jongman2011]
Similarly, existing models of perceptual normalization---an alternative, but mutually compatible, hypothesis---also predict proportional changes in PSE. <!-- TO DO: write these sections for SI. along a single continuum normalization shift of cue mean just shifts cat function. for multi-dimensional cues, the same is true under cue integration models (variance and thus weighting does not change; shift along each dimension follows same logic as for single cue). for multi-dimensional multivariate models with interacting cues viewed from the perspective of a single cue, shifts can *appear* non-proportional but the pattern observed in KJ16---non proportionality without changes in slopes---would seem to be impossible to create as long as the test stimuli form a line in the multidimensional cue space -->

## Exemplar theory
<!-- generally the most similar exemplars should dominate perception. So it's unclear why one wouldn't converge against the input -->




# Comparing predictions of different adaptive mechanisms
Use database from @chodroff-wilson2018 as a starting point, and then expose the three different adaptive mechanisms introduced in @xie2023 to the distributions from the experiment.

```{r functions-for-adaptive-changes}
# TO DO: consider making plots a bit pretty, e.g., by grouping participant responses together
# into 5msec VOT bins and by making model prediction a line plot, rather than a point plot.
plot_predicted_vs_actual_categorization_responses <- function(data, colors.group = NULL) {
  p <-
    data %>%
    mutate(ExposureGroup = gsub("_Up\\sto", "", ExposureGroup)) %>%
    ggplot(aes(x = VOT)) +
    stat_summary(fun = mean, geom = "line", aes(y = Response)) +
    stat_summary(fun = mean, geom = "line", aes(y = Predicted_posterior), color = "gray") +
    stat_summary(fun.data = mean_cl_boot, geom = "pointrange", aes(y = Response), size = 1/4) +
    facet_wrap(~ ExposureGroup, ncol = 3) 
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group)
  plot(p)

  p %+%
    (data %>%
       mutate(
         ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
         ExposureGroup = gsub("[ABC]A", "", ExposureGroup))) %>%
    plot()

  p <- 
    data %>%
    group_by(ExposureGroup, VOT) %>%
    summarise(across(c(Response, Predicted_posterior), mean)) %>%
    ungroup() %>%
    mutate(
      Test = gsub("^.*test(.*)$", "\\1", ExposureGroup),
      Test = ifelse(Test == "no exposure", 0, Test),
      ExposureGroup = gsub("_Up\\sto", "", ExposureGroup),
      ExposureGroup = gsub("[ABC]A", "", ExposureGroup),
      ExposureGroup = gsub("^.*Shift([0-9]+).*$", "+\\1", ExposureGroup)) %>%
    ggplot(aes(x = Predicted_posterior, y = Response)) +
    geom_abline(intercept = 0, slope = 1, color = "lightgray") +
    geom_text(aes(color = ExposureGroup, label = Test)) +
    geom_smooth(aes(color = ExposureGroup)) +
    annotate(
      geom = "text",
      label = paste0(
        "R^2 = ",
        round(data %>%
                with(., cor(Predicted_posterior, Response)) %>%
                . ^ 2, 3) * 100, "%"),
      x = .1, y = .9) +
    xlab('Predicted proportion "t"-responses') +
    ylab('Observed proportion "t"-responses') 
  
  if (!is.null(colors.group)) p <- p + scale_color_manual("Exposure condition", values = colors.group[c(3, 6, 9, 10)])
  plot(p)
}
```

## Setting prior beliefs about marginal cue means and category means and covariance matrices
<!-- TO DO: consider subsetting to /i/ as following vowel context -->

```{r predictions-exposure-conditions-by-IOs}
# We decided to use C-CuREd VOT and f0 (in Mel) as the input to the ideal observers.
# Our approach to C-CuREing removes effects of speech rate as well as talker-specific
# means of the cues.
cues <- c("VOT", "f0_Mel")

prior_marginal_VOT_f0_stats <-
  d.chodroff_wilson %>%
  group_by(Talker) %>%
  summarise(across(all_of(cues), mean)) %>%
  ungroup() %>%
  summarise(
    x_mean = list(c(VOT = mean(VOT), f0 = mean(f0_Mel))),
    x_var_VOT = var(VOT),
    x_var_f0 = var(f0_Mel),
    x_cov = list(cov(cbind(VOT, f0_Mel))))

m_MVG.VOT_f0 <-
  make_MVG_from_data(
  data = d.chodroff_wilson,
  category = "category",
  cues = cues)

m_IO.VOT_f0 <-
  m_MVG.VOT_f0 %>%
  lift_MVG_to_MVG_ideal_observer(
      Sigma_noise = matrix(c(80, 0, 0, 878), ncol = 2, dimnames = list(names(first(.$mu)), names(first(.$mu)))),
      prior = c("/d/" = .5, "/t/" = .5),
      # TO DO: set to lapse rate inferred in psychometric model fit to participants
      lapse_rate = plogis(fixef(fit_test)[[2]]),
      lapse_bias = c("/d/" = .5, "/t/" = .5))

m_IA.VOT_f0 <-
  m_IO.VOT_f0 %>%
  lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = 10, nu = 10)
```

```{r prepare-data-for-adaptive-changes}
# Make a data frame that splits the entire data in unique exposure-test combinations.
# This data frame will be used for adaptive changes in normalization and category
# representations.
d_for_ASP <-
  d.for_analysis %>%
  ungroup() %>%
  # Exclude catch trials and the final two test blocks since we expect unlearning during those blocks
  # (which is not modeled)
  filter(
    !(Phase == "exposure" & is.na(Item.ExpectedResponse.Voicing)),
    as.numeric(Block) <= 7) %>%
  # Use F0 as intended by generation script. This makes fitting more computationally feasible
  # (336 instead of 1001 unique combinations of group & test locations), and also removes the
  # potential that annotation / measurement error perturb the f0 values. It does, however,
  # make the assumption that f0 is the same across the three minimal pairs. While likely wrong,
  # this assumption better aligns with the fact that the model doesn't have a way to keep track
  # of any lexical, phonological, or phonetic context effects on VOT (and thus no way to predict
  # any potential differences between minimal pairs).
  # rename(VOT = Item.VOT, f0_Mel = Item.F0_target_for_generation_script) %>%
  # Create new condition variable that uniquely identifies what participants have seen at any of the
  # tests and a new block variable that identifies the type and order of blocks. Also create two
  # convenience variables, x (stimulus) and Response.Category (to make matching with the category
  # column of ideal observers/adaptors more straightforward).
  mutate(
    Condition.for_ASP = paste0(Condition.Exposure, List.ExposureBlockOrder, List.ExposureMaterials),
    Block.for_ASP = paste0(Phase, Block),
    x = map2(VOT, f0_Mel, ~ c(.x, .y)),
    Response.Category = factor(ifelse(Response.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/")),
    Item.ExpectedResponse.Category = factor(ifelse(Item.ExpectedResponse.Voicing == "voiced", "/d/", "/t/"), levels = c("/d/", "/t/"))) %>%
  slice_into_unique_exposure_test(
    condition_var = "Condition.for_ASP",
    block_var = "Block.for_ASP",
    block_order = c("test1", "exposure2", "test3", "exposure4", "test5", "exposure6", "test7")) %>%
  mutate(ExposureGroup = factor(ExposureGroup)) %>%
  select(
    ExposureGroup, Phase, ParticipantID, Block, Trial, x,
    VOT, f0_Mel, vowel_duration, Response.Category, Item.ExpectedResponse.Category)
```


```{r set-subsample-proportion}
set.seed(1976)
# For changes in decision-making, the adaptive algorithm is order-sensitive. This makes it computationally
# demanding to find the optimal parameterization for this algorithm (as optimization requires updating the
# data and calculating the likelihood of the test responses under the updated model, but order-sensitivity
# means that the updating cannot be vectorized but must be calculated trial-by-trial). This makes it
# computationally infeasible to find the optimal learning rate (beta) for the entire data set. We thus
# select a subset of participants for each exposure-test combination, optimize the parameter for that subset,
# and then use that (approximately) 'optimal' parameter to update the *entire* data set (once) to calculate
# the likelihood, R2, etc. for that parameter.
#
# In order to penalize all adaptive models equally, we subset the same amount of data for each model,
# including the other two change models (even though they are computationally much less demanding to fit).
sample_proportion_of_participants_per_exposure_test_condition <- .5

# TO DO: IMPLEMENT SUBSAMPLING FOR NORMALIZATION AND FOR IBBU TO COMPARE LIKELIHOODS IN A MEANINGFUL WAY.
# ---> Actually since the other algorithms are not order sensitive, they should not depend on how many
# participants are included in the fitting. Perhaps a better way to make this a fair comparison is to 
# subset the test data?
```

# Fitting models to the data

## Changes in normalization

```{r}
# Only one participant per exposure group is needed for exposure data
# (since they all have the same exposure up to that point, and our
# approach to normalization is order-insensitive; so doing this separately
# for each participant shouldn't make a difference).
#
# -------------------------------------------------------------------------
# TO DO: results changes after limiting data to first participant in each
# exposure group. this is unexpected. One possibility is that this is simply
# an optimization problem. another possibility is that participants differ
# in their cue means even within ExposureGroup (e.g., due to missing data)
# even though that shouldn't be the case.
# -------------------------------------------------------------------------
filename.parameter <- "../models/best_performing_parameters.normalization.rds"
filename.df <- "../models/best_performing_data.normalization.rds"

if (RESET_MODELS || !file.exists(filename.parameter)) {
  # This function is intended for the optimization run right below it
  history.optimization_normalization <- tibble(.rows = 0)
  range.prior_kappa.normalization <- c(10^-5, 10^5)

  d_for_ASP.for_normalization <-
    d_for_ASP %>%
    group_by(ExposureGroup, Phase) %>%
    filter(
      ParticipantID %in%
        sample(
          x = unique(ParticipantID),
          size = round(length(unique(ParticipantID)) * sample_proportion_of_participants_per_exposure_test_condition)))

  best_performing_parameters.normalization <-
    optim(
      par = log(mean(range.prior_kappa.normalization)),
      fn = get_likelihood_from_updated_normalization,
      method = "L-BFGS-B",
      lower = log(min(range.prior_kappa.normalization)),
      upper = log(max(range.prior_kappa.normalization)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  saveRDS(best_performing_parameters.normalization, filename.parameter)

  # Get the normalized data based on the optimal parameter for normalization
  # and then prepare it for plotting
  df.normalized <-
    update_normalization_and_normalize_test_data(
      kappa = exp(best_performing_parameters.normalization$par[1]),
      data = d_for_ASP)

  df.normalized %<>%
    mutate(observationID = 1:nrow(.)) %>%
    # Get posterior of prior model for the normalized data and join it to the
    # data.
    left_join(
      df.normalized %>%
        group_map(
          .f =
            ~ get_posterior_from_model(
              model = m_IO.VOT_f0,
              x = .x$x,
              noise_treatment = "marginalize",
              lapse_treatment = "marginalize") %>%
            filter(category == "/t/")) %>%
        .[[1]],
      by = "observationID") %>%
    # Prepare the data frame for plotting by renaming variable names to those
    # expected by plot_predicted_vs_actual_categorization_responses
    mutate(
      VOT = map_dbl(x.x, ~ .x[1]),
      Response = ifelse(Response.Category == "/t/", 1, 0)) %>%
    rename(Predicted_posterior = posterior_probability)

  # TO DO: calculate likelihood for whole data set here

  saveRDS(df.normalized, filename.df)
  d_for_ASP.for_normalization <- NULL
} else {
  best_performing_parameters.normalization <- readRDS(filename.parameter)
  df.normalized <- readRDS(filename.df)
}

cat("Changes in normalization achieve maximum log-likelihood of", best_performing_parameters.normalization$value, "on", sample_proportion_of_participants_per_exposure_test_condition * 100, "percent of the data for kappa =", exp(best_performing_parameters.normalization$par))
cat("The same kappa achieved log-likelihood of", NA, "one whole data set.")
```

```{r, fig.width=7, fig.height=6, fig.cap="Ideal Adapter predicted categorization behavior"}
df.normalized %>% 
  plot_predicted_vs_actual_categorization_responses()
```

## Changes in decision-making

```{r}
filename.parameter <- "../models/best_performing_parameters.decision_making.rds"
filename.df <- "../models/best_performing_data.decision_making.rds"

if (RESET_MODELS || !file.exists(filename.parameter)) {
  history.optimization_bias <- tibble(.rows = 0)
  range.beta <- c(10^-3, 10^3)

  d_for_ASP.for_decision_changes <-
    d_for_ASP %>%
    ungroup() %>%
    # Include all test observations but only the exposure groups that contain all 3 exposure
    # blocks (which then are reformatted as part of the updating function in order to derive
    # the predicted updated decision biases after each of the three exposure blocks).
    filter(
      # We exclude the no exposure group since its likelihood does not depend on
      # beta, and thus cannot affect the optimization of beta.
      ExposureGroup != "no exposure",
      Phase == "test" | grepl(x = ExposureGroup, pattern = ".*Up to test7")) %>%
    # Sample proportion of participants from each exposure condition
    mutate(ExposureGroup_withoutBlock = gsub("_Up to test.*", "", ExposureGroup)) %>%
    group_by(ExposureGroup_withoutBlock) %>%
    filter(ParticipantID %in%
             sample(
               x = unique(ParticipantID),
               size = round(length(unique(ParticipantID)) * sample_proportion_of_participants_per_exposure_test_condition))) %>%
    arrange(ParticipantID, Block, ExposureGroup, Phase, Trial) %>%
    droplevels()

  # Calculating exposure and test data outside of the optimization loop, in order to minimize
  # computations that need to be conducted on each optimization step.
  d_for_ASP.for_decision_changes.exposure <-
    d_for_ASP.for_decision_changes %>%
    ungroup() %>%
    filter(Phase == "exposure") %>%
    group_by(ExposureGroup, ParticipantID, Phase) %>%
    droplevels() %>%
    mutate(
      (!! sym(cues[1])) := map_dbl(x, ~ .x[1]),
      (!! sym(cues[2])) := map_dbl(x, ~ .x[2])) %>%
    group_by(ExposureGroup, ParticipantID)

  d_for_ASP.for_decision_changes.test <-
    d_for_ASP.for_decision_changes %>%
    filter(Phase == "test") %>%
    ungroup() %>%
    select(ExposureGroup, ParticipantID, x, Response.Category) %>%
    group_by(ExposureGroup, ParticipantID) %>%
    nest(data = c(x, Response.Category))

  best_performing_parameters.bias <-
    optim(
      par = mean(log(range.beta)),
      fn = get_likelihood_from_updated_bias,
      method = "L-BFGS-B",
      lower = log(min(range.beta)),
      upper = log(max(range.beta)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  saveRDS(best_performing_parameters.bias, filename.parameter)

  # Use the optimal parameter to update the model's decision biases for *all* observations and then apply
  # the model to the test data to calculate the log-likelihood and predicted responses for the entire data.
  df.bias <-
    update_decision_bias_by_group(
      prior = m_IO.VOT_f0,
      beta = exp(best_performing_parameters.bias$par),
      data =  
        d_for_ASP %>%
        group_by(ExposureGroup, ParticipantID, Phase) %>%
        filter(Phase == "exposure", grepl(x = ExposureGroup, pattern = ".*Up to test7")) %>%
        arrange(ParticipantID, Phase, Block) %>%
        droplevels() %>%
        mutate(
          (!! sym(cues[1])) := map_dbl(x, ~ .x[1]),
          (!! sym(cues[2])) := map_dbl(x, ~ .x[2])) %>%
        group_by(ExposureGroup, ParticipantID)) %>%
    format_updated_bias_models_and_join_test_data(
      prior = m_IO.VOT_f0,
      # Here we do *not* exclude the "no exposure" condition since we want to calculate the likelihood for
      # the entire data set.
      data.test = d_for_ASP %>%
        filter(Phase == "test") %>%
        select(ExposureGroup, ParticipantID, x, Response.Category) %>%
        group_by(ExposureGroup, ParticipantID) %>%
        nest(data = c(x, Response.Category)))

  # TO DO: get ll for entire data here

  df.bias %<>%
    # Categorize test tokens
    mutate(
      categorization = future_map2(
        model,
        data,
        ~ get_categorization_from_MVG_ideal_observer(
          model = .x,
          x = .y$x,
          decision_rule = "proportional",
          noise_treatment = "marginalize",
          lapse_treatment = "marginalize") %>%
          filter(category == "/t/"))) %>%
    # Prepare the data frame for plotting by renaming variable names to those
    # expected by plot_predicted_vs_actual_categorization_responses
    mutate(Response = map(data, ~ ifelse(.x$Response.Category == "/t/", 1, 0))) %>%
    select(-c(model, data)) %>%
    unnest(c(categorization, Response)) %>%
    mutate(VOT = map_dbl(x, ~ .x[1])) %>%
    rename(Predicted_posterior = response)

  saveRDS(df.bias, filename.df)
  d_for_ASP.for_decision_changes <- NULL
} else {
  best_performing_parameters.bias <- readRDS(filename.parameter)
  df.bias <- readRDS(filename.df)
}

cat("Changes in decision-making achieve maximum likelihood of", best_performing_parameters.bias$value, "on", sample_proportion_of_participants_per_exposure_test_condition * 100, "percent of the data for beta =", exp(best_performing_parameters.bias$par))
cat("The same kappa achieved log-likelihood of", NA, "one whole data set.")
```



```{r, fig.width=7, fig.height=6}
df.bias %>% plot_predicted_vs_actual_categorization_responses()
```

## Changes in category representations

```{r, eval=FALSE}
# NOT YET IMPLEMENTED
filename <- "../models/best_performing_parameters.representation.rds"

# This function is intended for the optimization run right below it
history.optimization_representation <- tibble(.rows = 0)
range.prior_kappa.representation <- c(3, 10^5)
range.prior_nu.representation <- c(3, 10^5)

update_representations <- function(
  prior,
  data
) {
  cues <- get_cue_labels_from_model(prior)

  data %>%
    filter(Phase == "exposure")
    mutate(
      (!!! syms(cues))[1] := map_dbl(x, ~ .x[1]),
      (!!! syms(cues))[2] := map_dbl(x, ~ .x[2])) %>%
      group_by(ExposureGroup) %>%
      group_map(
        .f = ~ update_NIW_ideal_adaptor_incrementally(
          prior = prior,
          exposure = .x,
          exposure.category = "Item.ExpectedResponse",
          exposure.cues = cues,
          noise_treatment = "marginalize",
          lapse_treatment = "marginalize",
          method = "label-certain",
          keep.update_history = FALSE,
          keep.exposure_data = FALSE) %>%
          nest(posterior = everything())) %>%
      reduce(bind_rows)
}

get_likelihood_from_updated_representation <- function(
    par,
    prior = m_IA.VOT_f0,
    data = d_for_ASP
) {
  kappa <- exp(par[1])

  ll <-
    update_representations(
      kappa = kappa,
      data = data) %>%
    get_likelihood_from_grouped_data(model = prior)

  history.optimization_representation <<-
    bind_rows(
      history.optimization_representation,
      tibble(kappa = kappa, nu = nu, log_likelihood = ll))

  return(ll)
}

if (RESET_MODELS || !file.exists(filename)) {
  best_performing_parameters.representation <-
    optim(
      par = log(mean(range.prior_kappanu.representation)),
      fn = get_likelihood_from_updated_normalization,
      method = "L-BFGS-B",
      lower = log(min(range.prior_kappanu.representation)),
      upper = log(max(range.prior_kappanu.representation)),
      control = list(
        fnscale = -1,
        factr = 10^8))

  best_performing_parameters.representation <- exp(best_performing_parameters.representation$par[1])
  saveRDS(best_performing_parameters.representation, filename)
} else {
  best_performing_parameters.representation <- readRDS(filename)
}
```

## Fit IA with 3-cue prior

```{r}
cues <- c("VOT", "f0_Mel", "vowel_duration")

m_MVG.VOT_f0_vowel <-
  make_MVG_from_data(
  data = d.chodroff_wilson.isolated,
  category = "category",
  cues = cues)

m_IO.VOT_f0_vowel <-
  m_MVG.VOT_f0_vowel %>%
  lift_MVG_to_MVG_ideal_observer(
      Sigma_noise = matrix(c(80, 0, 0, 0, 878, 0, 0, 0, 80), ncol = 3, dimnames = list(names(first(.$mu)), names(first(.$mu)))),
      prior = c("/d/" = .5, "/t/" = .5),
      # TO DO: set to lapse rate inferred in psychometric model fit to participants
      lapse_rate = plogis(fixef(fit_test)[[2]]),
      lapse_bias = c("/d/" = .5, "/t/" = .5))


# fit ideal adaptor with 3 cues priors
m_IA.VOT_f0_vowelduration_test <-
  infer_prior_beliefs(
    exposure = d_for_ASP %>% filter(Phase == "exposure"),
    test = d_for_ASP %>% filter(Phase == "test"),
    cues = c("VOT", "f0_Mel", "vowel_duration"),  
    category = "Item.ExpectedResponse.Category",
    response = "Response.Category",
    group = "ParticipantID",
    group.unique = "ExposureGroup",
    center.observations = T,   
    scale.observations = T,
    pca.observations = F,
    lapse_rate = NULL,
    mu_0 = m_IO.VOT_f0_vowel$mu,
    Sigma_0 = m_IO.VOT_f0_vowel$Sigma,
    sample = T,
    file = "../models/IBBU_VOT+f0+vowelduration_test_priors.RDS",
    warmup = 3000, iter = 4000,
    control = list(adapt_delta = .995, max_treedepth = 15))

```

