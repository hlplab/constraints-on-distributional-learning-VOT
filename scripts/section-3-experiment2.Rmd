```{r}
require(tidyverse)
require(magrittr)

require(brms)
require(MVBeliefUpdatr)
require(phonR)
require(rsample)

source("functions.R")
```


# EXPERIMENT 2: Listeners' adaptation to an unfamiliar talker
The aim of experiment 2 was to investigate the incremental changes in listener categorization when perceiving speech of an unfamiliar talker with cue-to-category mappings characterised by varying degrees of typicality of an L1-US English talker. Listeners performed a task similar to that of experiment 1, that is, they heard isolated words on a /d/ - /t/ continuum and were required to select the word they heard. Unlike experiment 1 where all listeners categorised stimuli on a single uninformative continuum, listeners in experiment 2 were divided into 3 groups with each group exposed to different VOT distributions that were informative of the talker's realisations of /d/ and /t/. 

We approximated a "typical" talker through the combined parameters estimated from the perceptual responses in experiment 1 and a database of L1-US English /d/ and /t/ productions [@Xie]. From this estimated baseline distribution (+0ms), we shifted the distribution by +10ms, and by +40ms, yielding three exposure talker conditions. To investigate the state of listener expectations as they move from having no information about how a new talker realises /d/s and /t/s to progressively more information about the talker's pronunciations we implement identical test blocks (i.e. test stimuli in identical locations) within and across conditions before, during, and after informative exposure. Under Bayesian ideal adaptor inferential processes, listeners' weighting of their prior beliefs about the category means and variances will determine the speed at which adaptation occurs. Motivated by prior work in supervised and unsupervised learning within lab contexts that repeatedly show adaptation to be a rapid process [@clarke2004rapid; @bradlow2008perceptual; @xie2021cross; @liu2018inferring; @norris2003perceptual @kleinschmidt2012continuum] we made the decision to test our participants early on in the experiment.

Previous studies were not designed to investigate incremental adaptation in this manner as they lacked designated test blocks; listeners' categorisation functions were instead estimated over portions of the exposure trials which ignores the possibility that not all participants had been exposed to the full distributional information at the trial cut-off point (although that would have been the case by the end of the experiment). With our novel design we gain better resolution at every testing point, since each participant would have heard the same number of VOT items at the beginning of a given test block. The other advantage is that identical test blocks across conditions standardises the assessment of behavioural changes between groups yielding more accurate comparisons. We specifically included a pre-exposure test block with a similar aim to experiment 1 -- in order to capture the implicit expectations of listeners about the cue-to-category mappings of US English /d/ and /t/. We later compare this block with the behavioural results of experiment 1. 

Another notable innovation we bring to this study in conjunction with the use of qualitatively more human-sounding stimuli (as described in section 2.X), relates to the parameters of the exposure distributions. Prior studies of this type simulate the voiced-voiceless distributions by exposing listeners to  category distributions that are symmetrical and equivalent between categories. It is however, unlikely that listeners encounter this in real life as evidenced from production data [@chodroffstructure]. By generating distributions that are closer in form to that of real data we hope to improve the ecological validity of the results.


## Methods
### Participants
Participants were recruited over the Prolific platform and experiment data (but not participant profile data) were collected, stored, and via proliferate (@schuster). They were paid $8.00 each (for a targeted remuneration of \$9.60/hour). The experiment was visible to participants following a selection of Prolific's available pre-screening criteria. Participants had to (1) have US nationality, (2) report to only know English, and (3) had not previously participated in any experiment from our lab on Prolific.

126 L1 US English listeners (male = 60, female = 59, NA = 3; mean age = 38 years; SD age = 12 years) completed the experiment. Due to data transfer errors 4 participants' data were not stored and therefore not included in this analysis. To be eligible, participants had to confirm that they (1) spent at least the first 10 years of their life in the US speaking only English, (2) were in a quiet place and free from distractions, and (3) wore in-ear or over-the-ears headphones that cost at least \$15.

### Materials
A subset of the materials described in experiment 1 were used, in particular three continua of the minimal pairs, dill-till, din-tin, and dip-tip. The dim-tim continuum was omitted in order to keep the pairs as distinguishable as possible. 

We employed a multi-block exposure-test design \@ref(fig:exp2-design-figure) which enabled the assessment of listener perception before informative exposure as well as incrementally at intervals  during informative exposure (after every 48 exposure trials). To have a comparable test between blocks and across conditions, test blocks were made up of a uniform distribution of 12 VOT stimuli (-5, 5, 15, 25, 30, 35, 40, 45, 50, 55, 65, 70), identical across test blocks and between conditions. Each of the test tokens were presented once at random. The test blocks were kept short to avoid cancelling out any distributional learning effects after each exposure. After the final exposure block we tripled the number of test blocks to increase the statistical power to detect exposure induced changes. 

The conditions were created by first obtaining the baseline distribution (+0ms shift) and then shifting that distribution by +10ms and by +40ms to create the remaining two conditions. 

The +0ms shift condition was estimated from the fitted point of subjective equality (PSE) from experiment 1. The PSE corresponds to the VOT measurement that was perceived as the most ambiguous by participants in experiment 1 (i.e. the stimulus that elicited equal probability of being categorised as /d/ or /t/) thus marking the categorical boundary. The PSE is where the likelihoods of both categories intersect and have equal density (we assumed Gaussian distributions and equal prior probability for each category) [SOMETHING HERE ABOUT GAUSSIANS BEING A CONVENIENT ASSUMPTION?]. To limit the infinite combinations of likelihoods that could intersect at this value, we set the variances of the /d/ and /t/ categories based on parameter estimates (@Kurumada_Xie_Jaeger_2022) obtained from the production database of @chodroff2017structure. To each variance value we added an 80ms noise variance following (@kronrod2016unified) to account for variability due to perceptual noise since these likelihoods were estimated from perceptual data. We took an additional degree of freedom of setting the *distance between the means* of the categories at 46ms; this too was based on the population parameter estimates derived from analyses of the production database. The means of both categories were then obtained through a grid-search process to find the likelihood distributions that crossed at 25ms VOT (see XX of SI for details on this procedure).

The distributional make up was determined through a process of sampling tokens from a discrete normal distribution (available through the `extraDistr` package in R). [EXPLAIN WHAT DISCRETE NORMAL SAMPLING GIVES] discretised normal distributions are approximation... 

For each exposure block 8 VOT tokens of each minimal pair item were sampled from discrete normal distributions of each category of the +0ms condition, giving 24 /d/ and 24 /t/ (48 critical trials) per block. Additionally, each exposure block contained 2 instances of 3 catch items, giving 6 catch trials per block. The sampled VOT tokens were increased by a margin of +10ms and +40 ms for the remaining two conditions. Three variants of each condition list were created so that exposure blocks followed a latin-square order. 

Lastly, half of the exposure trials were randomly assigned as labelled trials. In labelled trials, participants receive clear information of the word's category as both orthographic options will always begin with the intended sound. For example if a trial was intended to be "dill" then the two image options will either be "dill" and "dip" or "dill" and "din". Test trials were always unlabelled. 


```{r exp2-design-figure, fig.height=3, fig.width=5, fig.cap="Experiment 2 multi-block design. Test blocks in grey comprised identical stimuli within and between conditions"}
knitr::include_graphics("../figures/experiment2_design_image.png")
```


```{r exp2-design-distribution, fig.height=3, fig.width=6, warning=FALSE, message=FALSE}
# read in exposure block sampled tokens
d.exposure <- read_csv("../data/exposure_block_tokens.csv", show_col_types = F)

# set variances of categories
var_d <- 80
var_t <- 270

d.means <- 
  crossing(condition = c("+0ms", "+10ms", "+40ms"),
         category = c("/d/", "/t/")) %>% 
  mutate(mean = c(5, 50, 15, 60, 45, 90))

d.exposure %>% 
  na.omit() %>% 
  filter(image_selection == "forward" & list_LSQ_variant == "A") %>% 
  mutate(condition = case_when(condition == "Shift0" ~ "+0ms",
                               condition == "Shift10" ~ "+10ms",
                               condition == "Shift40" ~ "+40ms"),
         labelling = as_factor(labelling)) %>% 
  ggplot() +
  geom_histogram(aes(x = VOT, fill = paste(category, labelling), 
                     color = paste(category, labelling),
                     linetype = labelling), 
                 alpha = .6) +
  scale_colour_manual(
    "Labelling",
    values = c(
    "/d/ labeled" = "#404040", 
    "/d/ unlabeled" = "#b3b3b3",
    "/t/ labeled" = "#808080",
    "/t/ unlabeled" = "#e6e6e6"),
    aesthetics = c("color", "fill"),
    labels = c("/d/ labeled", "/d/ unlabeled", "/t/ labeled", "/t/ unlabeled")) +
  guides(colour = guide_legend(override.aes = list(
    linetype = c(2, 1)))) +
  stat_function(fun = function(x) 72 * 5 * dnorm(x, 5, sqrt(var_d)),
                color = "black", size = .6, alpha = .7, linetype = 2) +
  stat_function(
    fun = function(x) 72 * 5 * dnorm(x, 50, sqrt(var_t)),
    color = "black", size = .6, alpha = .5, linetype = 2) +
  geom_rug(data = tibble(VOT = c(5, 50)), aes(x = VOT), sides = "t") +
  scale_x_continuous("VOT (ms)", breaks = seq(-50, 150, 30)) +
  scale_y_continuous("Count") +
  geom_text(data = d.means,
            aes(x = 103, 
                y = 17,
                label = paste("mean", category, "=", mean)),
            size = 2,
            position = position_dodge2v(height = -8),
            inherit.aes = F) +
  facet_wrap(~ condition, scales = "free_y") +
  guides(linetype = "none") +
  theme(legend.position = "top")
```



### Procedure
The code for the experiment is available as part of the OSF repository for this article. A live version is available at (https://www.hlp.rochester.edu/FILLIN-FULL-URL). The first page of the experiment informed participants of their rights and the requirements for the experiment: that they had to be native listeners of English, wear headphones for the entire duration of the experiment, and be in a quiet room without distractions. Participants had to pass a headphone test, and were asked to keep the volume unchanged throughout the experiment. Participants could only advance to the start of the experiment by acknowledging each requirement and consenting to the guidelines of the Research Subjects Review Board of the University of Rochester. 

On the next page, participants were informed about the task for the remainder of the experiment. They were informed that they would hear a female talker speak a single word on each trial, and had to select which word they heard. They were also informed that they needed to click a green button that would be displayed during each trial when it "lights up" in order to hear the recording of the speaker saying the word. Participants were instructed to listen carefully and answer as quickly and as accurately as possible. They were also alerted to the fact that the recordings were subtly different and therefore may sound repetitive. This was done to encourage their full attention.

The trials were presented in the same way as in experiment 1 except that the audio playback was controlled by the participant. This additional step was implemented to increase participant attention to the stimuli. The placement of the image presentations were counter-balanced across participants.

Participants underwent 234 trials which included 6 catch trials in each exposure block (18 in total). Participants were given the opportunity to take breaks after every 60 trials during exposure blocks. Participants took an average of 17 minutes (SD = 9) to complete the 234 trials, after which they answered a short survey about the experiment.

```{r}
# load formatted exposure and test data from experiment 2
d.exposure_test <- read_csv("../data/exposure_test_experiment2.csv", show_col_types = F)

# load f0-5ms-into-vowel measurements of stimuli
d.f0.5ms <- 
  read_csv("../data/AEDLVOT_stimuli_f0_5ms.csv", show_col_types = F) %>% 
  select(filename, VOT, f0_5ms_into_vowel) %>% 
  rename(Item.VOT = VOT,
         Item.F0_5ms = f0_5ms_into_vowel,
         Item.Filename = filename) %>% 
  mutate(Item.Filename = paste0(Item.Filename, ".wav"))

# add f0-5ms data
d.exposure_test %<>% 
ungroup() %>%
  left_join(d.f0.5ms, by = c("Item.Filename", "Item.VOT")) %>% 
              mutate(Item.Mel_F0_5ms = normMel(Item.F0_5ms)) 

# mark catch trials rows and mark those to be excluded
d.exposure_test %<>% 
  mutate(
    Is.CatchTrial = ifelse(Item.ExpectedResponse %in% c("flare", "rare", "share"), TRUE, FALSE),
    CatchTrial.Correct = ifelse(Is.CatchTrial == TRUE, 
                    ifelse(Item.ExpectedResponse == Response, TRUE, FALSE), NA),
    Answer.sex.Correct = ifelse(sex == "woman", TRUE, FALSE)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Exclude_participant.due_to_catch_trials = ifelse(sum(CatchTrial.Correct, na.rm = TRUE) < 17, TRUE, FALSE)) %>%
  ungroup()

# mark labelled trials
d.exposure_test %<>% 
  mutate(Response.Correct = ifelse(Item.ExpectedResponse == Response, TRUE, FALSE),
         LabeledTrial.Correct = ifelse(Item.Labeled == TRUE, ifelse(Response.Correct == TRUE, TRUE, FALSE), NA)) %>% 
  group_by(ParticipantID) %>% 
  mutate(Exclude_participant.due_to_labeled_trials = ifelse(sum(LabeledTrial.Correct, na.rm = T) < 68, TRUE, FALSE))
```



```{r set-exclusion-due-to-categorisation-slope, warning=FALSE}
# get data for exclusion due to categorisation slope of first 36 trials. 
d.VOT_exclusion <- d.exposure_test %>% 
  filter(Block == 1 | (Block == 2 & Trial %in% c(13:36))) %>% 
  drop_na(c(ParticipantID, Response.Voicing, Item.VOT)) %>% 
  mutate(Response.ProportionVoiceless = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  select(c(Experiment, ParticipantID, Condition.Exposure, Response.Voicing, Response.ProportionVoiceless, Item.VOT))

# set the VOT criteria 
empirical_means <- c(17, 62)
VOT_for_targeted_proportion_t <- empirical_means + c(-20, 20)
targeted_proportion_t <- c(.15, .80) 


# Fit logistic regression by participant to get model predictions (REPLACE WITH LAPSE ESTIMATED NON-LINEAR FIT)
d.VOT_exclusion %<>%
  group_by(ParticipantID, Experiment, Condition.Exposure) %>%
  nest() %>%
  mutate(
    CategorizationModel =
      map(data, ~ glm(Response.ProportionVoiceless ~ 1 + Item.VOT, data = .x, family = binomial))) %>% 
  # type = "response" in predict() gives the probability
  summarise(Model.predicted.Reponse = 
              map(CategorizationModel, ~ predict(object = .x, 
                                                 newdata = tibble(Item.VOT = VOT_for_targeted_proportion_t), 
                                                 type = "response"))) %>% 
  mutate(Exclude_participant.due_to_VOT_slope = 
           map(Model.predicted.Reponse, ~ ifelse(.x[1] > targeted_proportion_t[1] || .x[2] < targeted_proportion_t[2], TRUE, FALSE)),
         Exclude_participant.due_to_lower_VOT = map(Model.predicted.Reponse, ~ ifelse(.x[1] > targeted_proportion_t[1], TRUE, FALSE)),
         Exclude_participant.due_to_higher_VOT = map(Model.predicted.Reponse, ~ ifelse(.x[2] < targeted_proportion_t[2], TRUE, FALSE))) %>% 
  select(ParticipantID, Experiment, Condition.Exposure, Exclude_participant.due_to_VOT_slope, Exclude_participant.due_to_lower_VOT, Exclude_participant.due_to_higher_VOT) %>% 
  mutate(across(c(Exclude_participant.due_to_VOT_slope, 
                  Exclude_participant.due_to_lower_VOT, 
                  Exclude_participant.due_to_higher_VOT), .fns = unlist)) %>% 
  ungroup()

# counting TRUEs
d.VOT_exclusion %>% 
  group_by(Exclude_participant.due_to_VOT_slope, Condition.Exposure) %>% 
  summarise(n())
  
d.exposure_test %<>% left_join(d.VOT_exclusion)
```



```{r set-RT-exclusion-criteria, message=FALSE}
# calculate RT exclusion AFTER removing excluded participants due to other criteria
d.test_exposure_for_analysis <- d.exposure_test %>%
  filter(
    Is.CatchTrial == FALSE &
    Exclude_participant.due_to_catch_trials == FALSE &
    Exclude_participant.due_to_labeled_trials == FALSE &
    Exclude_participant.due_to_VOT_slope == FALSE) %>% 
  group_by(ParticipantID) %>%
  mutate(
    Response.log_RT = log10(ifelse(Response.RT <= 0, NA_real_, Response.RT)),
    Response.log_RT.scaled = scale(Response.log_RT), # deducts each value with subject's own mean, divide by own SD
    Response.log_RT.mean = mean(Response.log_RT, na.rm = T)) %>% 
  ungroup() %>% 
  mutate(Exclude_participant.due_to_RT = ifelse(abs(scale(Response.log_RT.mean)) > 3, TRUE, FALSE)) %>% 
  filter(Exclude_participant.due_to_RT == FALSE) %>% 
  mutate(Exclude_trial.due_to_RT = ifelse(abs(Response.log_RT.scaled) > 3, TRUE, FALSE))

# get number of participants excluded due to RT
excl.RT.participant <- d.test_exposure_for_analysis %>% 
  filter(Exclude_participant.due_to_RT == TRUE) %>% 
  tally()

# get number of trials excluded due to RT
excl.RT.trial <- d.test_exposure_for_analysis %>%  
  filter(Exclude_trial.due_to_RT == TRUE) %>% 
  tally()

# proportion of trials excluded due to RT
proportion.trials.excluded <- excl.RT.trial/nrow(d.test_exposure_for_analysis)

# get number of participants excluded due to catch trial
excl.catch <- d.exposure_test %>% 
  filter(Exclude_participant.due_to_catch_trials == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally() 

# get number of participants excluded due to labelled trials
excl.labeled <- d.exposure_test %>% 
  filter(Exclude_participant.due_to_labeled_trials == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally() 

# count number of exclusions due to VOT slope
excl.VOT <- d.exposure_test %>% 
  filter(Exclude_participant.due_to_VOT_slope == TRUE) %>% 
  group_by(ParticipantID) %>% 
  summarise() %>% 
  tally()

# remove excluded trials due to slow RT
d.test_exposure_for_analysis %<>% filter(Exclude_trial.due_to_RT == FALSE) 
```

### Exclusions
We excluded from analysis participants who committed more than 3 errors out of the 18 catch trials (<84% accuracy, N = 1), participants who committed more than 4 errors out of the 72 catch trials (<94% accuracy, N = 0), participants with an average reaction time (RT) more than three standard deviations from the mean of the by-participant means (N = 0), and participants who reported not to have used headphones (N = 0) or not to be native (L1) speakers of US English (N = 0). 

In addition, participants' categorization during the early phase of the experiment were scrutinised for their slope orientation and their proportion of "t"-responses at the least ambiguous locations of the VOT continuum. The early phase of the experiment was defined as the first 36 trials and the least ambiguous locations were defined as -20ms from the empirical mean of the /d/ category and +20ms from the empirical mean of the /t/ category. These means were taken from the production data estimates by @Kurumada_Xie_Jaeger_2022.
For the remaining participants, trials that were more than three SDs from the participant's mean RT were excluded from analysis (`r proportion.trials.excluded * 100 `%). Finally, we excluded participants (N = 0) who had less than 50% data remaining after these exclusions. 



## Behavioral results
We first present participants' categorisation responses. Given that this experiment was designed to give pre-exposure test data, we run an analysis on test block 1 that is similar to the IO analysis of experiment 1. 



### Analysis approach


```{r load-br-model-block1}
# make Block 1 subset
d.block1 <- d.test_exposure_for_analysis %>% 
  filter(Block == 1) 
# set the mean and SD values for scaling/unscaling purposes. Use means and SD of VOTs within test blocks only
VOT.mean_exp2 <- mean(d.block1$Item.VOT)
VOT.sd_exp2 <- sd(d.block1$Item.VOT)
f0.mean_exp2 <- mean(d.block1$Item.Mel_F0_5ms)
f0.sd_exp2 <- sd(d.block1$Item.Mel_F0_5ms)

d.block1 %<>% 
  mutate(
    Response.Voiceless = ifelse(Response.Voicing == "voiceless", 1, 0),
    VOT_gs = (Item.VOT - VOT.mean_exp2)/ (2 * VOT.sd_exp2),
    F0_gs = (Item.Mel_F0_5ms - f0.mean_exp2) /(2 * f0.sd_exp2))

# load the full experiment fit to obtain lapse rate
fit_mix_uniform_bias <- readRDS("../models/Exp-AE-DLVOT-labelled-lapsing-GLMM.rds")
lapse_exp2 <- summary(fit_mix_uniform_bias)$fixed["theta1_Intercept", 1]

# run brms model of data for block 1 using lapse rate from full model (section X.X) 
fit_mix_block1 <- brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 1 + VOT_gs + (1 + VOT_gs | ParticipantID),
    theta1 ~ 0 + offset(lapse) + (1 | ParticipantID)),
  data = d.block1 %>% 
    mutate(lapse = lapse_exp2),
  cores = 4,
  iter = 3000, # iterations to run
  warmup = 1500, # samples used to fit
  chains = chains,
  family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
  control = list(adapt_delta = .999),
  file = "../models/Block1-AEDLVOT-lapse-exp2_2ndfit.rds")
```






```{r}
# get PSE of Block 1
PSE.fit_mix_block1 <- descale(-(summary(fit_mix_block1)$fixed["mu2_Intercept", 1] / summary(fit_mix_block1)$fixed["mu2_VOT_gs", 1]), VOT.mean_exp2, VOT.sd_exp2)

# get posterior samples of intercept and slope, and median qi of the PSE 
post_sample_block1 <- fit_mix_block1 %>% 
  spread_draws(b_mu2_Intercept, b_mu2_VOT_gs) %>% 
  mutate(PSE = descale(-(b_mu2_Intercept/b_mu2_VOT_gs),VOT.mean_exp2, VOT.sd_exp2)) %>% 
  median_qi(PSE)

#plot the fitted psychometric function for Block 1
psychometric_fit_block1 <- conditional_effects(
    fit_mix_block1, 
    effects = "VOT_gs",
    method = "posterior_epred",
    plot = F)[[1]]

# make base plot
p.fit_block1 <- psychometric_fit_block1 %>% 
  ggplot(aes(x = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2), 
             y = estimate__)) +
  scale_x_continuous("VOT (ms)", limits = c(-5, 70)) +
  scale_y_continuous("Fitted proportion of 't' responses") +
  geom_ribbon(aes(ymin = lower__, ymax = upper__), alpha = .1) +
  geom_line(linewidth = 1.5, 
            colour = "#333333",
            alpha = .8) +
   geom_errorbarh(
    data = post_sample_block1 %>% 
      mutate(y = .01),
    mapping = aes(xmin = .lower, xmax = .upper, y = y), 
    color = "#333333",
    height = 0,
    alpha = .5,
    size = 1, 
    inherit.aes = F) +
  geom_point(
    data = post_sample_block1 %>% 
      median_qi(PSE) %>% 
      mutate(y = 0.01),
    mapping = aes(x = PSE, y = y),
    color = "#333333", 
    size = 1.3,
    alpha = .5) +
  annotate(
    geom = "text",
    x = 55,
    y = 0.02, 
    label = paste(round(post_sample_block1[[2]]), "ms", "-", round(post_sample_block1[[3]]), "ms"),
    size = 3.5,
    colour = "darkgray") +
   geom_rug(
    data = d.block1 %>% distinct(Item.VOT),
    mapping = aes(x = Item.VOT),
    alpha = .6,
    inherit.aes = F)
```



```{r IO-categorization-VOT-no-noise-block1}
lapse_exp2 = plogis(summary(fit_mix_uniform_bias)$fixed["theta1_Intercept", 1])

VOT.no.noise_block1 <- 
  get_IO_categorization(
    cues = c("VOT"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    with_noise = FALSE, 
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT.no.noise_block1")

PSE_VOT.no.noise_block1 <- get_PSE_quantiles(data = VOT.no.noise, group = "gender")

p.IOs.VOT.no.noise_block1 <- plot_IO_fit(
  data.production = VOT.no.noise_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT.no.noise_block1
)
```


```{r IO-categorization-VOT-block1}
VOT_block1 <- 
  get_IO_categorization(
    cues = c("VOT"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    with_noise = T, 
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT_block1")

PSE_VOT_block1 <- get_PSE_quantiles(data = VOT_block1, group = "gender")

p.IOs.VOT_block1 <- plot_IO_fit(
  data.production = VOT_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT_block1
)
```


```{r IO-categorization-VOT-centered-block1, message=FALSE}
VOT.centered_block1 <- 
  get_IO_categorization(
    cues = c("VOT_centered"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT.centered_block1")

PSE_VOT.centered_block1 <- get_PSE_quantiles(data = VOT.centered_block1, group = "gender")

p.IOs.VOT.centered_block1 <- plot_IO_fit(
  data.production = VOT.centered_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT.centered_block1
)
```




```{r IO-categorization-VOT-F0-no-noise-block1, message=FALSE}
VOT_F0.no.noise_block1 <- 
  get_IO_categorization(
    cues = c("VOT", "f0_Mel"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    with_noise = F, 
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT_F0.no.noise_block1")

PSE_VOT_F0.no.noise_block1 <- get_PSE_quantiles(data = VOT_F0.no.noise_block1, group = "gender")

p.IOs.VOT_F0.no.noise_block1 <- plot_IO_fit(
  data.production = VOT_F0.no.noise_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT_F0.no.noise_block1
)
```



```{r IO-categorization-VOT-F0-block1, message=FALSE}
VOT_F0_block1 <- 
  get_IO_categorization(
    cues = c("VOT", "f0_Mel"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT_F0_block1")

PSE_VOT_F0_block1 <- get_PSE_quantiles(data = VOT_F0_block1, group = "gender")

p.IOs.VOT_F0_block1 <- plot_IO_fit(
  data.production = VOT_F0_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT_F0_block1
)
```



```{r IO-categorisation-VOT-F0-centered-block1}
VOT_F0.centered_block1 <- 
  get_IO_categorization(
    cues = c("VOT_centered", "f0_Mel_centered"), 
    groups = c("Talker", "gender"), 
    lapse_rate = lapse_exp2,
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT_F0.centered_block1")

PSE_VOT_F0.centered_block1 <- get_PSE_quantiles(data = VOT_F0.centered_block1, group = "gender")

p.IOs.VOT_F0.centered_block1 <- plot_IO_fit(
  data.production = VOT_F0.centered_block1, 
  data.perception = psychometric_fit_block1 %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1,
  PSE_VOT_F0.centered_block1
)
```



```{r, message=FALSE}
# prepare data for IO that centers perceptual input relative to database mean before categorization
chodroff.means <- d.chodroff_wilson.selected %>% 
  group_by(Talker) %>% 
  summarise(across(c(VOT, f0_Mel), mean)) %>% 
  ungroup() %>% 
  summarise(across(c(VOT, f0_Mel), mean)) 

chodroff.mean_VOT <- chodroff.means %>% pull(VOT)
chodroff.mean_f0 <- chodroff.means %>% pull(f0_Mel)

# center exposure data relative to database mean
psychometric_fit_block1_centered <- psychometric_fit_block1 %>% 
  mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2),
         Item.VOT = Item.VOT + (chodroff.mean_VOT - VOT.mean_exp2),
         VOT_gs = (Item.VOT - VOT.mean_exp2)/(2 * VOT.sd_exp2))


# get expected values of the centered EXPOSURE VOT under the linear model
if (file.exists("../models/centered_exposure_block1_exp2.rds")) {
  epred_centered_block1 <- read_rds("../models/epred_centered_exposure_block1_exp2.rds")
} else {
  epred_centered_block1 <- epred_draws(
    object = fit_mix_block1,
    newdata = psychometric_fit_block1_centered,
    re_formula = NA,
    ndraws = 2000)
  write_rds(epred_centered_block1, file = "../models/epred_centered_exposure_block1_exp2.rds")
}
```


```{r IO-categorisation-VOT-F0-exposure-centered, warning=FALSE}
VOTs = seq(0, 85, .5) + (chodroff.mean_VOT - VOT.mean_exp2) # defining VOTs here as temporary debugging measure
VOT_F0.centered.input_block1 <- 
  get_IO_categorization(
    cues = c("VOT_centered", "f0_Mel_centered"), 
    groups = c("Talker", "gender"), 
    VOTs = seq(0, 85, .5) + (chodroff.mean_VOT - VOT.mean_exp2),
    F0s = normMel(predict_f0(VOTs)) + (chodroff.mean_f0_Mel - f0.mean_exp2),
    lapse_rate = lapse_exp2,
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT_F0.centered.input_block1")

PSE_VOT_F0.centered.input_block1 <- get_PSE_quantiles(data = VOT_F0.centered.input_block1, group = "gender")

# plot predicted categorisation after centering exposure, against IOs
p.IOs.VOT_F0.centered.input_block1 <- plot_IO_fit(
  data.production = VOT_F0.centered.input_block1, 
  data.perception = epred_centered_block1 %>% 
  group_by(VOT_gs) %>% 
  summarise(lower__ = quantile(.epred, probs = .025),
    median = quantile(.epred, probs = .5),
    upper__ = quantile(.epred, probs = .975),
    estimate__ = median(.epred)) %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2) - (chodroff.mean_VOT - VOT.mean_exp2)),
  data.test = d.block1 %>%  
    ungroup() %>% distinct(Item.VOT),
  data.percept.PSE = post_sample_block1 %>% 
    mutate(PSE = PSE - (chodroff.mean_VOT - VOT.mean_exp2),
           .lower = .lower - (chodroff.mean_VOT - VOT.mean_exp2),
           .upper = .upper - (chodroff.mean_VOT - VOT.mean_exp2)),
  PSE_VOT_F0.centered.input_block1 %>% 
    mutate(PSE.lower = round(PSE.lower - (chodroff.mean_VOT - VOT.mean_exp2), 2),
           PSE.median = PSE.median - (chodroff.mean_VOT - VOT.mean_exp2),
           PSE.upper = round(PSE.upper - (chodroff.mean_VOT - VOT.mean_exp2), 2))) 
```


```{r IO-categorization-VOT-f0-centered-input-block1, warning=FALSE}
VOT.centered.input_block1 <- 
  get_IO_categorization(
    cues = c("VOT_centered"), 
    groups = c("Talker", "gender"), 
    VOTs = seq(0, 85, .5) + (chodroff.mean_VOT - VOT.mean_exp2),
    F0s = normMel(predict_f0(VOTs)) + (chodroff.mean_f0_Mel - f0.mean_exp2),
    lapse_rate = lapse_exp2,
    alpha = .1, 
    linewidth = 0.3,
    io.type = "VOT.centered.input_block1")

PSE_VOT.centered.input_block1 <- get_PSE_quantiles(data = VOT.centered.input_block1, group = "gender")

p.IOs.VOT.centered.input_block1 <- plot_IO_fit(
  data.production = VOT.centered.input_block1, 
  data.perception = epred_centered_block1 %>% 
  group_by(VOT_gs) %>% 
  summarise(lower__ = quantile(.epred, probs = .025),
    median = quantile(.epred, probs = .5),
    upper__ = quantile(.epred, probs = .975),
    estimate__ = median(.epred)) %>% 
    mutate(Item.VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2) - (chodroff.mean_VOT - VOT.mean_exp2)),
  data.test = d.block1 %>% 
    ungroup() %>% 
    distinct(Item.VOT),
  data.percept.PSE = post_sample_block1 %>% 
    mutate(PSE = PSE - (chodroff.mean_VOT - VOT.mean_exp2),
           .lower = .lower - (chodroff.mean_VOT - VOT.mean_exp2),
           .upper = .upper - (chodroff.mean_VOT - VOT.mean_exp2)),
  PSE_VOT.centered.input_block1 %>% 
    mutate(PSE.lower = round(PSE.lower - (chodroff.mean_VOT - VOT.mean_exp2), 2),
           PSE.median = PSE.median - (chodroff.mean_VOT - VOT.mean_exp2),
           PSE.upper = round(PSE.upper - (chodroff.mean_VOT - VOT.mean_exp2), 2))
)
```






```{r prepare-plots-to-make-IO-plot-grid-block1, fig.width=4, fig.height=8.5, warning=FALSE}
# make plot grid
remove_all_titles <- 
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank())
guide_spec <-  guides(linetype = "none")

p11 <- p.IOs.VOT.no.noise_block1 + 
  remove_all_titles

p12 <- p.IOs.VOT_block1 + 
  remove_all_titles 

p13 <- p.IOs.VOT_F0_block1 + 
  remove_x_title

p14 <- p.IOs.VOT_F0.centered_block1 + 
  remove_all_titles

p15 <- p.IOs.VOT_F0.centered.input_block1 + 
  remove_y_title  


p11 + p12 + p13 + p14 + p15 + 
  plot_layout(ncol = 1, byrow = F, guides = "collect") &
  theme(legend.position = "top")
```






```{r compute-average-likelihood-of-IOs-block1, message=FALSE}
# join all 8 IOs into one dataframe
d.IOs_block1 <- 
  rbind(VOT.no.noise_block1, VOT_block1, VOT.centered_block1, VOT.centered.input_block1, VOT_F0.no.noise_block1, VOT_F0_block1, VOT_F0.centered_block1, VOT_F0.centered.input_block1) %>% 
  select(Talker, gender, io, io.type) %>% 
  mutate(io.type = factor(io.type))

d.test.IO_block1 <- 
  d.block1 %>%
  ungroup() %>%
  select(Item.Filename, Item.VOT, Item.Mel_F0_5ms, Response, Response.Voicing) %>%
  filter(Item.VOT > -5) %>%  # filter to only the +ve for the time being; may need to include the -ve values
  mutate(
    VOT_F0 = map2(Item.VOT, Item.Mel_F0_5ms, ~ c(.x, .y)),
    Item.VOT_centered = Item.VOT + (chodroff.mean_VOT - VOT.mean_exp2),
    Item.Mel_F0_centered = Item.Mel_F0_5ms + (chodroff.mean_f0 - f0.mean_exp2), 
    VOT_F0_centered = map2(Item.VOT_centered, Item.Mel_F0_centered, ~ c(.x, .y)),
    Response.category = ifelse(Response.Voicing == "voiced", "/d/", "/t/"),
    Response.t = ifelse(Response.Voicing == "voiceless", 1, 0)) %>%
  nest(d.perception = everything()) %>%
  crossing(d.IOs_block1) 

d.test.IO_block1 %<>%
  mutate(
    log_likelihood_per_response =
      pmap(
        list(d.perception, io, io.type), 
        ~ get_average_log_likelihood_of_perception_data_under_IO(
          observed_inputs = if(str_detect(..3, "VOT.centered.input")) {..1$Item.VOT_centered} 
          else if (str_detect(..3, "VOT_F0.centered.input")) {..1$VOT_F0_centered} 
          else if (str_detect(..3, "VOT_F0_block1$")) {..1$VOT_F0} 
          else if (str_detect(..3, "VOT_F0.no.noise")) {..1$VOT_F0} 
          else if (str_detect(..3, "VOT_F0.centered")) {..1$VOT_F0}
          else {..1$Item.VOT}, 
          observed_responses = ..1$Response.category, 
          model = ..2)) %>% 
      unlist(),
    likelihood_per_response = exp(log_likelihood_per_response)) %>% 
  mutate(
    io.type = fct_relevel(io.type, c("VOT.no.noise_block1", "VOT_block1", "VOT.centered_block1", "VOT.centered.input_block1", "VOT_F0.no.noise_block1", "VOT_F0_block1", "VOT_F0.centered_block1", "VOT_F0.centered.input_block1")),
    gender = fct_relevel(gender, c("female", "male")))

IO.likelihood_block1 <- d.test.IO_block1 %>% 
  group_by(io.type) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response),
    mode_likelihood_per_response = max(likelihood_per_response, na.rm = T)
  )

# use bootstrap samples by io type to find the CI of the median
set.seed(7896)
d.bootstrap_block1 <- d.test.IO_block1 %>% 
  droplevels() %>% 
  select(-c(d.perception, io)) %>% 
  mutate(io.type = factor(io.type)) %>% 
  # nest so that we can sample from groups of rows corresponding to each io type
  nest(data = -c(gender, io.type)) %>% 
  mutate(samples = 
           map(data, ~ bootstraps(.x, times = 1000))) %>% 
  unnest(samples) %>% 
  mutate(splits = map(splits, ~ as_tibble(.x)),
         median_sample = map_dbl(splits, ~ median(.x$likelihood_per_response)),
         mode_sample = map_dbl(splits, ~ max(.x$likelihood_per_response))) 
```



```{r, fig.width=4.5, fig.height=4.5, warning=FALSE}
# plot median likelihood per response with bootstrapped 95% CI
p.likelihood.univar_block1 <- d.test.IO_block1 %>% 
  group_by(io.type, gender) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response),
    mode_likelihood_per_response = max(likelihood_per_response)) %>% 
  pivot_longer(cols = c(median_likelihood_per_response, mode_likelihood_per_response), 
               names_to = "parameter", names_pattern = "(.*)_likelihood_per_response") %>% 
  ggplot(aes(x = io.type, colour = gender, y = value, shape = parameter)) +
  geom_point(size = 3, 
             position = position_dodge(.2)) +
  # geom_point(aes(y = mode_likelihood_per_response), 
  #            size = 3,
  #            position = position_dodge(.2),
  #            shape = 17) +
  geom_errorbar(
    data = d.bootstrap_block1 %>% 
      group_by(io.type, gender) %>% 
               summarise(
                 median.lower = quantile(median_sample, probs = .025),
                 median.upper = quantile(median_sample, probs = .975),
                 mode.lower = quantile(mode_sample, probs = .025),
            mode.upper = quantile(mode_sample, probs = .975)) %>% 
  pivot_longer(cols = c(3:6), names_to = c("parameter", "quantile"), names_pattern = "(.*)\\.(.*)") %>% 
 pivot_wider(names_from = quantile, values_from = "value"),
             mapping = 
               aes(x = io.type, ymin = lower, ymax = upper, colour = gender),
             position = position_dodge(.2),
             linewidth = 1.5,
             alpha = .6,
             width = 0,
             inherit.aes = F) +
      # geom_errorbar(
      #   data = d.bootstrap_block1 %>% 
      #     group_by(io.type, gender) %>% 
      #     summarise(
      #       mode.lower = quantile(mode_sample, probs = .025),
      #       mode.upper = quantile(mode_sample, probs = .975)),
      #   mapping = aes(x = io.type, ymin = mode.lower, ymax = mode.upper, colour = gender),
      #   position = position_dodge(.2),
      #   linewidth = 1.5,
      #   alpha = .6,
      #   width = 0,
      #   inherit.aes = F) +
     scale_y_continuous("Median likelihood per response", limits = c(.10736, .11005), labels = scales::label_number(accuracy = 0.0001)) + 
     scale_x_discrete( 
       limits = c("VOT.no.noise_block1", "VOT_block1", "VOT.centered_block1", "VOT.centered.input_block1"),
       breaks = c("VOT.no.noise_block1", "VOT_block1", "VOT.centered_block1", "VOT.centered.input_block1"),
       labels = c(VOT.no.noise_block1 = "VOT", 
                  VOT_block1 = "VOT\n(+perceptual noise)", 
                  VOT.centered_block1 = "VOT\n(talker-centered)", 
                  VOT.centered.input_block1 = "VOT\n(+exposure-centered)")) +
     scale_colour_manual("Gender", values = colours.sex, labels = c("Female", "Male")) +
     theme(legend.position = "top") +
     guides(x = guide_axis(angle = 45)) +
     coord_trans(y = "log10")

p.likelihood.univar_block1
```



```{r likelihood-comparison-plots-block1, fig.width=5.5, fig.height=4.5, warning=FALSE, cache=FALSE}
# zoom in on multi-variate models
p.likelihood.multivar_block1 <- d.test.IO_block1 %>% 
  group_by(gender, io.type) %>% 
  summarise(
    median_likelihood_per_response = median(likelihood_per_response),
    mode_likelihood_per_response = max(likelihood_per_response)) %>%
  ggplot(aes(x = io.type, colour = gender)) +
  geom_point(aes(y = median_likelihood_per_response),
             size = 3, position = position_dodge(.2)) +
  geom_point(aes(x = io.type, 
                 y = mode_likelihood_per_response), 
             size = 3,
             position = position_dodge(.2),
             shape = 17) +
  geom_errorbar(
    data = d.bootstrap_block1 %>% 
      group_by(gender, io.type) %>% 
      summarise(
        median.lower = round(quantile(median_sample, probs = .025), 3),
        median.upper = round(quantile(median_sample, probs = .975), 3)),
    mapping = 
      aes(x = io.type, ymin = median.lower, ymax = median.upper, colour = gender),
    linewidth = 1.5,
    position = position_dodge(.2),
    alpha = .6,
    width = 0,
    inherit.aes = F) + 
  geom_errorbar(
    data = d.bootstrap_block1 %>% 
      group_by(gender, io.type) %>%
      summarise(
        mode.lower = quantile(mode_sample, probs = .025),
        mode.upper = quantile(mode_sample, probs = .975)),
    mapping = aes(x = io.type, ymin = mode.lower, ymax = mode.upper, colour = gender),
    size = 1.5,
    position = position_dodge(.2),
    alpha = .6,
    width = 0,
    inherit.aes = F) +
  scale_y_continuous("Median likelihood per response", limits = c(.156, .746), labels = scales::label_number(accuracy = 0.001)) +
  scale_x_discrete( 
    limits = c("VOT_F0.no.noise_block1", "VOT_F0_block1", "VOT_F0.centered_block1", "VOT_F0.centered.input_block1"),
    breaks = c("VOT_F0.no.noise_block1", "VOT_F0_block1", "VOT_F0.centered_block1", "VOT_F0.centered.input_block1"),
    labels = c(VOT_F0.no.noise_block1 = "VOT_F0", VOT_F0_block1 = "VOT-F0\n(+perceptual noise)", VOT_F0.centered_block1 = "VOT-F0\n (talker-centered)", VOT_F0.centered.input_block1 = "VOT-F0 \n (exposure-centered)")) +
  scale_colour_manual("Gender", values = colours.sex, labels = c("Female", "Male")) +
  guides(x = guide_axis(angle = 45)) + 
  theme(legend.position = "top") +
  coord_trans(y = "log10")

p.likelihood.multivar_block1
```
## Regression analysis
The regression analysis addresses two main questions: 
Do participants shift their categorisation behaviour in an incremental fashion, i.e. do they exhibit categorisation behaviour that draws closer to the ideal categorisation function with each successive exposure block?
Are the differences in shifts between the conditions proportional to the magnitude of the shifts between exposure distributions i.e. is the +40ms condition 3 times that of the +10ms condition?

As with experiment 1 we fit a Bayesian mixed-effects psychometric model with lapse and perceptual components. Continuous predictors were standardised to twice the standard deviation and priors and sampling parameters were identical to those specified in experiment 1. 

To analyse the incremental effects of exposure condition on the proportion of /t/ responses at test, the perceptual model contained exposure condition (backward difference coded, comparing the +10ms against the +0ms shift condition, and the +40ms against the +10ms shift condition), test block (backward difference coded from the first to last test block), VOT (Gelman scaled), and their full factorial interaction. For the perceptual model, "t"-responses were regressed on the three-way interaction of VOT, condition, and block. Random effects were modelled with varying intercepts and slopes by participant and varying intercepts and slopes by minimal pair item. The lapsing model which estimates participant bias on trials with attention lapses was fitted without an intercept but with an offset [how does one describe this? what does offset(0) represent]. Finally, a population-level intercept was fitted to estimate the lapse rate. Random effects for the lapsing model and lapse rates were not fitted to limit the number of parameters and to ensure model convergence.

### Expectations
Given previous findings of @kleinschmidt2016you we expected participants in the various exposure conditions to shift their average categorization functions towards the direction of the ideal categorization function implied by their respective exposure distributions. We expected the differences between the groups to be most pronounced after the final exposure block as they would have had the complete exposure to all the tokens that make up the exposure distributions. This follows from predictions of incremental Bayesian belief-updating -- that listeners would integrate their prior expectations with the current input to infer the present talker's cue-to-category-mapping (the posterior distribution). 
Also based on previous findings, we expected the +40ms group to not fully  converge on the ideal categorization function implied by the exposure distribution as the further an exposure talker's cue distributions deviates from a *typical* talker, the further the distance the average categorization function from the implied boundary. We therefore expected to see differences in categorizations between the +10ms and +40ms conditions such that listeners in the +40ms condition would shift more than those in the +10ms condition but to have an average categorization function located to the left of the ideal function. [@kleinschmidt2016you].
```{r, results='hide'}
# set condition levels
levels_Condition.Exposure = c("Shift0", "Shift10", "Shift40")

contrasts <- d.test_exposure_for_analysis %>% 
    filter(Phase == "test") %>% 
    prepVars(levels.Condition = levels_Condition.Exposure) 
```



```{r warning=FALSE}
d.test_exposure_for_analysis %<>% 
  mutate(Response.Voiceless = ifelse(Response.Voicing == "voiceless", 1, 0))

my_priors <- c(
  prior(student_t(3, 0, 2.5), class = "b", dpar = "mu2"),
  prior(student_t(3, 0, 2.5), class = "b", dpar = "theta1"),
  prior(cauchy(0, 2.5), class = "sd"),
  prior(lkj(1), class = "cor"))

# simplifying model with uniform bias
fit_mix_uniform_bias <- brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~ 1 + VOT_gs * Condition.Exposure * Block + (1 + VOT_gs * Block | ParticipantID) + (1 + VOT_gs * Condition.Exposure * Block | Item.MinimalPair),
    theta1 ~ 1),
  data = d.test_exposure_for_analysis %>% 
    filter(Phase == "test") %>% 
    prepVars(levels.Condition = levels_Condition.Exposure),
  cores = 4,
  chains = chains,
  init = 0,
  iter = 4000, # iterations to run
  warmup = 2000, # samples used to fit
  family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
  control = list(adapt_delta = .99),
  file = "../models/Exp-AE-DLVOT-labelled-lapsing-GLMM.rds")

fixed_eff_exp2 <- tidy(fit_mix_uniform_bias, effects = "fixed") %>% 
  select(-c(effect, component))

kable(fixed_eff_exp2, booktabs = TRUE, caption = "Population estimates") %>% 
  kable_styling(c("striped", "repeat_header"), position = "center") 
```


Fig. XX summarizes participants’ categorization functions across the different test blocks. A first point to note are the average categorization functions of the respective conditions before exposure to the talker. As depicted in the first panel, the average functions converge on the same boundary or PSE (4xms, CI = ) which suggests that participants largely had similar expectations about the cue distribution corresponding to /d/ and /t/ for this type of talker. What it also shows is that in setting our baseline condition we may have underestimated the perceived boundary for our test stimuli by approximately 20ms which implies that the +10ms shift and the +40ms shift were in fact -10ms and +20ms respectively.[ELABORATION]

There was a main effect of VOT $\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_VOT_gs", "mu2_VOT_gs > 0")`; participants were more likely to respond "t" as VOT increased. Condition had a main effect on responses such that with larger shifts, participants on average responded with fewer "t"s.  Additionally, the difference in average "t" responses between the +40ms and +10ms conditions ($\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_Condition.Exposure_Shift10vs.Shift40", "mu2_Condition.Exposure_Shift10vs.Shift40 < 0")` reduction in log-odds) was *larger* than the difference between the +10 and +0 conditions ($\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_Condition.Exposure_Shift0vs.Shift10", "mu2_Condition.Exposure_Shift0vs.Shift10 < 0")` reduction in log-odds). 
Qualitatively, the results indicate listeners adjust their expectations to align with the statistics of the exposure talker, consonant with previous findings of studies employing this paradigm (e.g., @clayards2008; @kleinschmidt2016you; @theodore2019distributional). 

While there was weak evidence for a main effect of block its interaction with condition revealed how participants in the respective groups responded as they progressively received more informative exposure. Most of the change took place after the first exposure block. participants in the +10ms condition on average, responded with fewer "ts" *relative* to participants in the +0ms condition in block 3 compared to that in block 1 ($\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_Condition.Exposure_Shift0vs.Shift10:Block_Block1vs.Block3", "mu2_Condition.Exposure_Shift0vs.Shift10:Block_Block1vs.Block3 < 0")`). This difference between the  +40ms and +10ms condition in block 3 compared to that in block 1 was more pronounce, reflecting the larger distance between the two exposure distributions ($\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_Condition.Exposure_Shift10vs.Shift40:Block_Block1vs.Block3", "mu2_Condition.Exposure_Shift10vs.Shift40:Block_Block1vs.Block3 < 0")`). 

In test block 5, the difference in average log-odds between +0ms and +10ms in block 5 relative to block 3 was *positive* such that the margin of difference between the two conditions in block 5 was smaller than the corresponding margin in block 3 ($\hat{\beta} =$ `r get_CI(fit_mix_uniform_bias, "mu2_Condition.Exposure_Shift0vs.Shift10:Block_Block3vs.Block5", "mu2_Condition.Exposure_Shift0vs.Shift10:Block_Block3vs.Block5 >0")`). In blocks 7 and 8, the  average log-odds difference between +0ms and +10ms increased marginally when compared to the preceding block (as indicated by the negative signs of the estimates; see table xx) while in block 9 the difference between the two exposure conditions began to narrow. The average difference between the +40ms and +10ms condition continued to widen in blocks 5 and block 7 relative to preceding blocks albeit by progressively smaller increments. This trend would reverse in blocks 8 and 9. In all, the respective conditions hit their maximal shifts between blocks 3 and 5 and began to display a reversal of the exposure effects by the end of block 7. This observation in the final test 3 test blocks was expected given previous findings that distributional learning effects can begin to dissipate after extended testing. While 

In summary, the analysis shows that listeners' categorization behaviour changed very early on in the experiment -- only after 24 exposure trials to each category -- but they reached the limits of their shifts just as quickly. Subsequent exposure blocks had a marginal effect on their categorization behaviour. Like this study's predecessor, the various exposure groups did 
```{r, results='hide'}
cond_fit <- conditional_effects(
    fit_mix_uniform_bias,
    effects = "VOT_gs:Condition.Exposure",
    conditions = make_conditions(
      d.test_exposure_for_analysis %>% 
        filter(Phase == "test") %>% 
        prepVars(levels.Condition = levels_Condition.Exposure), 
      vars = c("Block")),
    method = "posterior_epred",
    ndraws = 500,
    re_formula = NA)

p.conditions <- cond_fit[[1]] %>%
  mutate(Condition.Exposure = case_when(Condition.Exposure == "Shift0" ~ "+0ms",
                                        Condition.Exposure == "Shift10" ~ "+10ms",
                                        Condition.Exposure == "Shift40" ~ "+40ms"),
         Test_block = factor(case_when(Block == 1 ~ "Block 1",
                                Block == 3 ~ "Block 3",
                                Block == 5 ~ "Block 5",
                                Block == 7 ~ "Block 7",
                                Block == 8 ~ "Block 8",
                                Block == 9 ~ "Block 9"))) %>% 
           ggplot(aes(x = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2), 
                      y = estimate__, 
                      group = Condition.Exposure)) +
           geom_ribbon(aes(ymin = lower__, ymax = upper__, fill = Condition.Exposure), alpha = .1) +
           geom_line(aes(color = Condition.Exposure), 
                     size = 1) +
           geom_rug(data = d.test_exposure_for_analysis %>%
                      distinct(Item.VOT),
                    aes(x = Item.VOT),
                    alpha = 0.5,
                    colour = "lightgrey",
                    inherit.aes = F) +
           stat_summary(
             data = d.test_exposure_for_analysis %>%
               filter(Phase == "test") %>%
               mutate(Condition.Exposure =
                        case_when(Condition.Exposure == "Shift0" ~ "+0ms",
                                  Condition.Exposure == "Shift10" ~ "+10ms",
                                  Condition.Exposure == "Shift40" ~ "+40ms"),
                      Test_block = factor(case_when(Block == 1 ~ "Block 1",
                                Block == 3 ~ "Block 3",
                                Block == 5 ~ "Block 5",
                                Block == 7 ~ "Block 7",
                                Block == 8 ~ "Block 8",
                                Block == 9 ~ "Block 9"))) %>%
               group_by(Condition.Exposure, Test_block),
             fun.data = mean_cl_boot,
             mapping = aes(x = Item.VOT,
                           y = Response.Voiceless,
                           colour = Condition.Exposure),
             geom = "pointrange",
             size = 0.2,
             position = position_dodge2(width = 2),
             inherit.aes = F) +
           scale_x_continuous("VOT (msec)", limits = c(-5, 75), breaks = c(-5, 5, 15, 25, 35,45, 55, 70)) +
           scale_y_continuous("Proportion \"t\"-responses") +
           scale_color_manual(
             "Condition",
             labels = c("+0ms", "+10ms", "+40ms"),
             values = c("#cc0000", "#12D432","#0481F3"),
             aesthetics = c("color", "fill")) +
           facet_wrap(
             . ~ Test_block,
             nrow = 2) + 
           theme(legend.position = "top")
```



```{r fitted-categorisation-function-condition-block, fig.width=6, fig.height=4.5}
p.conditions 
```


```{r}
# d.contrast.compare <- d.test_exposure_for_analysis %>% 
#   filter(Phase == "test") %>% 
#   select(Condition.Exposure, Block) %>% 
#   distinct()
# 
# newdata <- crossing(d.contrast.compare, VOT_gs = 0)
# 
# predictions <- epred_draws(
#   fit_mix_uniform_bias,
#     newdata = newdata,
#   re_formula = NA,
#   ndraws = 100) %>% 
#   group_by(Condition.Exposure, Block) %>% 
#   summarise(mean_log_odds = mean(.epred)) %>% 
#   mutate(VOT = descale(VOT_gs, VOT.mean_exp2, VOT.sd_exp2), 
#          prob = plogis(.epred))
```




```{r}
# nested models for extracting the 'intercepts' and 'slopes' for each Block and Condition
fit_mix_nested <- brm(
  bf(
    Response.Voiceless ~ 1,
    mu1 ~ 0 + offset(0),
    mu2 ~  0 + I(paste(Condition.Exposure, Block, sep = "x")) / VOT_gs + 
      (0 + Block / VOT_gs | ParticipantID) + 
      (0 + I(paste(Condition.Exposure, Block, sep = "x")) / VOT_gs | Item.MinimalPair),
    theta1 ~ 1),
  data = d.test_exposure_for_analysis %>% 
    filter(Phase == "test") %>% 
    prepVars(levels.Condition = levels_Condition.Exposure),
  cores = 4,
  chains = chains,
  init = 0,
  iter = 4000, # iterations to run
  warmup = 2000, # samples used to fit
  family = mixture(bernoulli("logit"), bernoulli("logit"), order = F),
  control = list(adapt_delta = .99),
  file = "../models/Exp-AE-DLVOT-labelled-lapsing-GLMM-nested.rds")
```








\newpage

<!-- This is a markdown comment that will NOT show when you knit the document.  -->

All data and code for this article can be downloaded from[https://osf.io/q7gjp/](OSF). This article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models. Readers can revisit any of the assumptions we make---for example, by substituting alternative models of linguistic representations. The supplementary information (SI, \@ref(sec:SI-software)) lists the software/libraries required to compile this document. Beyond our immediate goals here, we hope that this can be helpful to researchers who are interested in developing more informative experimental designs, and to facilitate the interpretation of existing results [see also @tan2021]. 